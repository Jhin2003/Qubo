{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "769dcd01",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m     text = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m pdf.pages:\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m         text += \u001b[43mpage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextract_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Initialize the text splitter\u001b[39;00m\n\u001b[32m     12\u001b[39m text_splitter = RecursiveCharacterTextSplitter(\n\u001b[32m     13\u001b[39m     chunk_size=\u001b[32m1000\u001b[39m,  \u001b[38;5;66;03m# Length of each chunk in characters\u001b[39;00m\n\u001b[32m     14\u001b[39m     chunk_overlap=\u001b[32m200\u001b[39m  \u001b[38;5;66;03m# Overlap between chunks to preserve context\u001b[39;00m\n\u001b[32m     15\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Migs\\Desktop\\ragbot\\.venv\\Lib\\site-packages\\pdfplumber\\page.py:530\u001b[39m, in \u001b[36mPage.extract_text\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    529\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs: Any) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m530\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_textmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtuplify_list_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.as_string\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Migs\\Desktop\\ragbot\\.venv\\Lib\\site-packages\\pdfplumber\\page.py:507\u001b[39m, in \u001b[36mPage._get_textmap\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    505\u001b[39m     defaults.update({\u001b[33m\"\u001b[39m\u001b[33mlayout_height\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.height})\n\u001b[32m    506\u001b[39m full_kwargs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] = {**defaults, **kwargs}\n\u001b[32m--> \u001b[39m\u001b[32m507\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m utils.chars_to_textmap(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchars\u001b[49m, **full_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Migs\\Desktop\\ragbot\\.venv\\Lib\\site-packages\\pdfplumber\\container.py:52\u001b[39m, in \u001b[36mContainer.chars\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchars\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> T_obj_list:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobjects\u001b[49m.get(\u001b[33m\"\u001b[39m\u001b[33mchar\u001b[39m\u001b[33m\"\u001b[39m, [])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Migs\\Desktop\\ragbot\\.venv\\Lib\\site-packages\\pdfplumber\\page.py:346\u001b[39m, in \u001b[36mPage.objects\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    344\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_objects\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    345\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._objects\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m \u001b[38;5;28mself\u001b[39m._objects: Dict[\u001b[38;5;28mstr\u001b[39m, T_obj_list] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._objects\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Migs\\Desktop\\ragbot\\.venv\\Lib\\site-packages\\pdfplumber\\page.py:449\u001b[39m, in \u001b[36mPage.parse_objects\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m objects.get(kind) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    448\u001b[39m         objects[kind] = []\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     objects[kind].append(obj)\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m objects\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import json\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Extract text from the PDF using pdfplumber\n",
    "with pdfplumber.open(\"NU.pdf\") as pdf:\n",
    "    text = \"\"\n",
    "    for page in pdf.pages:\n",
    "        text += page.extract_text()\n",
    "\n",
    "# Initialize the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # Length of each chunk in characters\n",
    "    chunk_overlap=200  # Overlap between chunks to preserve context\n",
    ")\n",
    "\n",
    "# Split the extracted text into chunks\n",
    "chunks = text_splitter.split_text(text)\n",
    "\n",
    "# Open a JSONL file to save the chunks and metadata\n",
    "with open(\"chunks_output.jsonl\", \"w\") as file:\n",
    "    # Iterate over all chunks and save them with metadata\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        # Create metadata for the chunk\n",
    "        metadata = {\n",
    "            \"chunk_number\": idx + 1,  # Chunk number\n",
    "            \"chunk_size\": len(chunk),  # Length of the chunk\n",
    "            \"text\": chunk  # Actual chunk text\n",
    "        }\n",
    "        \n",
    "        # Convert the metadata dictionary to a JSON string and write to the file\n",
    "        file.write(json.dumps(metadata) + \"\\n\")\n",
    "        \n",
    "        # Optionally print the first 10 chunks to the console\n",
    "        if idx < 10:\n",
    "            print(f\"Chunk {idx + 1}:\\n{chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4774764",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1154530c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Migs\\Desktop\\ragbot\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Function to read chunks from the JSONL file\n",
    "def read_jsonl(file_path):\n",
    "    chunks = []\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            metadata = json.loads(line)  # Read each line and parse as JSON\n",
    "            chunks.append(metadata[\"text\"])  # Extract chunk text\n",
    "    return chunks\n",
    "\n",
    "# Read chunks from the JSONL file\n",
    "chunks = read_jsonl(\"chunks_output.jsonl\")\n",
    "\n",
    "# Initialize the Sentence-BERT model (or other embedding model)\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # You can use any other Sentence-BERT model\n",
    "\n",
    "# Embed the chunks into vectors (embeddings)\n",
    "embeddings = model.encode(chunks)\n",
    "\n",
    "# Convert the embeddings to a NumPy array (FAISS requires NumPy arrays)\n",
    "embeddings_np = np.array(embeddings)\n",
    "\n",
    "# Initialize a FAISS index (using L2 distance for semantic similarity)\n",
    "index = faiss.IndexFlatL2(embeddings_np.shape[1])  # L2 distance index\n",
    "\n",
    "# Add the embeddings to the FAISS index (this creates the vector database)\n",
    "index.add(embeddings_np)\n",
    "\n",
    "# Save the FAISS index to a file for later use\n",
    "faiss.write_index(index, \"vector_database.index\")\n",
    "\n",
    "print(f\"FAISS index with {len(chunks)} vectors saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9b47dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
