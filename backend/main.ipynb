{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "769dcd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 418 chunks with rich metadata to chunks_output.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Migs\\AppData\\Local\\Temp\\ipykernel_17840\\3145691935.py:64: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  created_at = datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "import pdfplumber\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "PDF_PATH = \"NU.pdf\"\n",
    "OUT_PATH = \"chunks_output.jsonl\"\n",
    "\n",
    "# --- Helpers ---------------------------------------------------------------\n",
    "\n",
    "def sha1_of_file(path, buf_size=1024 * 1024):\n",
    "    h = hashlib.sha1()\n",
    "    with open(path, \"rb\") as f:\n",
    "        while True:\n",
    "            b = f.read(buf_size)\n",
    "            if not b:\n",
    "                break\n",
    "            h.update(b)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def make_chunk_id(source_sha1: str, page: int, global_idx: int, page_idx: int) -> str:\n",
    "    # Deterministic, human-readable-ish ID\n",
    "    core = f\"{source_sha1[:12]}:p{page}:g{global_idx}:k{page_idx}\"\n",
    "    return hashlib.sha1(core.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "# --- Extract per-page text -------------------------------------------------\n",
    "\n",
    "with pdfplumber.open(PDF_PATH) as pdf:\n",
    "    total_pages = len(pdf.pages)\n",
    "    page_texts = []\n",
    "    for i, page in enumerate(pdf.pages, start=1):\n",
    "        txt = page.extract_text() or \"\"\n",
    "        if txt.strip():\n",
    "            page_texts.append((i, txt))\n",
    "\n",
    "# --- Splitter (per page to keep page provenance) ---------------------------\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "# Build a global list of (page, chunk_text)\n",
    "chunks_with_pages = []\n",
    "page_local_counts = defaultdict(int)\n",
    "\n",
    "for page_num, page_text in page_texts:\n",
    "    # Split this page's text; chunks won't cross pages\n",
    "    page_chunks = splitter.split_text(page_text)\n",
    "    for ch in page_chunks:\n",
    "        page_local_counts[page_num] += 1\n",
    "        chunks_with_pages.append((page_num, page_local_counts[page_num], ch))\n",
    "\n",
    "# --- File-level provenance -------------------------------------------------\n",
    "\n",
    "source_name = os.path.basename(PDF_PATH)\n",
    "source_size = os.path.getsize(PDF_PATH)\n",
    "source_sha1 = sha1_of_file(PDF_PATH)\n",
    "created_at = datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\"\n",
    "\n",
    "# --- Write JSONL with rich metadata ---------------------------------------\n",
    "\n",
    "with open(OUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    for global_idx, (page_num, idx_in_page, chunk_text) in enumerate(chunks_with_pages):\n",
    "        meta = {\n",
    "            # Identifiers\n",
    "            \"id\": make_chunk_id(source_sha1, page_num, global_idx, idx_in_page),\n",
    "            \"chunk_number\": global_idx + 1,              # 1-based global index\n",
    "            \"chunk_index_global\": global_idx,            # 0-based\n",
    "            \"chunk_index_in_page\": idx_in_page,          # 1-based within page\n",
    "\n",
    "            # Content\n",
    "            \"text\": chunk_text,\n",
    "            \"chunk_char_count\": len(chunk_text),\n",
    "\n",
    "            # Provenance (file-level)\n",
    "            \"source_file\": source_name,\n",
    "            \"source_path\": os.path.abspath(PDF_PATH),\n",
    "            \"source_type\": \"pdf\",\n",
    "            \"source_size_bytes\": source_size,\n",
    "            \"source_sha1\": source_sha1,\n",
    "\n",
    "            # Provenance (page-level)\n",
    "            \"page_start\": page_num,\n",
    "            \"page_end\": page_num,\n",
    "            \"page_total\": total_pages,\n",
    "\n",
    "            # Process info\n",
    "            \"created_at\": created_at,\n",
    "            \"splitter\": {\n",
    "                \"type\": \"RecursiveCharacterTextSplitter\",\n",
    "                \"chunk_size\": 1000,\n",
    "                \"chunk_overlap\": 200\n",
    "            },\n",
    "        }\n",
    "        f.write(json.dumps(meta, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Wrote {len(chunks_with_pages)} chunks with rich metadata to {OUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4774764",
   "metadata": {},
   "source": [
    "Embedding and Index Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1154530c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Migs\\AppData\\Local\\Temp\\ipykernel_17840\\2369102216.py:33: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "c:\\Users\\Migs\\Desktop\\ragbot\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index saved to 'vector_databases.index' with 391 documents (metadata included).\n",
      "Metadata preview written to 'chunks_metadata_preview.jsonl'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Import Document (handles both old/new LangChain versions)\n",
    "try:\n",
    "    from langchain.schema import Document\n",
    "except Exception:\n",
    "    from langchain_core.documents import Document\n",
    "\n",
    "# --- Read chunks *with* metadata from JSONL and build Documents ---\n",
    "def read_jsonl_as_documents(file_path: str):\n",
    "    docs = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            obj = json.loads(line)\n",
    "            text = obj.pop(\"text\", \"\")  # remove text from metadata dict\n",
    "            # Ensure a chunk_number exists even if not provided\n",
    "            obj.setdefault(\"chunk_number\", i)\n",
    "            docs.append(Document(page_content=text, metadata=obj))\n",
    "    return docs\n",
    "\n",
    "# Paths\n",
    "chunks_path = \"chunks_output.jsonl\"\n",
    "index_dir = \"vector_databases.index\"\n",
    "\n",
    "# Load docs\n",
    "docs = read_jsonl_as_documents(chunks_path)\n",
    "\n",
    "# Embeddings\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Build FAISS *with metadata* (docstore will keep it)\n",
    "vectorstore = FAISS.from_documents(docs, embedding_model)\n",
    "\n",
    "# Save FAISS index + docstore (includes metadata)\n",
    "Path(index_dir).mkdir(parents=True, exist_ok=True)\n",
    "vectorstore.save_local(index_dir)\n",
    "\n",
    "print(f\"FAISS index saved to '{index_dir}' with {len(docs)} documents (metadata included).\")\n",
    "\n",
    "# --- Optional: write a human-readable sidecar of metadata for quick inspection ---\n",
    "sidecar = \"chunks_metadata_preview.jsonl\"\n",
    "with open(sidecar, \"w\", encoding=\"utf-8\") as out:\n",
    "    for d in docs:\n",
    "        out.write(json.dumps(\n",
    "            {\n",
    "                # Short preview to avoid huge files\n",
    "                \"text_preview\": d.page_content[:160],\n",
    "                **d.metadata\n",
    "            },\n",
    "            ensure_ascii=False\n",
    "        ) + \"\\n\")\n",
    "print(f\"Metadata preview written to '{sidecar}'.\")\n",
    "\n",
    "# --- Example: how to load and retrieve later (metadata comes back in results) ---\n",
    "# from langchain.vectorstores import FAISS\n",
    "# vectorstore = FAISS.load_local(index_dir, embedding_model, allow_dangerous_deserialization=True)\n",
    "# hits = vectorstore.similarity_search(\"message from the president\", k=3)\n",
    "# for h in hits:\n",
    "#     print(h.metadata, h.page_content[:120])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb494028",
   "metadata": {},
   "outputs": [],
   "source": [
    "Retrieval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5341fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Table of Contents\n",
      "Message from the President ................................................................................................................. v\n",
      "Privacy Statement ................................................................................................................................. vi\n",
      "History ................................................................................................................................................ viii\n",
      "National University Hymn ......................................................................................................................x\n",
      "School Logo, Colors and Motto ............................................................................................................. xi\n",
      "Vision, Mission and Dynamic Filipinism ............................................................................................... xii\n",
      "\n",
      "[2] the University.\n",
      "Aside from the norms in this handbook, bulletin board postings, electronic announcements, and\n",
      "published announcements are the usual channels by which the University Administration informs\n",
      "the student body of official business.\n",
      "The administrative authority of the University is vested in the President of the institution. The\n",
      "continued attendance of any student at National University subjects them to this authority,\n",
      "conforming to the spirit of the ongoing policies set by the academic community.\n",
      "As members of the National University community, every student is expected to familiarize\n",
      "themselves and be guided by the contents of this handbook.\n",
      "The 2022 Student Handbook was reviewed and approved by the multisectoral Student Handbook\n",
      "Committee and shall take effect beginning in Academic Year 2022-2023 unless otherwise\n",
      "amended or revoked.\n",
      "Thank you.\n",
      "RENATO CARLOS H. ERMITA, JR. PhD\n",
      "President/CEO\n",
      "vPrivacy Statement\n",
      "\n",
      "[3] its establishment in 1900. It upholds high standards of educational services for the holistic\n",
      "development of lifelong learners.\n",
      "The President and CEO, with the management team, shall formulate the quality policy of the National\n",
      "University. The quality policy supports the vision, mission, and the objectives of the University.\n",
      "The management, faculty, and staff commit to continuously improving the efficiency of operational\n",
      "and management processes to meet ISO 9001 and applicable statutory, regulatory, and institutional\n",
      "requirements.\n",
      "xviData Privacy Policy\n",
      "(For Applicants, Students, and Alumni)\n",
      "Welcome to the National University. This Privacy Policy (also known as a Privacy Notice) tells you about\n",
      "our policy regarding the data that we collect, use, or otherwise process your personal data. If you are\n",
      "a parent/legal guardian of an applicant or student (current or former) who is a minor (below 18 years\n",
      "old), understand that this Policy refers to the personal data of your child/ward.\n",
      "\n",
      "[4] 1) The student respondent shall be informed in writing of the allegation(s) and must submit a\n",
      "written explanation. The student’s parent(s) or guardian shall be furnished with a copy of\n",
      "the letter.\n",
      "2) The student shall have the right to listen and examine the evidence presented, ask\n",
      "clarificatory questions, and present evidence on their behalf.\n",
      "3) In all stages of the proceedings, the student shall have the right to the assistance of a counsel\n",
      "of their choice.\n",
      "4) The UPCC must have a quorum to pass a resolution.\n",
      "5) Once a resolution is made, the UPCC shall submit its report to the President’s Council as a\n",
      "recommending body.\n",
      "6) Upon approval of the recommendation, the student shall be informed of the decision in\n",
      "writing.\n",
      "4. Appeal\n",
      "Should the respondent find the decision of the UPCC unfavorable, they may file an appeal within\n",
      "five (5) working days of the receipt of the decision. It must state the grounds and reasons for the\n",
      "\n",
      "[5] School Hymn shall be sung either before the activity begins or ends. With this, we shall inculcate a\n",
      "deep pride and love for National University.\n",
      "I pledge my life, my honor,\n",
      "To thee, my Alma Mater,\n",
      "Who made me grow in wisdom,\n",
      "Gave me love and made me strong.\n",
      "I shall defend thy good name.\n",
      "I’ll strive to bring thee more fame.\n",
      "I shall wave thee gold and blue,\n",
      "The colors of National U.\n",
      "I shall wave thee gold and blue,\n",
      "The colors of National U!\n",
      "(Repeat)\n",
      "xSchool Logo, Colors and Motto\n",
      "School Logo\n",
      "Colors\n",
      "Blue stands for the noble cause of National University.\n",
      "Gold portrays the unwavering dedication of the school to nation-building.\n",
      "Motto\n",
      "The motto of National University is “Education That Works”.\n",
      "xiVision, Mission and Dynamic Filipinism\n",
      "Vision\n",
      "We are National University, a dynamic private institution committed to nation-building, recognized\n",
      "internationally in education and research.\n",
      "Mission\n",
      "Guided by the core values and characterized by our cultural heritage of Dynamic Filipinism, National\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the FAISS index from disk\n",
    "vectorstore = FAISS.load_local(\"vector_databases.index\", embedding_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "# Query the index\n",
    "query = \"message from the president and what is it about\"\n",
    "results = vectorstore.similarity_search(query, k=5)  # Get top 5 most similar chunks\n",
    "\n",
    "# Display the results\n",
    "for idx, doc in enumerate(results, start=1):\n",
    "    print(f\"[{idx}] {doc.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27892fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec9b47dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The message from the President in the provided documents is a welcome message for the 2022 Student Handbook. In this message, President RENATO CARLOS H. ERMITA, JR. welcomes the students to the National University and provides information about the handbook, its purpose, and when it will take effect. The handbook serves as a guide for students on the rules, policies, and procedures of the university.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "# Define the model name\n",
    "model = \"mistral:instruct\"\n",
    "\n",
    "# Prepare the retrieved content for the Mistral model prompt\n",
    "retrieved_text = \"\\n\\n\".join([doc.page_content for doc in results])\n",
    "\n",
    "# Formulate the prompt including the retrieved context\n",
    "prompt = f\"Here are some documents related to your query:\\n\\n{retrieved_text}\\n\\nBased on the information above, answer the following question: {query}\"\n",
    "\n",
    "# Send the prompt to the Mistral model\n",
    "response = ollama.chat(model=model, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "\n",
    "# Print the response content\n",
    "print(response['message']['content'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
