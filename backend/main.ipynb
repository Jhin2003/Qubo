{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdbc0298",
   "metadata": {},
   "source": [
    "Ingestion and Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "769dcd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 1007 chunks to chunks.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Migs\\AppData\\Local\\Temp\\ipykernel_13692\\1226643297.py:59: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  created_at = datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "\n",
    "import pdfplumber\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "PDF_PATH = \"data_store/pdfs/prob.pdf\"\n",
    "OUT_PATH = \"chunks.jsonl\"\n",
    "\n",
    "# --- Helpers ---------------------------------------------------------------\n",
    "\n",
    "def sha1_of_file(path, buf_size=1024 * 1024):\n",
    "    h = hashlib.sha1()\n",
    "    with open(path, \"rb\") as f:\n",
    "        while True:\n",
    "            b = f.read(buf_size)\n",
    "            if not b:\n",
    "                break\n",
    "            h.update(b)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def make_chunk_id(source_sha1: str, page: int, global_idx: int, page_idx: int) -> str:\n",
    "    # Deterministic, human-readable-ish ID\n",
    "    core = f\"{source_sha1[:12]}:p{page}:g{global_idx}:k{page_idx}\"\n",
    "    return hashlib.sha1(core.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "# --- Extract per-page text -------------------------------------------------\n",
    "\n",
    "with pdfplumber.open(PDF_PATH) as pdf:\n",
    "    total_pages = len(pdf.pages)\n",
    "    page_texts = []\n",
    "    for i, page in enumerate(pdf.pages, start=1):\n",
    "        txt = page.extract_text() or \"\"\n",
    "        if txt.strip():\n",
    "            page_texts.append((i, txt))\n",
    "\n",
    "# --- Splitter (per page to keep page provenance) ---------------------------\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=10,\n",
    "    chunk_overlap=1,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "# Build a global list of (page, chunk_text)\n",
    "chunks_with_pages = []\n",
    "for page_num, page_text in page_texts:\n",
    "    # Split this page's text; chunks won't cross pages\n",
    "    page_chunks = splitter.split_text(page_text)\n",
    "    for idx_in_page, ch in enumerate(page_chunks, start=1):\n",
    "        chunks_with_pages.append((page_num, idx_in_page, ch))\n",
    "\n",
    "# --- File-level provenance -------------------------------------------------\n",
    "\n",
    "source_name = os.path.basename(PDF_PATH)\n",
    "source_sha1 = sha1_of_file(PDF_PATH)\n",
    "created_at = datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\"\n",
    "\n",
    "# --- Write JSONL with simplified structure -------------------------------\n",
    "\n",
    "with open(OUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    for global_idx, (page_num, idx_in_page, chunk_text) in enumerate(chunks_with_pages):\n",
    "        meta = {\n",
    "            # Content (directly storing the text without extra nesting)\n",
    "            \"content\": chunk_text,\n",
    "            \n",
    "            # Metadata\n",
    "            \"metadata\": {\n",
    "                \"id\": make_chunk_id(source_sha1, page_num, global_idx, idx_in_page),\n",
    "                \"source\": source_name,\n",
    "                \"page\": page_num,\n",
    "                \"author\": \"Unknown\",  # You can add a way to extract the author if needed\n",
    "                \"created_at\": created_at,\n",
    "            },\n",
    "        }\n",
    "        f.write(json.dumps(meta, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Wrote {len(chunks_with_pages)} chunks to {OUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e986facd",
   "metadata": {},
   "source": [
    "\n",
    "Embedding and Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1154530c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 15 documents.\n",
      "FAISS index saved to 'data_store/vector_databases.index' with 15 documents (metadata included).\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Paths\n",
    "chunks_path = \"chunks.jsonl\"\n",
    "index_dir = \"data_store/vector_databases.index\"\n",
    "\n",
    "# --- Read chunks *with* metadata from JSONL ---\n",
    "def read_jsonl(file_path: str):\n",
    "    texts = []\n",
    "    metadatas = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            obj = json.loads(line)\n",
    "            text = obj[\"content\"]\n",
    "            metadata = obj.get(\"metadata\", {})\n",
    "            metadata.setdefault(\"chunk_number\", i)\n",
    "            texts.append(text)\n",
    "            metadatas.append(metadata)\n",
    "    return texts, metadatas\n",
    "\n",
    "\n",
    "\n",
    "# Load docs\n",
    "texts, metadatas = read_jsonl(chunks_path)\n",
    "print(f\"Loaded {len(texts)} documents.\")\n",
    "\n",
    "# Embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# --- Create FAISS vectorstore with normalization ---\n",
    "vectorstore = FAISS.from_texts(\n",
    "    texts=texts,\n",
    "    embedding=embedding_model,\n",
    "    metadatas=metadatas,\n",
    "    normalize_L2=True   # <-- This ensures cosine similarity\n",
    ")\n",
    "\n",
    "# Save FAISS index + docstore\n",
    "Path(index_dir).mkdir(parents=True, exist_ok=True)\n",
    "vectorstore.save_local(index_dir)\n",
    "\n",
    "print(f\"FAISS index saved to '{index_dir}' with {len(texts)} documents (metadata included).\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb494028",
   "metadata": {},
   "outputs": [],
   "source": [
    "Similarity Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b5341fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "Content: INTRODUCTION TO COMPUTING Course Code: CCINCOM/L\n",
      "Probability\n",
      "• Measurement of uncertainty.\n",
      "• We ponder on tonight’s traffic jam, tomorrow’s weather, next week’s stock prices,\n",
      "Review in Probability, Intro to RL Concepts\n",
      "an upcoming election, or where we left our hat, often we do not know an\n",
      "outcome with certainty.\n",
      "• Also used in games and simulations.\n",
      "• Probability tells us how likely it is that a particular event will occur.\n",
      "• Often denoted as 𝑃(𝐴) where 𝐴 is an event or a collection of possible\n",
      "outcomes.\n",
      "• Conforms to the Discrete Probability Law:\n",
      "𝑁𝑢𝑚𝑏𝑒𝑟 𝑜𝑓 𝑒𝑙𝑒𝑚𝑒𝑛𝑡𝑠 𝑖𝑛 𝐴\n",
      "𝑃(𝐴) =\n",
      "𝑛 𝑝𝑜𝑠𝑠𝑖𝑏𝑙𝑒 𝑜𝑢𝑡𝑐𝑜𝑚𝑒𝑠\n",
      "RJoEsINeFpOh RMCaErMviEnN RT. ILmEpAeRrNiaINl G NU College of Computing and InCfoourrmsaet Cioond Tee: cChCnRoNloFgLiReLs\n",
      "Metadata: {'id': '63d252931056443cc2c0979d136f95cda05271dd', 'source': 'prob.pdf', 'page': 2, 'author': 'Unknown', 'created_at': '2025-08-26T11:29:15Z', 'chunk_number': 1}\n",
      "Score: 0.3912\n",
      "--------------------------------------------------\n",
      "[2]\n",
      "Content: INTRODUCTION TO COMPUTING Course Code: CCINCOM/L\n",
      "Probability\n",
      "There’s only one 4, and six possible numbers\n",
      "Review in Probability, Intro to RL Concepts\n",
      "total.\n",
      "So we say the probability is 1 out of 6, or:\n",
      "P(4) = 1/6\n",
      "That matches the formula shown on the slide:\n",
      "P(A) = (Number of elements in A) / (Total\n",
      "possible outcomes)\n",
      "What’s the probability\n",
      "of getting a 4?\n",
      "Where A is the event you're interested in.\n",
      "RJoEsINeFpOh RMCaErMviEnN RT. ILmEpAeRrNiaINl G NU College of Computing and InCfoourrmsaet Cioond Tee: cChCnRoNloFgLiReLs\n",
      "Metadata: {'id': '644daba8dd004c301bad831ed401cfc3f3c90070', 'source': 'prob.pdf', 'page': 3, 'author': 'Unknown', 'created_at': '2025-08-26T11:29:15Z', 'chunk_number': 2}\n",
      "Score: 0.2061\n",
      "--------------------------------------------------\n",
      "[3]\n",
      "Content: INTRODUCTION TO COMPUTING Course Code: CCINCOM/L\n",
      "Example\n",
      "• The most fundamental stochastic experiment is the experiment where a coin\n",
      "is tossed a number of times, say 𝑛 times.\n",
      "Review in Probability, Intro to RL Concepts\n",
      "• Tossing a coin a few times won’t immediately give you a 50% chance for all\n",
      "outcomes. But tossing it many, many times will.\n",
      "the Law of Large Numbers. It tells us that\n",
      "over a large number of trials, the results will\n",
      "settle into predictable patterns.\n",
      "RJoEsINeFpOh RMCaErMviEnN RT. ILmEpAeRrNiaINl G NU College of Computing and InCfoourrmsaet Cioond Tee: cChCnRoNloFgLiReLs\n",
      "Metadata: {'id': '6163987db17fd5e83e42f8a3ca8ce3414d5ae349', 'source': 'prob.pdf', 'page': 4, 'author': 'Unknown', 'created_at': '2025-08-26T11:29:15Z', 'chunk_number': 3}\n",
      "Score: 0.1793\n",
      "--------------------------------------------------\n",
      "[4]\n",
      "Content: INTRODUCTION TO COMPUTING Course Code: CCINCOM/L\n",
      "Probability Mass Function (PMF)\n",
      "• PMF of a random variable is a function that maps possible outcomes of\n",
      "a random variable to the corresponding probabilities.\n",
      "Review in Probability, Intro to RL Concepts\n",
      "PMF = How likely each outcome is\n",
      "• We can represent PMFs as a formula, a graph, or a table.\n",
      "P(X = 0) 1/8\n",
      "P(X = 1) 3/8\n",
      "P(X = 2) 3/8\n",
      "P(X = 3) 1/8\n",
      "P(X >= 4) 0\n",
      "RJoEsINeFpOh RMCaErMviEnN RT. ILmEpAeRrNiaINl G NU College of Computing and InCfoourrmsaet Cioond Tee: cChCnRoNloFgLiReLs\n",
      "Metadata: {'id': '521d7fcd87c84a5766e8b192c4f8a2a4a4d84220', 'source': 'prob.pdf', 'page': 11, 'author': 'Unknown', 'created_at': '2025-08-26T11:29:15Z', 'chunk_number': 10}\n",
      "Score: 0.1565\n",
      "--------------------------------------------------\n",
      "[5]\n",
      "Content: INTRODUCTION TO COMPUTING Course Code: CCINCOM/L\n",
      "Review in Probability, Intro to RL Concepts\n",
      "Key idea:\n",
      "A simple event is when we’re focusing on just one specific\n",
      "outcome.\n",
      "RJoEsINeFpOh RMCaErMviEnN RT. ILmEpAeRrNiaINl G NU College of Computing and InCfoourrmsaet Cioond Tee: cChCnRoNloFgLiReLs\n",
      "Metadata: {'id': 'f82e04e65af1c05c8deb7e1f09d0356d8bdda630', 'source': 'prob.pdf', 'page': 9, 'author': 'Unknown', 'created_at': '2025-08-26T11:29:15Z', 'chunk_number': 8}\n",
      "Score: 0.1198\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Load the FAISS index from disk\n",
    "vectorstore = FAISS.load_local(\"data_store/vector_databases.index\", embedding_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "# Query the index\n",
    "query = \"what is probability\"\n",
    "results = vectorstore.similarity_search_with_relevance_scores(query, k=5)\n",
    "\n",
    "# Print results\n",
    "for i, (doc, score) in enumerate(results, start=1):\n",
    "    print(f\"[{i}]\")\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "    print(f\"Score: {score:.4f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928d8da3",
   "metadata": {},
   "source": [
    "Retrieval-Augmented Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ec9b47dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The question involves rolling two six-sided dice, where each die has 6 faces numbered from 1 to 6. Since there are two dice being rolled, the total number of possible outcomes (or sample space) is the product of the number of sides on each die, which in this case is 6 * 6 = 36.\n",
      "\n",
      "A random variable X is defined as the sum of the numbers rolled on the two dice. The possible values of X range from 2 to 12, since the minimum sum is obtained by rolling a 1 on one die and a 1 on the other, while the maximum sum is obtained by rolling a 6 on both dice.\n",
      "\n",
      "The probability distribution (Probability Mass Function - PMF) gives the probabilities of each possible outcome. For example, P(X = 2) gives the probability of rolling a 2 as the sum when two dice are rolled. This can be calculated using the formula:\n",
      "\n",
      "P(X = x) = (Number of ways to get x) / Total number of outcomes\n",
      "\n",
      "In this case, there is only one way to get a sum of 2, which is by rolling a 1 on each die. Therefore, P(X = 2) = 1/36. Similarly, the probabilities for other values of X can be calculated based on the number of ways to obtain those sums.\n",
      "\n",
      "The PMF table and graph should have the x-axis labeled with the possible values of X (from 2 to 12), and the y-axis labeled with the probability of each outcome. The graph would show the distribution of probabilities for different sums when rolling two dice.\n",
      "\n",
      "Sources:\n",
      "- prob.pdf (page 15)\n",
      "- prob.pdf (page 6)\n",
      "- prob.pdf (page 3)\n",
      "- prob.pdf (page 4)\n",
      "- prob.pdf (page 2)\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import Ollama\n",
    "\n",
    "# Initialize your retriever from FAISS\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "\n",
    "# Initialize your Ollama LLM\n",
    "llm = Ollama(model=\"mistral:instruct\")\n",
    "\n",
    "\n",
    "with open(\"prompt_template/qa.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    prompt_text = f.read()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_text\n",
    ")\n",
    "\n",
    "# Create a RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    "    return_source_documents=True,        # <<— important\n",
    ")\n",
    "\n",
    "query = \"can you explain me about the dice\"\n",
    "\n",
    "res = qa_chain.invoke({\"query\": query})  # prefer .invoke to get the dict\n",
    "print(res[\"result\"])\n",
    "\n",
    "# Pretty-print unique sources\n",
    "seen = set()\n",
    "print(\"\\nSources:\")\n",
    "for doc in res[\"source_documents\"]:\n",
    "    src = doc.metadata.get(\"source\") or doc.metadata.get(\"file_path\") or \"unknown\"\n",
    "    page = doc.metadata.get(\"page\")\n",
    "    key = (src, page)\n",
    "    if key in seen: \n",
    "        continue\n",
    "    seen.add(key)\n",
    "    # nice short name\n",
    "    short = Path(src).name if isinstance(src, str) else str(src)\n",
    "    if page is not None:\n",
    "        print(f\"- {short} (page {page})\")\n",
    "    else:\n",
    "        print(f\"- {short}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61581dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mCreating model: ('PP-OCRv5_server_det', None)\u001b[0m\n",
      "\u001b[31mNo available model hosting platforms detected. Please check your network connection.\u001b[0m\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "No available model hosting platforms detected. Please check your network connection.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpaddleocr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PaddleOCR\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m ocr = \u001b[43mPaddleOCR\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_doc_orientation_classify\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_doc_unwarping\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_textline_orientation\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# text detection + text recognition\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# ocr = PaddleOCR(use_doc_orientation_classify=True, use_doc_unwarping=True) # text image preprocessing + text detection + textline orientation classification + text recognition\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# ocr = PaddleOCR(use_doc_orientation_classify=False, use_doc_unwarping=False) # text detection + textline orientation classification + text recognition\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# ocr = PaddleOCR(\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m \u001b[38;5;66;03m#     use_doc_unwarping=False,\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m#     use_textline_orientation=False) # Switch to PP-OCRv5_mobile models\u001b[39;00m\n\u001b[32m     15\u001b[39m result = ocr.predict(\u001b[33m\"\u001b[39m\u001b[33mZZ.jpg\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Migs\\Desktop\\ragbot\\.venv\\Lib\\site-packages\\paddleocr\\_pipelines\\ocr.py:163\u001b[39m, in \u001b[36mPaddleOCR.__init__\u001b[39m\u001b[34m(self, doc_orientation_classify_model_name, doc_orientation_classify_model_dir, doc_unwarping_model_name, doc_unwarping_model_dir, text_detection_model_name, text_detection_model_dir, textline_orientation_model_name, textline_orientation_model_dir, textline_orientation_batch_size, text_recognition_model_name, text_recognition_model_dir, text_recognition_batch_size, use_doc_orientation_classify, use_doc_unwarping, use_textline_orientation, text_det_limit_side_len, text_det_limit_type, text_det_thresh, text_det_box_thresh, text_det_unclip_ratio, text_det_input_shape, text_rec_score_thresh, return_word_box, text_rec_input_shape, lang, ocr_version, **kwargs)\u001b[39m\n\u001b[32m    159\u001b[39m         base_params[name] = val\n\u001b[32m    161\u001b[39m \u001b[38;5;28mself\u001b[39m._params = params\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mbase_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Migs\\Desktop\\ragbot\\.venv\\Lib\\site-packages\\paddleocr\\_pipelines\\base.py:67\u001b[39m, in \u001b[36mPaddleXPipelineWrapper.__init__\u001b[39m\u001b[34m(self, paddlex_config, **common_args)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;28mself\u001b[39m._common_args = parse_common_args(\n\u001b[32m     64\u001b[39m     common_args, default_enable_hpi=_DEFAULT_ENABLE_HPI\n\u001b[32m     65\u001b[39m )\n\u001b[32m     66\u001b[39m \u001b[38;5;28mself\u001b[39m._merged_paddlex_config = \u001b[38;5;28mself\u001b[39m._get_merged_paddlex_config()\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m \u001b[38;5;28mself\u001b[39m.paddlex_pipeline = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_paddlex_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Migs\\Desktop\\ragbot\\.venv\\Lib\\site-packages\\paddleocr\\_pipelines\\base.py:102\u001b[39m, in \u001b[36mPaddleXPipelineWrapper._create_paddlex_pipeline\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    100\u001b[39m kwargs = prepare_common_init_args(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mself\u001b[39m._common_args)\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_merged_paddlex_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m DependencyError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    104\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    105\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mA dependency error occurred during pipeline creation. Please refer to the installation documentation to ensure all required dependencies are installed.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    106\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Migs\\Desktop\\ragbot\\.venv\\Lib\\site-packages\\paddlex\\inference\\pipelines\\__init__.py:166\u001b[39m, in \u001b[36mcreate_pipeline\u001b[39m\u001b[34m(pipeline, config, device, pp_option, use_hpip, hpi_config, *args, **kwargs)\u001b[39m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    164\u001b[39m     config.pop(\u001b[33m\"\u001b[39m\u001b[33mhpi_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m pipeline = \u001b[43mBasePipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpp_option\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpp_option\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_hpip\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_hpip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhpi_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhpi_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pipeline\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Migs\\Desktop\\ragbot\\.venv\\Lib\\site-packages\\paddlex\\utils\\deps.py:202\u001b[39m, in \u001b[36mpipeline_requires_extra.<locals>._deco.<locals>._wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(pipeline_cls.\u001b[34m__init__\u001b[39m)\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    201\u001b[39m     require_extra(extra, obj_name=pipeline_name, alt=alt)\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mold_init_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Migs\\Desktop\\ragbot\\.venv\\Lib\\site-packages\\paddlex\\inference\\pipelines\\_parallel.py:103\u001b[39m, in \u001b[36mAutoParallelSimpleInferencePipeline.__init__\u001b[39m\u001b[34m(self, config, *args, **kwargs)\u001b[39m\n\u001b[32m     97\u001b[39m         \u001b[38;5;28mself\u001b[39m._executor = MultiDeviceSimpleInferenceExecutor(\n\u001b[32m     98\u001b[39m             \u001b[38;5;28mself\u001b[39m._pipelines,\n\u001b[32m     99\u001b[39m             batch_sampler,\n\u001b[32m    100\u001b[39m             postprocess_result=\u001b[38;5;28mself\u001b[39m._postprocess_result,\n\u001b[32m    101\u001b[39m         )\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._multi_device_inference:\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m     \u001b[38;5;28mself\u001b[39m._pipeline = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_internal_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Migs\\Desktop\\ragbot\\.venv\\Lib\\site-packages\\paddlex\\inference\\pipelines\\_parallel.py:158\u001b[39m, in \u001b[36mAutoParallelImageSimpleInferencePipeline._create_internal_pipeline\u001b[39m\u001b[34m(self, config, device)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_internal_pipeline\u001b[39m(\u001b[38;5;28mself\u001b[39m, config, device):\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pipeline_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpp_option\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpp_option\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_hpip\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43muse_hpip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhpi_config\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhpi_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Migs\\Desktop\\ragbot\\.venv\\Lib\\site-packages\\paddlex\\inference\\pipelines\\ocr\\pipeline.py:117\u001b[39m, in \u001b[36m_OCRPipeline.__init__\u001b[39m\u001b[34m(self, config, device, pp_option, use_hpip, hpi_config)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mUnsupported text type \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\u001b[38;5;28mself\u001b[39m.text_type))\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28mself\u001b[39m.text_det_model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_det_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlimit_side_len\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtext_det_limit_side_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlimit_type\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtext_det_limit_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_side_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtext_det_max_side_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mthresh\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtext_det_thresh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbox_thresh\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtext_det_box_thresh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43munclip_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtext_det_unclip_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m text_rec_config = config.get(\u001b[33m\"\u001b[39m\u001b[33mSubModules\u001b[39m\u001b[33m\"\u001b[39m, {}).get(\n\u001b[32m    129\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mTextRecognition\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    130\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mmodel_config_error\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mconfig error for text_rec_model!\u001b[39m\u001b[33m\"\u001b[39m},\n\u001b[32m    131\u001b[39m )\n\u001b[32m    132\u001b[39m \u001b[38;5;28mself\u001b[39m.text_rec_score_thresh = text_rec_config.get(\u001b[33m\"\u001b[39m\u001b[33mscore_thresh\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Migs\\Desktop\\ragbot\\.venv\\Lib\\site-packages\\paddlex\\inference\\pipelines\\base.py:105\u001b[39m, in \u001b[36mBasePipeline.create_model\u001b[39m\u001b[34m(self, config, **kwargs)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    103\u001b[39m     pp_option = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m model = \u001b[43mcreate_predictor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbatch_size\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpp_option\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpp_option\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_hpip\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_hpip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhpi_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhpi_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Migs\\Desktop\\ragbot\\.venv\\Lib\\site-packages\\paddlex\\inference\\models\\__init__.py:69\u001b[39m, in \u001b[36mcreate_predictor\u001b[39m\u001b[34m(model_name, model_dir, device, pp_option, use_hpip, hpi_config, *args, **kwargs)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m     67\u001b[39m         model_name \u001b[38;5;129;01min\u001b[39;00m official_models\n\u001b[32m     68\u001b[39m     ), \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe model (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) is not supported! Please using directory of local model files or model name supported by PaddleX!\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     model_dir = \u001b[43mofficial_models\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m Path(model_dir).exists(), \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not exists!\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Migs\\Desktop\\ragbot\\.venv\\Lib\\site-packages\\paddlex\\inference\\utils\\official_models.py:582\u001b[39m, in \u001b[36m_ModelManager.__getitem__\u001b[39m\u001b[34m(self, model_name)\u001b[39m\n\u001b[32m    581\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_name):\n\u001b[32m--> \u001b[39m\u001b[32m582\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_model_local_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Migs\\Desktop\\ragbot\\.venv\\Lib\\site-packages\\paddlex\\inference\\utils\\official_models.py:557\u001b[39m, in \u001b[36m_ModelManager._get_model_local_path\u001b[39m\u001b[34m(self, model_name)\u001b[39m\n\u001b[32m    555\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mNo available model hosting platforms detected. Please check your network connection.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    556\u001b[39m     logging.error(msg)\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(msg)\n\u001b[32m    558\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._download_from_hoster(\u001b[38;5;28mself\u001b[39m._hosters, model_name)\n",
      "\u001b[31mException\u001b[39m: No available model hosting platforms detected. Please check your network connection."
     ]
    }
   ],
   "source": [
    "from paddleocr import PaddleOCR\n",
    "\n",
    "ocr = PaddleOCR(\n",
    "    use_doc_orientation_classify=False, \n",
    "    use_doc_unwarping=False, \n",
    "    use_textline_orientation=False) # text detection + text recognition\n",
    "# ocr = PaddleOCR(use_doc_orientation_classify=True, use_doc_unwarping=True) # text image preprocessing + text detection + textline orientation classification + text recognition\n",
    "# ocr = PaddleOCR(use_doc_orientation_classify=False, use_doc_unwarping=False) # text detection + textline orientation classification + text recognition\n",
    "# ocr = PaddleOCR(\n",
    "#     text_detection_model_name=\"PP-OCRv5_mobile_det\",\n",
    "#     text_recognition_model_name=\"PP-OCRv5_mobile_rec\",\n",
    "#     use_doc_orientation_classify=False,\n",
    "#     use_doc_unwarping=False,\n",
    "#     use_textline_orientation=False) # Switch to PP-OCRv5_mobile models\n",
    "result = ocr.predict(\"ZZ.jpg\")\n",
    "for res in result:\n",
    "    res.print()\n",
    "    res.save_to_img(\"output\")\n",
    "    res.save_to_json(\"output\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
