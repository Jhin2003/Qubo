{"content": "Improving Retrieval Augmented Language Model with Self-Reasoning\nYuanXia1,JingboZhou2,*,ZhenhuiShi1,JunChen1,HaifengHuang1\n1BaiduInc.,China2BaiduResearch,China\n{xiayuan,zhoujingbo,shizhenhui,chenjun22,huanghaifeng}@baidu.com\nAbstract Documents\nQuestion\nThe Retrieval-Augmented Language Model (RALM) has Who painted the ceiling of the Florence Cathedral?\ndemonstrated remarkable performance on knowledge-\nintensive tasks by integrating external knowledge during Relevant-Aware Process\ninference, which mitigates the factual hallucinations in-\nThe provide documents are relevant with question", "metadata": {"id": "62b8c7d45c6b08e90eb76ab033fb9418b848375a", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 1, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "The provide documents are relevant with question\nherited in large language models (LLMs). Despite these 1\nadvancements, challenges persist in the implementation of\nEvidence-Aware Selective Process\nRALMs, particularly in terms of reliability and traceability.\nSpecifically, the irrelevant document retrieval may result Document 1: … painted surface, was started in\nin unhelpful responses or even deteriorate the performance 1568 by Giorgio Vasari and Federico Zuccari and\nwould last till 1579. The upper portion, near the", "metadata": {"id": "16a876fbdeae7d694ecf998cc185d9f06da3294b", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 1, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "would last till 1579. The upper portion, near the\nof LLMs, while the lack of appropriate citations in outputs lantern, representing The 24 Elders of Apocalypse\ncomplicates efforts to verify the trustworthiness of the was finished by Vasari before his death … SELF-REASONING\nmodels. To this end, we propose a novel self-reasoning\nThis piece indicates that the painting of the Florence\nframeworkaimedatimprovingthereliabilityandtraceability\nCathedral was started in 1568 by Giorgio Vasari and\nof RALMs, whose core idea is to leverage reasoning Federico Zuccari, which directly answers the", "metadata": {"id": "4a578cda42a78c4b370a79cdc41a8e10894c9be9", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 1, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "of RALMs, whose core idea is to leverage reasoning Federico Zuccari, which directly answers the\ntrajectories generated by the LLM itself. The framework question.\n2\ninvolves constructing self-reasoning trajectories through\nthree processes: a relevance-aware process, an evidence- Trajectory Analysis Process\naware selective process, and a trajectory analysis process. The ceiling of the Florence Cathedral was painted\nby Federico Zuccari and his collaborators, beginning\nWeevaluatedourframeworkacrossfourpublicdatasets(two\nin 1568 and completing in 1579. The work was", "metadata": {"id": "38d7ebfc8b5232487e0062d8a82b81e36a76d622", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 1, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "Weevaluatedourframeworkacrossfourpublicdatasets(two\nin 1568 and completing in 1579. The work was\nshort-formQAdatasets,onelong-formQAdataset,andone started by Giorgio Vasari and later continued by\nfact verification dataset) to demonstrate its superiority. Our Federico Zuccari, with contributions from various\nartists, including Domenico Cresti [1][3]\nmethod can outperform existing state-of-the-art models and\n3\nachieve performance comparable with GPT-4, using only\n2,000trainingsamples. Final Answer\nGiorgio Vasari, Federico Zuccari\nIntroduction\nThe Retrieval-Augmented Language Model (RALM), also", "metadata": {"id": "1b7b10447796ed22f3321fc618380d6ed75bc607", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 1, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "Giorgio Vasari, Federico Zuccari\nIntroduction\nThe Retrieval-Augmented Language Model (RALM), also\nFigure1:AnexampleofhowSELF-REASONINGframework\ngeneratesreasoningtrajectories.\nknownasRetrieval-AugmentedGeneration(RAG),hasbe-\ncome a crucial enhancement for Large Language Models\n(LLMs)byintegratingexternalknowledgeduringinference.\nDespitetheiradvancedcapabilitiesinlanguageunderstand-\nareinherentinLLMs(Kwiatkowskietal.2019;Petronietal.\ningandgeneration(Brownetal.2020;Touvronetal.2023),\n2021;Rametal.2023).\nLLMs are prone to producing hallucinated and inaccurate", "metadata": {"id": "20f062888fbf6550f5f398c264e9f78fd04a0a40", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 1, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "2021;Rametal.2023).\nLLMs are prone to producing hallucinated and inaccurate\nNevertheless, there are still limitations associated with\ncontent, especially in knowledge-intensive tasks (Ji et al.\nRALMs,particularlyconcerningreliabilityandtraceability.\n2023). Augmenting LLMs with relevant information ob-\nFirstly,thereliabilityoftheretrievedinformationremainsa\ntained from external sources like Wikipedia and search en-\nsubstantialconcern.Previousstudieshaveshownthatnoisy\ngines has proven effective in reducing these inaccuracies\nretrieval can adversely affect the performance of an LLM", "metadata": {"id": "822e9ff921f8fc2b33509f0f8ce03a02655d83b6", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 1, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "retrieval can adversely affect the performance of an LLM\n(Guu et al. 2020; Lewis et al. 2020; Borgeaud et al. 2022;\n(Menick et al. 2022; Li et al. 2023), as irrelevant data can\nIzacard et al. 2022; Asai et al. 2024). This approach has\nlead to misguided responses and disturb the model’s abil-\nproveneffectiveinmitigatingthefactualhallucinationsthat\nitytoleverageitsintrinsicknowledgeeffectively.Secondly,\nthe interpretability and traceability of outputs generated by\n*JingboZhouisthecorrespondingauthor.", "metadata": {"id": "e79727819ffcd7cb3265380a5e75be268cb95409", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 1, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "the interpretability and traceability of outputs generated by\n*JingboZhouisthecorrespondingauthor.\nCopyright©2025,AssociationfortheAdvancementofArtificial RALMsneedtobeimproved.AlthoughRALMsincorporate\nIntelligence(www.aaai.org).Allrightsreserved. retrieved documents during both the training and inference\n4202\nceD\n91\n]LC.sc[\n3v31891.7042:viXra", "metadata": {"id": "237c2c18042f6efeeec03b797579dc396b4a85fa", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 1, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "phases,theymayfailtoexplicitlycitethesedocuments,thus • Weevaluateourframeworkonfourpublicdatasets(two\ncomplicatingtheprocessoftracingandverifyingtheclaims short-formQA,onelong-formQA,andonefactverifica-\nmadebyLLMs.Toimprovetheretrievalrobustness,recent tion), demonstrating that our method surpasses existing\nstudies have explored incorporating external tools such as state-of-the-artmodelsinperformanceusingonly2,000\nnatural language inference (NLI) models (Honovich et al. trainingsamples.\n2022) and document summarization models during infer-\nence(Yoranetal.2023;Xuetal.2024).However,theeffec-", "metadata": {"id": "e07e8b02c9f8669a9a3686576fdb47a9e7201ad8", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 2, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "ence(Yoranetal.2023;Xuetal.2024).However,theeffec-\nRelatedWork\ntivenessoftheseexternaltoolslargelyinfluencestheoverall\nRetrieval-augmentedLMs\nperformanceofRALMs.Additionally,trainingandoptimiz-\ning these auxiliary models require additional costs. Conse- Manystudieshaveinvestigatedaugmentingtheperformance\nquently,identifyingthemostappropriatetrainingandselec- ofLLMswithexternallyretrievedinformation(Izacardetal.\ntionmethodsforNLIandsummarizationmodelsremainsa 2022; Guu et al. 2020; Borgeaud et al. 2022) and some of", "metadata": {"id": "8fa5f43e7b88075894bbeab6a0c2872b2af16ce4", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 2, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "criticalchallengeinleveragingtheseapproaches. thempre-trainlanguagemodelswithretrievedpassages.For\nToaddresstheabovelimitations,weproposeanovelend- works focusing on RALMs with citations, Menick et al.\nto-end SELF-REASONING framework to improve the per- (2022); Nakano et al. (2021) instruct or train an LLM to\nformance of RALMs. For convenience, we will also re- answerquestionswithretrieveddocumentswhileproviding\nfer to this framework as SELF-REASONING RAG and use citations.Gaoetal.(2023b)proposesanend-to-endsystem", "metadata": {"id": "f88ef7a2018468a4cb03cc8fffc911a825cceabf", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 2, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "the terms interchangeably. Our intuition is that the explicit to retrieve supporting evidence and generate answers with\nself-reasoningtrajectorycraftedbyLLMscanimproveboth citations,whileonlyfocusingonpromptingwithoutupdat-\ntheretrievalrobustnessandaccuracyinquestionanswering. ing their model weights. Other works instruct or fine-tune\nDuring the pre-training phase, while an LLM primarily fo- LLMstouseexternaltoolstoretrievedynamically(Schick\ncuses on knowledge acquisition, it does not learn to reason et al. 2023; Yao et al. 2023; Jiang et al. 2023), which of-", "metadata": {"id": "4d1ac6e8c667cccd672d8a8bcad8036da3c840dc", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 2, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "from retrieved documents to generate answers. To address fers an adaptive method of when and what to search. Gao\nthis,afeasibleapproachistoincorporatereasoningtrajecto- etal.(2023a)improvestheattributionandfactualityoflan-\nriesintoapost-trainingphase.Suchanapproachcouldpo- guage models by taking outputs of LLMs and applying a\ntentially teach the model to reason and distinguish relevant post-processretrieve-and-editapproach.\nand irrelevant documents, thereby enhancing its query re-\nsponseaccuracy.AnexampleofhowourSELF-REASONING RobustnessforRALMs", "metadata": {"id": "b77846a3b7d05aa56061458767d719749ad59758", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 2, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "sponseaccuracy.AnexampleofhowourSELF-REASONING RobustnessforRALMs\nframework generates reasoning trajectories is illustrated in To improve the robustness of RALMs, previous works can\nFigure1.Incontrast,asshowninthemiddlepartofFigure bedividedintotwocategories.Thefirstcategoryutilizesre-\n2, the conventional RALM methods gather all documents trieveddocumentstoenhancetheChainofThought(CoT).\nin a non-selective manner, leading to the distraction of the Forexample,IRCoT(Trivedietal.2023)iterativelyusesre-", "metadata": {"id": "d72ce1acd6342cd083139583cf52b7a17596cd9e", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 2, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "LLMbyirrelevantcontentandconsequentlyresultinginthe trieved documents to generate CoT, which is then used to\ngenerationoferroneousanswers. retrievefurtherdocumentsinsubsequentsteps.ReAct(Yao\nOur framework constructs self-reasoning trajectories et al. 2023) introduces an iterative CoT paradigm that in-\ncomprising three processes: 1) a Relevance-Aware Process tegrates reasoning with search results. However, irrelevant\n(RAP), which instructs the LLM to judge the relevance retrievals may produce misguided CoT, adversely affecting", "metadata": {"id": "ff133286a365389f8a852f6138514b5dcc3f5644", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 2, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "between the retrieved documents and the question, 2) an LLMperformance(Menicketal.2022;Lietal.2023).\nEvidence-AwareSelectiveProcess(EAP),whichdirectsthe Toaddresstheissueofirrelevantretrievalinformation,the\nLLMtochooseandciterelevantdocuments,andthenauto- secondcategoryproposesusingexternalmodulestoprocess\nmaticallyselectsnippetsofkeysentencesasevidencefrom retrieved documents during inference. For instance, Yoran\ntheciteddocuments,3)aTrajectoryAnalysisProcess(TAP), et al. (2023) utilize a natural language inference model to", "metadata": {"id": "1b7d3e5816329a52420764af682f9ae07e6c4a3a", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 2, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "which requires the LLM to synthesize a concise analysis filter out irrelevant documents, Yan et al. (2024) employ a\nbased on all gathered self-reasoning trajectories generated retrievalevaluatortoclassifydocumentsbasedontheirqual-\nby previous two processes and subsequently provide the ity, and Xu et al. (2024) and Yu et al. (2023) apply models\nfinal inferred answer. Furthermore, we propose a gradual to filter out or compress retrieved documents. Baek et al.\ntrainingmethodbyemployingstage-wisemaskingstrategies (2023) deploy a separate small language model as a veri-", "metadata": {"id": "96da0ff0e0d6d6daf5105801828f430534512ad8", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 2, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "toenhancetheperformanceofourframework.Wesumma- fier to detect and correct errors in LLMs during retrieval.\nrizeourcontributionsasfollows: A method presented by Asai et al. (2024), which appears\nmost similar to our approach, develops a technique that in-\n• We propose a novel end-to-end SELF-REASONING\nstructsmodelstoretrieveinformationusingspecificallyde-\nframework that improves the robustness of RALMs by\nsigned reflection tokens. However, this approach needs to\nleveraging reasoning trajectories generated by the LLM\ntrainextracriticmodelsandgeneratormodelstopredictthe", "metadata": {"id": "af8fd3e71f9c3e6bba42f90a68df7a23dc0fce80", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 2, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "trainextracriticmodelsandgeneratormodelstopredictthe\nitself,withouttheneedforexternaltools.\nreflectiontokens,whichrequirestensofthousandsofextra\n• We carefully design three processes to enhance the in- trainingsamples.\nterpretability and traceability of RALMs by requiring Unlike the second group of works, which rely on exter-\nLLMs to explicitly generate snippets and citations from naltoolsoradditionalmodulestoeliminateirrelevantinfor-\ndocuments,andfurtherexplainthereasonwhyciteddoc- mation,theSELF-REASONINGRAGmethodintegratesself-", "metadata": {"id": "3bcb1ba8f5a54cade5248707ed5379b10e7c82c1", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 2, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "documents,andfurtherexplainthereasonwhyciteddoc- mation,theSELF-REASONINGRAGmethodintegratesself-\numentscanhelpanswerthequestion. reasoningdirectlyintothemodel’sarchitecture,therebyen-", "metadata": {"id": "bccf3fba2e2b27b107c105cbad12cad9b526c8ea", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 2, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "Basic LLMs\nInput Question Retrieved documents Retrieval Augmented LLMs Output 2000 ❌\n(Raw LLM Answer）\nWhen was Catch Me the film due to her busy schedule. Answer 1989 ❌\nIf You Can made? The original start date was Generation (RAG Answer）\nJanuary 2002, but was pushed to\nFebruary 7 in Los Angeles, 2002 ✅\nCalifornia. Locations … (Self-Reasoning Short Answer）\nSelf-Reasoning with Trajectories\n(Relevant Aware Process） (Evidence Aware Selective Process） (Trajectory Analysis Process）\nRelevant: Analysis:\nTrue Cite content: The film 'Catch Me If You Can' was made in", "metadata": {"id": "3e52f9f6d03448d13c1af96ec75eed7ad5e6b3a6", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 3, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "Relevant: Analysis:\nTrue Cite content: The film 'Catch Me If You Can' was made in\nRelevant Reason: Cit[e1 ]c …on ttheen to:riginal start date was 2002. It started filming in April 2002 in Park\nThe provided documents [1]J …an tuhaer yo r2ig0i0n2a,l bsutat rwt daas tpeu wsahse d to Avenue, just outside the Waldorf-Astoria\nare relevant with question.\nJanFueabrryu a2r0y0 72 ,i nb uLto ws aAsn gpeulsehse, d… to Hotel, and moved to Orange, New Jersey,\nFebruary 7 in Los Angeles, … before returning to Brooklyn for bank and\nReason to cite: courthouse scenes [1].", "metadata": {"id": "f5ea7c650dbbc7374bbeb1a4dab06bf3b0262090", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 3, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "Reason to cite: courthouse scenes [1].\nReTahsiosn p tioe cceit ep:rovides information on\nThtihs ep iceocme mpreonvcideemse inntf oarnmda ltoiocna toionn of\nthefi lcmoimngm feonr c'Cematechn tM aen dIf loYcoaut iCona no',f\nfilminindgic afotirn 'gC athtcaht iMt seta Irf tYeodu in C Aapnr',il 2002.\nindicating that it started in April 2002\nFigure 2: An illustration of the SELF-REASONING framework. The upper is the basic LLMs which answer the question by\ninherentknowledge.ThemiddleisthestandardretrievalaugmentedLMs,whichuseretrieveddocumentstohelpanswerthe", "metadata": {"id": "af2f900ca15355c9473fa657a4330fd1684f9cf8", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 3, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "question.ThebottomisourSELF-REASONINGframeworkwhichusesself-generatedreasontrajectoriestooutputanswers.\nhancingtheperformanceofLLMsandprovidingamoreef- Relevance-AwareProcess\nficientandscalablesolution.FurtherrelatedworksonLLMs\nIn this work, we choose DPR (Karpukhin et al. 2020) and\nforreasoningarediscussedintheAppendix. Contriever (Izacard et al. 2021) as default retrievers R to\nrecall the top-k relevant documents. When presented with\nPreliminary a question and a set of documents, people can determine\nwhetherthequestionisrelevanttotheretrieveddocuments.", "metadata": {"id": "f336d765ec729528659eb5eaf56f0446736591ca", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 3, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "whetherthequestionisrelevanttotheretrieveddocuments.\nWeformallydefinetheproblemofretrievalaugmentedgen- Therefore,wefirstinstructthemodeltojudgetherelevance\nerationwithself-reasoning.Givenaqueryqandacorpusof betweentheretrieveddocumentsDandthegivenquestionq.\ndocumentsD,anLLM-generatedanswerwithmstatements Wefurtherrequestthemodeltoexplicitlygeneratereasons\nand n tokens can be defined as y = (s 1 ,s 2 ,··· ,s m ) = explaining why given documents are identified as relevant.", "metadata": {"id": "2635736ed9ca8f520f5d7356639991de145a65b5", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 3, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "(w 1 ,w 2 ,··· ,w n ), where s i is the i-th statement and w j is The output should include two fields as relevant and rele-\nthej-thtokeninthegeneratedanswer.Inaddition,forlong- vant reason, as depicted in Figure 2. If all of the retrieved\nformQAsettings,eachstatements i shouldcitealistofdoc- documents are irrelevant, the model should provide an an-\numentsC = {c(1),c(2),...},wherec(k) ∈ D.Inourwork, swer based on the internal knowledge acquired during its\ni i i i\nwetrainanLLM(e.g.LLaMA2)tofirstgeneratereasoning pre-trainingphase.Wedefinetheself-reasoningtrajectories", "metadata": {"id": "e675da89e2d5bd42d2f8a87ec8b0ccbb818fb928", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 3, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "trajectoriesτ throughself-reasoningandthentogeneratean- generatedbyRAPasτ r .\nswersy∗(short-formanswers)onconditionofτ.Themodel\noutput is y = concat(τ,y∗), which is the concatenation of Evidence-AwareSelectiveProcess\nτ andy∗.Notethatthegenerationsofτ andy∗ aredonein When answering a question, people generally first identify\nasinglepasswithintheSELF-REASONINGframework. thecrucialsentencesfromtheprovideddocumentsandthen\ncite or highlight them as key points. This process of cit-\nMethod ingthedocumentfacilitatesreadingcomprehensionandcan\nserve as a technique for combining multiple short answers", "metadata": {"id": "e9d6e662776e7ea3806aa364160b138e357284f6", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 3, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "serve as a technique for combining multiple short answers\nHere we provide a detailed implementation of the self- toaddressvariousaspects.Whilepeoplemaycarryoutthis\nreasoning process which involves three processes: 1) a selectiveprocessandcitationinstantaneously,LLMsneedto\nRelevance-AwareProcess(RAP),2)anEvidence-AwareSe- formulatetheself-reasoningtrajectoriesexplicitly.\nlectiveProcess(EAP),and3)aTrajectoryAnalysisProcess In our work, we require the LLM to explicitly state the\n(TAP).AnillustrationofourSELF-REASONINGframework reasonwhytheselectedsentenceissupportiveandplausible", "metadata": {"id": "577f121f3e66445fc066a23d5869e4d4bcf959f1", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 3, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "isshowninFigure2.Additionally,weoutlinetheprocessof in answering the question. We define the selected sentence\ndatagenerationandqualitycontrol,andpresentthespecifics as evidence in our paper. Specifically, after retrieving the\nofmodeltraining. top-k documents, the self-reasoning method for Evidence-", "metadata": {"id": "242d7a238f7bcd3367051a543f2355e2e91e0476", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 3, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "AwareSelectiveProcesscanbeformulatedasfollows:First, tion:1)Thefirstmethodistousetheoff-the-shelftools1 in\nweinstructtheLLMtochooserelevantdocumentsandau- Gao et al. (2023b) to automatically verify the performance\ntomaticallyselectsnippetsofkeysentencesfortheselected ofdatagenerationfordocumentcitations.Wecalculatethe\ndocuments.Then,werequesttheLLMtooutputthereason citation precision and recall score for each training sample\nwhy the selected snippets can answer the question. The in- and filter out scores lower than our pre-defined thresholds", "metadata": {"id": "9991cdcb53006cd7856ddacc440730445b643093", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 4, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "termediateoutputisalistcontainingmultiplecontents,each δ and δ , for citation precision and recall, respectively. 2)\np r\ncontent should include two fields, as cite content and rea- Second, though the validation of self-reasoning trajectories\nson for cite, which is illustrated in Figure 2. We define the and citations generated by GPT-4 is challenging, verifying\nself-reasoningtrajectoriesgeneratedbyEAPasτ . thecorrectnessofthefinalanswerisstraightforward.There-\ne\nfore, we filter out the trajectories that lead to the incorrect\nanswersandonlykeepthecorrectones.Wetotallygenerate", "metadata": {"id": "cba8f27d1445e9f34347ce5937fd851675512675", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 4, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "answersandonlykeepthecorrectones.Wetotallygenerate\nTrajectoryAnalysisProcess\n10,000 training samples by GPT-4, after the filtering strat-\nFinally,weconsolidatealltheself-reasoningtrajectories(τ egy by quality control, we finally keep 2,000 training sam-\nr\nand τ ) in the previous processes together to form a chain ples with high quality. More details and pseudo-codes can\ne\nofreasoningsnippets,therebyenhancingtheoverallperfor- befoundintheAppendix.\nmanceoftheretrievalaugmentationgeneration.Specifically,\nweasktheLLMtoanalyzethereasoningtrajectorieswithin ModelTraining", "metadata": {"id": "1cc3d332f0e8fb183addf5ace17e85541dde9db6", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 4, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "weasktheLLMtoanalyzethereasoningtrajectorieswithin ModelTraining\nitselfandultimatelytooutputaconciseanalysisandashort Wetraintheself-reasoningRAGmodelϕbyourconstructed\nanswer. We instruct the LLM to output content with two corpus which is augmented with self-reasoning trajectories\nfields as analysis and answer, which is shown in Figure 2. τ usingthestandardlanguagemodelingobjective,maximiz-\nWe define the self-reasoning trajectories generated by TAP inglikelihood:\nasτ .Inthiswork,theanalysisoutputisdefinedasalong-\nform a answer, and the answer output is defined as a short- m\nϕ", "metadata": {"id": "aca8131a948028e0a4b0526411e81a4480b6447f", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 4, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "form a answer, and the answer output is defined as a short- m\nϕ\naxE (q,τ,y)∼Dsr logp ϕ (y |τ,q)p ϕ (τ |q) (1)\nformanswer.Intheexperimentsection,wefurtherexplored\ntheperformanceoflong-formandshort-formQAsettings. whereτ =τ r ⊕τ e ⊕τ a aretheself-reasoningtrajectories,⊕\nisaconcatenationoperator,τ ,τ ,τ aretrajectoriesgener-\nr e a\natedbyabovethreeprocessesrespectively.qistheprovided\nDataGenerationandQualityControl\nquestion,andyisthemodeloutput,includingtheintermedi-\nTraining Data Generation. For the Relevance-Aware atereasontrajectoriesandthefinalanswer.D sr isthetrain-", "metadata": {"id": "a83c50df1a4b2ea18e9b87c9d647dec3516c0e94", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 4, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "Process data generation, as manually labeling the relevant ingcorpusaugmentedwithself-reasoningtrajectories.\nandirrelevantdocumentsislabel-intensive,werequestGPT- During training, we observed that it is more challenging\n4 (OpenAI 2023) to generate answers as ground truth. to ensure the correctness of an LLM with 13B parameters\nSpecifically,weinstructGPT-4togeneratelabelsregarding whengeneratinglongreasoningtrajectoriesthanshortones.\nirrelevant fields, and further to output the reasons why the WehypothesizethatanLLM’seffectivereasoninglengthis", "metadata": {"id": "d2c1ce8723bbc4c7252fae55b26d2075b4ea1859", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 4, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "given documents cannot answer the question. We concate- limitedandexceedingthislimitmightleadtoerroraccumu-\nnatethegivenquestionandtheretrieveddocumentsaspos- lation during the inference stage. Therefore, we propose a\nitive samples. For negative samples, we randomly select a gradual training method by employing stage-wise masking\ndifferentquestionfromthetrainingsetandretrievethetop-k strategiestograduallylearntogeneratelongtrajectories.\ndocumentsrelatedtoit.Thesedocumentsarethenconcate- Specifically, we propose a stage-wise training process", "metadata": {"id": "a97e1e6a9e1156310cc117afbdaa8d2985168125", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 4, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "natedwiththeinitialquestiontoformnegativesamples.To whilewetraintheLLMstagebystage.Inthefirststage,we\navoidorderbiasinthetrainingdata,weshuffletheorderof maskthetrajectoriesproducedbythenexttwostages(EAP\nthedocuments. andTAP)andtrainthemodelwithalearningrater a .Then\ninthesecondstage,weonlymaskthetrajectoriesgenerated\nFortheEAPandTAPdatageneration,manuallyannotat-\nbyTAPandtrainthemodelwithalearningrater .Finally,\ning the citation and writing the self-reasoning process for b\nweconcatenatethereasoningtrajectoriesfromallstagesand\neachquestionisnotfeasibleinpractice.Therefore,wefol-", "metadata": {"id": "4587fc9b87c8bec2b64c6dbf458ab66f90d4bf6d", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 4, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "eachquestionisnotfeasibleinpractice.Therefore,wefol-\nputthemintoaself-reasoningLLMforend-to-endtraining\nlow a similar process to RAP, we first instruct GPT-4 to\nwith a learning rate r . Hyper-parameters for training are\ngenerate a snippet of selected documents and subsequently c\ndescribedintheAppendix.\noutputthereasoningprocessastrajectories.Themethodfor\nconstructing the EAP training data is the same as RAP ex-\nExperiments\ncept that the instructions given to GPT-4 are different. The\ndetailsoftheinstructionsareshownintheAppendix. DatasetsandSettings", "metadata": {"id": "eaa583507aa270013735279b90acee4061afeff8", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 4, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "detailsoftheinstructionsareshownintheAppendix. DatasetsandSettings\nTo demonstrate the effectiveness of our proposed SELF-\nData Quality Control. For training data generation, cor-\nREASONING framework, we conduct an extensive experi-\nrect and comprehensive reasoning trajectories are very im-\nmental evaluation on two short-form QA datasets (Natu-\nportant. When training an LLM, the quality of the train-\nralQuestion(Kwiatkowskietal.2019)andPopQA(Mallen\ning samples is more important than the quantity (Zhou\net al. 2023)), one long-form QA dataset (ASQA (Stelmakh", "metadata": {"id": "5ca69ac515f846402fa657a6944ebb9b2f134ddc", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 4, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "et al. 2023)), one long-form QA dataset (ASQA (Stelmakh\netal.2023).Aswecannotguaranteethecorrectnessofself-\nreasoning trajectories and citations by GPT-4, we develop 1Toolsareavailableathttps://github.com/princeton-nlp/ALCE/\ntwoefficientmethodstocontrolthequalityofdatagenera- tree/main", "metadata": {"id": "50f015c3b10ab044fdb27c3646cff92530b5869f", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 4, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "NaturalQuestion PopQA FEVER ASQA\nModels\n(acc) (acc) (acc) (em-recall) (precision) (recall)\nBaselineswithoutretrieval\nLLaMA2 19.2 18.4 23.2 10.2 - -\n7B\nLLaMA2 24.0 22.6 25.3 15.3 - -\n13B\nLLaMA2 20.2 21.5 26.5 16.3 - -\n7B-chat\nLLaMA2 23.2 25.9 28.4 18.3 - -\n13B-chat\nBaselineswithretrieval\nLLaMA2 27.8 47.8 39.8 28.5 13.6 9.59\n7B\nLLaMA2 34.0 48.1 35.2 26.8 21.8 16.3\n13B\nLLaMA2 27.4 52.9 43.4 25.3 34.5 33.2\n7B-chat\nLLaMA2 32.7 53.5 53.4 26.4 39.4 38.4\n13B-chat\nVicuna (Chiangetal.2023) 28.0 55.2 62.4 24.3 45.7 40.8\n7B\nVicuna (Chiangetal.2023) 35.4 56.1 60.6 27.3 51.3 50.2\n13B", "metadata": {"id": "08f153faea79220b438acdf32df73e8b3f45bffc", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 5, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "7B\nVicuna (Chiangetal.2023) 35.4 56.1 60.6 27.3 51.3 50.2\n13B\nLLaMA2-FT 36.8 54.4 67.5 28.5 47.2 45.4\n7B\nReAct(Yaoetal.2023) - - 64.6 - - -\nRECOMP(Xuetal.2024) 38.4 - - - - -\nSelf-RAG (Asaietal.2024) 37.2 54.9 70.2 30.0 66.9 67.8\n7B\nSelf-RAG (Asaietal.2024) 38.8 55.8 72.1 31.7 70.3 71.3\n13B\nSELF-REASONING7B 38.0 54.2 78.6 33.9 66.3 70.8\nSELF-REASONING13B 41.4 57.3 83.9 35.2 71.2 72.3\nGPT-4 46.6 62.5 87.7 41.3 75.6 68.5\nTable1:Performancecomparisonswithdifferentbaselinemodelsontwoshort-formQAdatasets,along-formQAdataset,and", "metadata": {"id": "d436845e3fa84795fd28868917761152b6205c30", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 5, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "a fact verification dataset. The numbers with bold black represent the best results excluding GPT-4. The results are averaged\noverfiveruns,andpresentedwithstandardvariancevaluesomitted(all≤2%).\net al. 2022)), and one fact verification dataset (FEVER Fact verification metrics. For the fact verification task,\n(Thorne et al. 2018)). Detailed descriptions of the datasets we report the accuracy as a metric, which is a three-class\ncan be found in the Appendix. We explore off-the-shelf classificationaccuracy,followingThorneetal.(2018).\nretrievers. We use DPR (Karpukhin et al. 2020) and", "metadata": {"id": "b496c77ae3fc853fb3d1e7c0316cd3d799116c97", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 5, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "retrievers. We use DPR (Karpukhin et al. 2020) and\nContriever-MSMARCO(Izacardetal.2021)toretrievethe BaselineModels\ntopfivedocumentsfromWikipedia. Baseline models without retrieval. We evaluate strong\nBydefault,weuseDPRasaretrieverfortheNQ,asDPR open-sourcepre-trainedLLMsasbaselinemodels.Forbasic\nhas been fine-tuned on the high-quality NQ data. On the LLMs,wetestLLaMA2-7B,LLaMA2-13B(Touvronetal.\nPopQA,wherequestionandanswerpairsarecreatedbased 2023)anditsinstruction-tunedchatversionLLaMA2-Chat-\nonWikipediain2022,therefore,forthePopQA,weusethe 7B,LLaMA2-Chat-13B.", "metadata": {"id": "04f673b496c78de7228f176d1797501b6f65d32a", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 5, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "onWikipediain2022,therefore,forthePopQA,weusethe 7B,LLaMA2-Chat-13B.\nDecember2020preprocessedWikipediacorpusprovidedby\nBaselinemodelswithretrieval. First,webenchmarkthe\n(Izacardetal.2022)anduseContrieverasaretriever.Forthe\nmodels using the LLaMA2 and the Vicuna (Chiang et al.\nASQA dataset, we use GTR (Ni et al. 2022) as a retrieval\n2023) series models for baselines. Additionally, for a fair\nthat corresponds to the experimental settings in (Gao et al.\ncomparison,wealsoincludeLLaMA2-FT,whereLLaMA2\n2023b).MoresettingscanbefoundintheAppendix.\nisfine-tunedonallthetrainingsamplesgeneratedbyGPT-", "metadata": {"id": "46486c4e72b0f8ed1f6407774742dc9c96116755", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 5, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "2023b).MoresettingscanbefoundintheAppendix.\nisfine-tunedonallthetrainingsamplesgeneratedbyGPT-\n4 except the self-reasoning trajectories. To establish strong\nEvaluationMetrics\nbaselines, we compare our method against RECOMP (Xu\nWeusedifferentevaluationmetricsforshort-formQA,long- et al. 2024), ReAct (Yao et al. 2023), and Self-RAG (Asai\nformQA,andfactverificationtasks. etal.2024),allofwhicharetrainedwithextraGPT-4gener-\natedsamplesorexternaltools.Wealsocompareourframe-\nShort-form QA metrics. We report accuracy for short-\nwork with GPT-4 (OpenAI 2023). We include categorical", "metadata": {"id": "d1f66dde1d271b30ce1da6d1a773168b3fc6821e", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 5, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "work with GPT-4 (OpenAI 2023). We include categorical\nform QA tasks, which is based on whether ground-truth\ncomparisonswiththebaselinemodelsintheAppendix.\nanswers are included in the model predictions instead of\nstrictly requiring exact matching, following Mallen et al.\nMainResults\n(2023);Schicketal.(2023).\nTable 1 shows the performance comparisons with different\nLong-formQAmetrics. Forlong-formQAtasks,were- methods on the four public datasets. For short-form QA\nport the EM recall as a correctness metric, and the citation evaluations, the performance of LLMs with augmented re-", "metadata": {"id": "e3aedeaaeefbca2b2fc579663bab2fb271ec1f36", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 5, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "recall and the citation precision for citation quality, which trieval is consistently better than that of basic ones, affirm-\narethesameasthemetricsin(Gaoetal.2023b). ing the effectiveness of the augmented approach. Notably,", "metadata": {"id": "ca46fd0badca83b82c3a4685ec5c94a0d7b277bc", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 5, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "Figure3:Noiserobustnessexperimentresultsonthreedifferentdatasets:(a)OntheleftistheNQdataset,(b)inthemiddleis\nthePopQAdataset,and(c)ontherightistheFEVERdataset.TheSelf-RAGandVicunaare13Bparametersizemodels.\nunder the same order of magnitude parameters, our SELF- NQ PopQA FEVER\nModels\nREASONING framework outperforms most of the strong\n(acc) (acc) (acc)\nbaselineLLMs.Specifically,comparedtotheSelf-RAG,our\nframeworkisanend-to-endsystemtrainedwithonly2,000 ORIGIN 41.4 57.3 83.9\nself-reasoningtrajectorysamples.Incontrast,theSelf-RAG w/o(RAP) 39.9 54.3 72.2\nw/o(EAP) 37.2 53.2 78.4", "metadata": {"id": "17687565fdfff1a80e50cad613d29d4f58cb9491", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 6, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "w/o(EAP) 37.2 53.2 78.4\nrequires training additional critic LMs to predict reflection\nw/o(TAP) 38.2 53.4 81.2\ntokens using an additional 46,000 instances generated by\nw/o(GL) 39.5 55.3 81.2\nGPT-4. This efficiency not only simplifies the training pro-\nw/o(QC) 37.7 54.2 80.8\ncessbutalsosignificantlyreducesresourceconsumption.\nInthecontextoflong-formQAevaluations,forthemet-\nTable2:Theablationstudyontwoshort-formQAdatasets\nrics of EM recall, it needs to comprehend multiple docu-\nandafactverificationdatasetwith13Bparametersizemod-\nments and merge answers. The EAP and TAP are specifi-", "metadata": {"id": "4b3dd65131cb54f06f883c87027498e2978e2deb", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 6, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "ments and merge answers. The EAP and TAP are specifi-\nels. In the table, the ORIGIN represents our self-reasoning\ncally designed for multi-document reading comprehension,\nmodelenhancedwithself-generatedtrajectories.\nenabling our performance to surpass other baselines. In\nterms of citation evaluation metrics, the SELF-REASONING\nRAG can achieve better results than GPT-4 in ASQA cita- NQ PopQA FEVER\nModels\ntionrecallmetrics(72.3vs.68.5).Thisislargelyduetothe\n(acc) (acc) (acc)\nreasoning trajectories generated in the EAP, which can en-\nLLaMA2 32.7 53.5 53.4", "metadata": {"id": "f032210d6e32b74ef176191f7e04dc393884e049", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 6, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "(acc) (acc) (acc)\nreasoning trajectories generated in the EAP, which can en-\nLLaMA2 32.7 53.5 53.4\nhancetherecallandprecisionofcitationevaluation,leading\n+trajectory 38.3 54.2 79.2\ntomoreinterpretableandtraceablegenerations.\nVicuna 35.4 56.1 60.6\nForfactverificationevaluations,weobservedthat SELF-\n+trajectory 38.5 56.4 79.6\nSEASONING is dominantly superior to all baseline models.\nOur method achieves a much higher accuracy rate than the\nTable3:Theanalysisontheeffectivenessofself-reasoning\nSelf-RAGmodel(83.9vs.72.1).TheRAPinourframework\ntrajectories with 13B parameter size models. In the table,", "metadata": {"id": "aad58bf241f8cd537ad1df6843417e81bdd8f7fa", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 6, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "trajectories with 13B parameter size models. In the table,\nisdesignedtojudgetherelevancebetweentheretrieveddoc-\nthe+trajectoryindicatestheresultofthebaselinemodelis\numentsandthequestion,whichleadstoanotableenhance-\nenhancedwithself-generatedtrajectoriesbyourframework.\nmentinaccuracyforthisfactverificationtask.\nToclearlydemonstratethepracticalapplicationsandben-\nefitsofourSELF-REASONINGframework,weprovideacase\nity control (QC) of data generation (a detailed analysis de-\nstudyforamorein-depthanalysisinAppendix,whichillus-\nscribed in the Appendix). The main ablation study results", "metadata": {"id": "13e9810ba3b518db4833eb4b19d83790fc6186af", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 6, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "scribed in the Appendix). The main ablation study results\ntrateshowourframeworkoperatesinreal-worldscenarios.\nareshowninTable2andTable3.\nAnalysis Effectiveness of RAP. First, we evaluate the effect of\nthe RAP. The removal of the RAP causes the overall per-\nAblationStudy\nformance to drop in two short-form QA datasets and a\nWeconductanablationstudyontwoshort-formQAdatasets fact verification dataset, suggesting that preliminary con-\nand a fact verification dataset to analyze the individual siderationoftherelevancebetweenquestionsandretrieved", "metadata": {"id": "2c7378aee94fc11b866b4b419d342c904f23421f", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 6, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "contributions of each process within our proposed SELF- documents can help improve performance. We notice that\nREASONING framework. We further explore the effective- the performance declines most significantly in the FEVER\nness of the gradual learning (GL) method and the qual- dataset.Detectingirrelevantdocumentsiscriticalinthefact-", "metadata": {"id": "a07127c78cabd31917740f4eebd004340d0c16a1", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 6, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "verificationtask.OurmodelwillimmediatelyoutputNotE-\nnoughInfoifitdetectsthatalldocumentsareirrelevant.\nEffectivenessofEAP. Thenweevaluatetheeffectofthe\nEAP.RemovingtheEAPcausestheoverallperformanceof\nthe average accuracy to decline from 60.9 to 56.3 in three\nshort-formQAdatasets,whichindicatesthatsnippetsofkey\nsentences and document citations generated through self-\nreasoningareinstrumentalinboostingaccuracy.\nEffectiveness of TAP. Finally, we evaluate the effect of\nthe TAP. When excluding the TAP, we can observe a per-\nformance decline on all three datasets, demonstrating that", "metadata": {"id": "921e37efb5e79b94fd3120c9fc5aeb58740253b4", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 7, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "formance decline on all three datasets, demonstrating that\nself-analysisbasedontwopreviousprocessesgeneratedtra-\njectories can also improve the performance of LLMs. Note\nthattheanalysiscontentgeneratedbyTAPisindispensable\nforthelong-formQAevaluation.\nEffectiveness of Self-Reasoning Trajectory. To verify\nwhether the trajectories generated by the self-reasoning\nframeworkaretrulyeffective,weputthetrajectoriesgener-\nated by our SELF-REASONING framework into the original\nbaselinemodelsasinputprompts,andthenusethebaseline\nmodelstoregeneratetheanswers.Weobservethatincorpo-", "metadata": {"id": "20753260e43027c384610902bf9e7d186ca4f8a5", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 7, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "modelstoregeneratetheanswers.Weobservethatincorpo-\nrating self-generated trajectories can significantly enhance\nperformanceinshortQAtasksandfactverificationtasks. Figure 4: Human citation quality evaluation vs. automatic\ncitationevaluationonthelong-formASQAdataset.\nRetrievalRobustnessAnalysis\nRetrieversarenotperfectandpastworkhasshownthatnoisy\nretrieval can have negative effects on the performance of givenastatementandalldocumentsthatthestatementrefers\nLLMs (Petroni et al. 2020; Li et al. 2023). In this section, toandareaskedtojudgewhetherthedocumentsfullysup-", "metadata": {"id": "b37986c06705c8163fae4eb46fc148162bcb30b4", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 7, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "we design two kinds of settings to validate the robustness\nportthegivenstatement;2)citationprecision:givenastate-\nofRALMs.Inthefirstsetting,wetestwhethertheorderof ment and one of its citations, annotators are asked to vali-\nthe retrieved documents will affect the performance of the date whether the citation fully supports, partially supports\nRALMs. Specifically, after retrieving the top-k documents\nordoesnotsupportthestatement.AsshowninFigure4,the\nusing retrievals with a descending relevance score, we ran- relativerankingsbyhumanevaluationalignwellwiththose", "metadata": {"id": "7b6e77c027e661e6dd1265a2ccb690deca75acff", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 7, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "domlyshuffletheorderoftheretrieveddocumentsandthen fromtheautomaticevaluation,andthehumanevaluationof-\ninput them to an LLM. In the second setting, we test how ten yields a closely higher score when compared with the\nnoisy documents impact the performance of LLMs. When automatic evaluation. Details of human annotation can be\nretrievingthetop-k documentsfromthegivenquestion,we foundintheAppendix.\nrandomlyreplace50%oftheretrieveddocumentswithother\nLatencyAnalysis\ndocumentssampledfromadifferentquestioninthedataset.", "metadata": {"id": "88cb2d820f07e266366db3e819c1b4a1039dfdc6", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 7, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "LatencyAnalysis\ndocumentssampledfromadifferentquestioninthedataset.\nFigure 3 shows the noise robustness experiment results We also compared the inference latency of SELF-\non three datasets. Our SELF-REASONING framework con- REASONING RAG with that of Self-RAG and GPT-4. The\nsistentlyoutperformstheSelf-RAGandVicunamodels.We resultsshowthatourmethodmaintainscomparablelatency\nobservethatrandomshufflingofretrieveddocumentshasa toSelf-RAGwhiledeliveringsubstantialperformancegains.\nminimalimpactontheperformanceofRALMs.Ifthepro- DetailedresultsareavailableintheAppendix.", "metadata": {"id": "56e9ab7089d309776da5850fce72d8d34df57786", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 7, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "minimalimpactontheperformanceofRALMs.Ifthepro- DetailedresultsareavailableintheAppendix.\nvided documents are supportive, it is trivial for a RALM\nto determine the correct answer. However, when presented Conclusion\nwith noisy documents, all models experience a decline in RALMs can effectively enhance the performance of LLMs\nperformance. The performance drop in our self-reasoning in handling knowledge-intensive tasks. Despite their effec-\nframework is relatively minimal, demonstrating the robust- tiveness, notable concerns about their reliability and trace-", "metadata": {"id": "6e6083cb0abe5d4a60691d46486edeb16daad185", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 7, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "nessofourmethodevenwhenhandlingnoisydocuments. ability persist. To address these limitations, we propose a\nnovel SELF-REASONING framework to improve the perfor-\nCitationAnalysis\nmanceofRALMsbyusingreasoningtrajectoriesgenerated\nAs the automatic evaluation by the NLI model cannot de- bytheLLMitself.Itiscomprisedofarelevance-awarepro-\ntectpartiallysupportedcitations,wediscusstheanalysisof cess, an evidence-aware selective process, and a trajectory\ncitationswithhumanevaluationinthissection.Similarlyto analysisprocess.Weconductextensiveexperimentsonfour", "metadata": {"id": "c6dbcedd3679247c676359868faf4e40e05615de", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 7, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "Liu, Zhang, and Liang (2023), we conduct a human eval- publicdatasetstodemonstratethesuperiorityofourframe-\nuationontwodimensions:1)citationrecall:annotatorsare workoverexistingstate-of-the-artmodels.", "metadata": {"id": "c53cc5d3c34999d487b8aa03c57849abbf2a5798", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 7, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "References Jiang,Z.;Xu,F.;Gao,L.;Sun,Z.;Liu,Q.;Dwivedi-Yu,J.;\nAsai, A.; Wu, Z.; Wang, Y.; Sil, A.; and Hajishirzi, H. Yang,Y.;Callan,J.;andNeubig,G.2023. ActiveRetrieval\n2024. Self-RAG: Learning to Retrieve, Generate, and Cri- AugmentedGeneration. InBouamor,H.;Pino,J.;andBali,\ntiquethroughSelf-Reflection. InTheTwelfthInternational K.,eds.,Proceedingsofthe2023ConferenceonEmpirical\nConferenceonLearningRepresentations. MethodsinNaturalLanguageProcessing,7969–7992.Sin-\ngapore.\nBaek,J.;Jeong,S.;Kang,M.;Park,J.;andHwang,S.2023.", "metadata": {"id": "9e204d91ef2501bf3aa63e33a2322535b63f40ec", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 8, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "gapore.\nBaek,J.;Jeong,S.;Kang,M.;Park,J.;andHwang,S.2023.\nKnowledge-Augmented Language Model Verification. In Karpukhin,V.;Oguz,B.;Min,S.;Lewis,P.;Wu,L.;Edunov,\nBouamor, H.; Pino, J.; and Bali, K., eds., Proceedings of S.; Chen, D.; and Yih, W.-t. 2020. Dense Passage Re-\nthe2023ConferenceonEmpiricalMethodsinNaturalLan- trieval for Open-Domain Question Answering. In Webber,\nguageProcessing,1720–1736.Singapore. B.; Cohn, T.; He, Y.; and Liu, Y., eds., Proceedings of the\n2020 Conference on Empirical Methods in Natural Lan-\nBorgeaud, S.; Mensch, A.; Hoffmann, J.; Cai, T.; Ruther-", "metadata": {"id": "4cc20bff2720e44b14c44026dda5ae0c0bae74fd", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 8, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "Borgeaud, S.; Mensch, A.; Hoffmann, J.; Cai, T.; Ruther-\nguageProcessing(EMNLP),6769–6781.Online.\nford, E.; Millican, K.; Van Den Driessche, G. B.; Lespiau,\nJ.-B.;Damoc,B.;Clark,A.;etal.2022.Improvinglanguage Kwiatkowski, T.; Palomaki, J.; Redfield, O.; Collins, M.;\nmodels by retrieving from trillions of tokens. In Interna- Parikh, A.; Alberti, C.; Epstein, D.; Polosukhin, I.; Devlin,\ntionalconferenceonmachinelearning,2206–2240.PMLR. J.;Lee,K.;Toutanova,K.;Jones,L.;Kelcey,M.;Chang,M.-\nW.; Dai, A. M.; Uszkoreit, J.; Le, Q.; and Petrov, S. 2019.\nBrown,T.;Mann,B.;Ryder,N.;Subbiah,M.;Kaplan,J.D.;", "metadata": {"id": "e7be87b58108b3e44cdf23c6e45ef57ad20b072c", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 8, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "Brown,T.;Mann,B.;Ryder,N.;Subbiah,M.;Kaplan,J.D.;\nNatural Questions: A Benchmark for Question Answering\nDhariwal,P.;Neelakantan,A.;Shyam,P.;Sastry,G.;Askell,\nA.;etal.2020. Languagemodelsarefew-shotlearners. Ad- Research. Transactions of the Association for Computa-\nvancesinneuralinformationprocessingsystems,33:1877–\ntionalLinguistics,7:452–466.\n1901. Kwon, W.; Li, Z.; Zhuang, S.; Sheng, Y.; Zheng, L.; Yu,\nChiang,W.-L.;Li,Z.;Lin,Z.;Sheng,Y.;Wu,Z.;Zhang,H.; C. H.; Gonzalez, J. E.; Zhang, H.; and Stoica, I. 2023. Ef-", "metadata": {"id": "8285318d8efb90ea277fcb556bc0e2c6722dccbf", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 8, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "Zheng,L.;Zhuang,S.;Zhuang,Y.;Gonzalez,J.E.;Stoica, ficient Memory Management for Large Language Model\nI.;andXing,E.P.2023. Vicuna:AnOpen-SourceChatbot Serving with PagedAttention. In Proceedings of the ACM\nImpressingGPT-4with90%*ChatGPTQuality. SIGOPS29thSymposiumonOperatingSystemsPrinciples.\nGao,L.;Dai,Z.;Pasupat,P.;Chen,A.;Chaganty,A.T.;Fan, Lewis, P.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin, V.;\nY.; Zhao, V.; Lao, N.; Lee, H.; Juan, D.-C.; and Guu, K. Goyal, N.; Ku¨ttler, H.; Lewis, M.; Yih, W.-t.; Rockta¨schel,", "metadata": {"id": "dfe884fc24c82aa74d9ec81ea7abbc61fd09f545", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 8, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "2023a. RARR: Researching and Revising What Language T.; et al. 2020. Retrieval-augmented generation for\nModelsSay,UsingLanguageModels. InRogers,A.;Boyd- knowledge-intensive nlp tasks. Advances in Neural Infor-\nGraber, J.; and Okazaki, N., eds., Proceedings of the 61st mationProcessingSystems,33:9459–9474.\nAnnual Meeting of the Association for Computational Lin- Li, D.; Rawat, A. S.; Zaheer, M.; Wang, X.; Lukasik, M.;\nguistics (Volume 1: Long Papers), 16477–16508. Toronto, Veit, A.; Yu, F.; and Kumar, S. 2023. Large Language\nCanada. ModelswithControllableWorkingMemory. InRogers,A.;", "metadata": {"id": "9f92360f34fb37a3cffb383a11bca910ba01969b", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 8, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "Canada. ModelswithControllableWorkingMemory. InRogers,A.;\nGao, T.; Yen, H.; Yu, J.; and Chen, D. 2023b. Enabling Boyd-Graber,J.;andOkazaki,N.,eds.,FindingsoftheAs-\nLarge Language Models to Generate Text with Citations. sociation for Computational Linguistics: ACL 2023, 1774–\nIn Bouamor, H.; Pino, J.; and Bali, K., eds., Proceedings 1793.Toronto,Canada.\nof the 2023 Conference on Empirical Methods in Natural Liu,N.;Zhang,T.;andLiang,P.2023. EvaluatingVerifia-\nLanguageProcessing,6465–6488.Singapore. bilityinGenerativeSearchEngines. InBouamor,H.;Pino,", "metadata": {"id": "93a3cad4d319a12e86616bc205e4f0a3074e9aec", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 8, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "LanguageProcessing,6465–6488.Singapore. bilityinGenerativeSearchEngines. InBouamor,H.;Pino,\nGuu, K.; Lee, K.; Tung, Z.; Pasupat, P.; and Chang, M.-W. J.;andBali,K.,eds.,FindingsoftheAssociationforCompu-\n2020. REALM: retrieval-augmented language model pre- tationalLinguistics:EMNLP2023,7001–7025.Singapore.\ntraining. In Proceedings of the 37th International Confer- Mallen, A.; Asai, A.; Zhong, V.; Das, R.; Khashabi, D.;\nenceonMachineLearning,ICML’20.JMLR.org. and Hajishirzi, H. 2023. When Not to Trust Language", "metadata": {"id": "febff3a1d89df41c082dc7b9d6bb6ac07d76d14a", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 8, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "enceonMachineLearning,ICML’20.JMLR.org. and Hajishirzi, H. 2023. When Not to Trust Language\nHonovich,O.;Aharoni,R.;Herzig,J.;Taitelbaum,H.;Kuk- Models:InvestigatingEffectivenessofParametricandNon-\nliansy, D.; Cohen, V.; Scialom, T.; Szpektor, I.; Hassidim, ParametricMemories. InRogers,A.;Boyd-Graber,J.;and\nA.;andMatias,Y.2022. TRUE:Re-evaluatingfactualcon- Okazaki, N., eds., Proceedings of the 61st Annual Meeting\nsistencyevaluation. arXivpreprintarXiv:2204.04991. oftheAssociationforComputationalLinguistics(Volume1:", "metadata": {"id": "9696c76bebac4cc79b01b6731926a79a453ad0d8", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 8, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "Izacard,G.;Caron,M.;Hosseini,L.;Riedel,S.;Bojanowski, LongPapers),9802–9822.Toronto,Canada.\nP.;Joulin,A.;andGrave,E.2021. Unsuperviseddensein- Menick, J.; Trebacz, M.; Mikulik, V.; Aslanides, J.; Song,\nformationretrievalwithcontrastivelearning. arXivpreprint F.; Chadwick, M.; Glaese, M.; Young, S.; Campbell-\narXiv:2112.09118. Gillingham, L.; Irving, G.; et al. 2022. Teaching language\nIzacard,G.;Lewis,P.;Lomeli,M.;Hosseini,L.;Petroni,F.; models to support answers with verified quotes. arXiv\nSchick,T.;Dwivedi-Yu,J.;Joulin,A.;Riedel,S.;andGrave, preprintarXiv:2203.11147.", "metadata": {"id": "5851ec57ae5e85f9d2d214509774d92d91a9d984", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 8, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "Schick,T.;Dwivedi-Yu,J.;Joulin,A.;Riedel,S.;andGrave, preprintarXiv:2203.11147.\nE. 2022. Few-shot learning with retrieval augmented lan- Min, S.; Michael, J.; Hajishirzi, H.; and Zettlemoyer, L.\nguagemodels. arXivpreprintarXiv:2208.03299. 2020. AmbigQA: Answering Ambiguous Open-domain\nJi,Z.;Lee,N.;Frieske,R.;Yu,T.;Su,D.;Xu,Y.;Ishii,E.; Questions.InWebber,B.;Cohn,T.;He,Y.;andLiu,Y.,eds.,\nBang,Y.J.;Madotto,A.;andFung,P.2023. Surveyofhal- Proceedingsofthe2020ConferenceonEmpiricalMethods\nlucinationinnaturallanguagegeneration. ACMComputing inNaturalLanguageProcessing(EMNLP),5783–5797.On-", "metadata": {"id": "955d874cbaa83c8c8edc1983385a2db30eea12ff", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 8, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "Surveys,55(12):1–38. line.", "metadata": {"id": "0f57a9394b541b8cbda2b2107e39fcba4b1247c8", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 8, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "Nakano,R.;Hilton,J.;Balaji,S.;Wu,J.;Ouyang,L.;Kim, andVERification. InWalker,M.;Ji,H.;andStent,A.,eds.,\nC.; Hesse, C.; Jain, S.; Kosaraju, V.; Saunders, W.; et al. Proceedingsofthe2018ConferenceoftheNorthAmerican\n2021. Webgpt: Browser-assisted question-answering with Chapter of the Association for Computational Linguistics:\nhumanfeedback. arXivpreprintarXiv:2112.09332. Human Language Technologies, Volume 1 (Long Papers),\nNi, J.; Qu, C.; Lu, J.; Dai, Z.; Hernandez Abrego, G.; Ma, 809–819.NewOrleans,Louisiana.", "metadata": {"id": "fb144e01361e67486b30959381c022cf89b32500", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 9, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "Ni, J.; Qu, C.; Lu, J.; Dai, Z.; Hernandez Abrego, G.; Ma, 809–819.NewOrleans,Louisiana.\nJ.;Zhao,V.;Luan,Y.;Hall,K.;Chang,M.-W.;andYang,Y. Touvron,H.;Martin,L.;Stone,K.;Albert,P.;Almahairi,A.;\n2022.LargeDualEncodersAreGeneralizableRetrievers.In Babaei,Y.;Bashlykov,N.;Batra,S.;Bhargava,P.;Bhosale,\nGoldberg,Y.;Kozareva,Z.;andZhang,Y.,eds.,Proceedings S.; et al. 2023. Llama 2: Open foundation and fine-tuned\nof the 2022 Conference on Empirical Methods in Natural chatmodels. arXivpreprintarXiv:2307.09288.", "metadata": {"id": "cf8ea02354fe55d1d53d0296a53f3671c611c9c7", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 9, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "of the 2022 Conference on Empirical Methods in Natural chatmodels. arXivpreprintarXiv:2307.09288.\nLanguageProcessing,9844–9855.AbuDhabi,UnitedArab Trivedi,H.;Balasubramanian,N.;Khot,T.;andSabharwal,\nEmirates. A.2023.InterleavingRetrievalwithChain-of-ThoughtRea-\nOpenAI,R.2023. Gpt-4technicalreport.arxiv2303.08774. soning for Knowledge-Intensive Multi-Step Questions. In\nViewinArticle,2:13. Proceedings of the 61st Annual Meeting of the Associa-\nPan,Z.;Luo,H.;Li,M.;andLiu,H.2024. Chain-of-action: tionforComputationalLinguistics(Volume1:LongPapers),", "metadata": {"id": "e87a29b8f1e098803680c239ea47525c9f533871", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 9, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "Faithful and multimodal question answering through large 10014–10037.\nlanguagemodels. arXivpreprintarXiv:2403.17359. Wang, X.; Wei, J.; Schuurmans, D.; Le, Q.; hsin Chi,\nPetroni, F.; Lewis, P.; Piktus, A.; Rockta¨schel, T.; Wu, E. H.; and Zhou, D. 2022. Self-Consistency Improves\nY.; Miller, A. H.; and Riedel, S. 2020. How context af- Chain of Thought Reasoning in Language Models. ArXiv,\nfects language models’ factual predictions. arXiv preprint abs/2203.11171.\narXiv:2005.04611. Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.;", "metadata": {"id": "e938c34d047ce8d98eeb1c5c31b784b5fe7e0dc4", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 9, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "arXiv:2005.04611. Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.;\nPetroni, F.; Piktus, A.; Fan, A.; Lewis, P.; Yazdani, M.; Chi, E.; Le, Q. V.; Zhou, D.; et al. 2022. Chain-of-\nDe Cao, N.; Thorne, J.; Jernite, Y.; Karpukhin, V.; Mail- thoughtpromptingelicitsreasoninginlargelanguagemod-\nlard, J.; Plachouras, V.; Rockta¨schel, T.; and Riedel, S. els. Advancesinneuralinformationprocessingsystems,35:\n2021. KILT: a Benchmark for Knowledge Intensive Lan- 24824–24837.\nguageTasks.InToutanova,K.;Rumshisky,A.;Zettlemoyer, Xu, F.; et al. 2024. RECOMP: Improving Retrieval-", "metadata": {"id": "841eb3b014064ef2a0944fb5da53834b5230af9b", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 9, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "L.; Hakkani-Tur, D.; Beltagy, I.; Bethard, S.; Cotterell, R.; Augmented LMs with Context Compression and Selective\nChakraborty,T.;andZhou,Y.,eds.,Proceedingsofthe2021 Augmentation. InTheTwelfthInternationalConferenceon\nConference of the North American Chapter of the Associa- LearningRepresentations.\ntionforComputationalLinguistics:HumanLanguageTech-\nXu, S.; Pang, L.; Shen, H.; Cheng, X.; and Chua, T.-s.\nnologies,2523–2544.Online.\n2023. Search-in-the-chain: Towards the accurate, credible", "metadata": {"id": "0e24a2696730dceb959694f030f1081a6cd1c6e8", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 9, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "nologies,2523–2544.Online.\n2023. Search-in-the-chain: Towards the accurate, credible\nPress, O.; Zhang, M.; Min, S.; Schmidt, L.; Smith, N.; and and traceable content generation for complex knowledge-\nLewis, M. 2023. Measuring and Narrowing the Composi- intensivetasks. arXivpreprintarXiv:2304.14732.\ntionality Gap in Language Models. In Bouamor, H.; Pino,\nYan, S.-Q.; Gu, J.-C.; Zhu, Y.; and Ling, Z.-H. 2024.\nJ.;andBali,K.,eds.,FindingsoftheAssociationforCompu-\nCorrective retrieval augmented generation. arXiv preprint\ntationalLinguistics:EMNLP2023,5687–5711.Singapore.\narXiv:2401.15884.", "metadata": {"id": "b40bcee8ebe311f18202577b57b5d0cb1b1a9055", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 9, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "tationalLinguistics:EMNLP2023,5687–5711.Singapore.\narXiv:2401.15884.\nRam,O.;Levine,Y.;Dalmedigos,I.;Muhlgay,D.;Shashua,\nYao, S.; Zhao, J.; Yu, D.; Du, N.; Shafran, I.; Narasimhan,\nA.; Leyton-Brown, K.; and Shoham, Y. 2023. In-Context\nK.; and Cao, Y. 2023. ReAct: Synergizing Reasoning and\nRetrieval-Augmented Language Models. Transactions of\nActing in Language Models. In International Conference\nthe Association for Computational Linguistics, 11: 1316–\nonLearningRepresentations(ICLR).\n1331.\nYoran,O.;Wolfson,T.;Ram,O.;andBerant,J.2023. Mak-", "metadata": {"id": "df3316dae8c467cc5dc7c0c4b2a239af164d0ce8", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 9, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "onLearningRepresentations(ICLR).\n1331.\nYoran,O.;Wolfson,T.;Ram,O.;andBerant,J.2023. Mak-\nRasley, J.; Rajbhandari, S.; Ruwase, O.; and He, Y. 2020.\ning retrieval-augmented language models robust to irrele-\nDeepspeed: System optimizations enable training deep\nvantcontext. arXivpreprintarXiv:2310.01558.\nlearning models with over 100 billion parameters. In Pro-\nYu, W.; Zhang, H.; Pan, X.; Ma, K.; Wang, H.; and\nceedings of the 26th ACM SIGKDD International Confer-\nYu, D. 2023. Chain-of-note: Enhancing robustness in\nenceonKnowledgeDiscovery&DataMining,3505–3506.", "metadata": {"id": "2d5a790ab6b6df5f7b4cc0568f58bb91ddf1d7c8", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 9, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "Yu, D. 2023. Chain-of-note: Enhancing robustness in\nenceonKnowledgeDiscovery&DataMining,3505–3506.\nretrieval-augmented language models. arXiv preprint\nSchick,T.;Dwivedi-Yu,J.;Dess`ı,R.;Raileanu,R.;Lomeli,\narXiv:2311.09210.\nM.; Zettlemoyer, L.; Cancedda, N.; and Scialom, T. 2023.\nZhou,C.;Liu,P.;Xu,P.;Iyer,S.;Sun,J.;Mao,Y.;Ma,X.;\nToolformer: Language models can teach themselves to use\nEfrat,A.;Yu,P.;Yu,L.;etal.2023. Lima:Lessismorefor\ntools. arXivpreprintarXiv:2302.04761.\nalignment. arXivpreprintarXiv:2305.11206.\nStelmakh,I.;Luan,Y.;Dhingra,B.;andChang,M.-W.2022.", "metadata": {"id": "6bab868891a5c2fbc450d9d65b3a00490b23e841", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 9, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "alignment. arXivpreprintarXiv:2305.11206.\nStelmakh,I.;Luan,Y.;Dhingra,B.;andChang,M.-W.2022.\nZhou,D.;Scharli,N.;Hou,L.;Wei,J.;Scales,N.;Wang,X.;\nASQA: Factoid Questions Meet Long-Form Answers. In\nGoldberg,Y.;Kozareva,Z.;andZhang,Y.,eds.,Proceedings Schuurmans, D.; Bousquet, O.; Le, Q.; and hsin Chi, E. H.\nof the 2022 Conference on Empirical Methods in Natural 2022. Least-to-MostPromptingEnablesComplexReason-\nLanguageProcessing,8273–8288.AbuDhabi,UnitedArab inginLargeLanguageModels. ArXiv,abs/2205.10625.\nEmirates.\nThorne,J.;Vlachos,A.;Christodoulopoulos,C.;andMittal,", "metadata": {"id": "798b77917926d7557f88f7dc44f4bc5765ff11fd", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 9, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "Emirates.\nThorne,J.;Vlachos,A.;Christodoulopoulos,C.;andMittal,\nA.2018. FEVER:aLarge-scaleDatasetforFactExtraction", "metadata": {"id": "53aeac842d5fdb3368baddc5039f242a0264ba9c", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 9, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "Appendix FEVER(Thorneetal.2018)isafactverificationdataset\nthat contains claims generated by rewriting sentences ex-\nMoreRelatedWorkofLMsforReasoning\ntracted from Wikipedia and subsequently verified without\nOne of the most well-known methods of using LLMs for\nknowledge of the sentence from which they were derived.\nreasoningistheChain-of-Thought(CoT)(Weietal.2022),\nThe claims are classified as Supported, Refuted, or NotE-\nwhich demonstrates the capability of LLMs to create their\nnoughInfo.\nthinking process for problem-solving. Zhou et al. (2022)", "metadata": {"id": "543bcdf5e6ce7f1060ca014fe2733480e4a24cc9", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 10, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "noughInfo.\nthinking process for problem-solving. Zhou et al. (2022)\nproposes a least-to-most prompting for solving complex\nExperimentsettings\ntasks.Wangetal.(2022)introducesamethodtoreasonwith\nself-consistency. Press et al. (2023) proposes a method to Training settings. During gradual learning, we fine-tune\nfurtherimprovethechainofthoughtbyreasoningexplicitly the LLaMA-2 (Touvron et al. 2023) model with our self-\ninsteadofimplicitly. reasoning framework for 3 epochs with a batch size set to", "metadata": {"id": "e55aa7e74b4acba86e215edb5b5731fb16f8b59e", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 10, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "insteadofimplicitly. reasoning framework for 3 epochs with a batch size set to\nRecent works have extended beyond the internal reason- 32, leveraging the DeepSpeed library (Rasley et al. 2020)\ning ability of LLMs to include interactions with external andtheZeROoptimizer,andweuseparameterpartitioning\ntools (e.g., search engines or retrievers) for solving com- ZeRO stage 3 with float16 precision. The learning rate r a\nplex tasks. The ReAct (Yao et al. 2023) presents an itera- for the first stage is set to 5e-5, the learning rate r b for the", "metadata": {"id": "b53f93a316a6b004c3cc14a83395f148be88540b", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 10, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "tive paradigm to combine reasoning and acting with LLMs second stage is set to 3e-5, and the learning rate r c for the\nfortacklinglanguagereasoninganddecision-makingtasks. finalstageissetto1e-5.OurSELF-REASONING13Bmodel\nXuetal.(2023)introducesaframeworktoenableinforma- istrainedontheNVIDIATesla8×V10032GBGPUfor4\ntion retrieval and LLMs to interact with each other effec- hours,whilethe7Bmodelistrainedfor2hours.\ntivelywithchain-of-querydecomposition.Panetal.(2024)\nInferencesettings. WeusethevLLM2framework(Kwon\nproposesanovelframeworknamedChain-of-Action(CoA),", "metadata": {"id": "d6e920666222fbcb466a26e62d15ec427a64f104", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 10, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "Inferencesettings. WeusethevLLM2framework(Kwon\nproposesanovelframeworknamedChain-of-Action(CoA),\net al. 2023) to accelerate the inference speed during infer-\nwhichintegratesareasoningretrievalmethodtodecompose\nence. The codes follow the Apache-2.0 license agreement.\ncomplexquestionsintochainsofconfigurableactions.\nWeusegreedydecodinginallexperimentstoensuredeter-\nDifferent from the above works, which are mostly\nministicgenerations.Wetestthetemperaturewithinarange\nbased on relatively large LLMs (e.g., ChatGPT), our pro-\nof{0.2,0.4,0.6,0.8,1.0},finallywesetthetemperatureto", "metadata": {"id": "ae5a403e34042298b3fecf743e2108849346e656", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 10, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "of{0.2,0.4,0.6,0.8,1.0},finallywesetthetemperatureto\nposed method focuses on enhancing smaller LLMs (e.g.,\n0.2,asweobservedlowertemperatureresultsinbetterper-\nLLaMA2) using only a limited number of samples to\nformanceintheopen-domainquestionansweringtask.The\nachievehighrobustnessandinterpretabilitythroughsingle-\nmaximum generation length is set to 2048 for our model.\nstepinteraction.\nAll baseline models are tested with zero-shot settings for\nshort-form QA datasets, and with one-shot settings for the\nInstructions\nlong-formQAandfactverificationdatasets.", "metadata": {"id": "6cb6ccf38f07d5c6ec8c3d67ff4c2fea3ddd1d8b", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 10, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "Instructions\nlong-formQAandfactverificationdatasets.\nThe instructions for GPT-4 to generate self-reasoning tra-\njectories are shown in Figure 5 (the short-form and long- Othersettings. Forthedocumentretrieval,weretrievethe\nformQAtasks)andFigure6(thefactverificationtask).The top-k relevant documents, and the k is set to 5. We use the\nwordsintheorangefontarekeyfieldsthatneedtobegen- DPRandtheContrieverinshort-formQAsettings.Forlong-\nerated. form QA, we use GTR as a retrieval and evaluate it using\none-shottoinstructthemodeltogeneratecitations.Forthe", "metadata": {"id": "f3e53356f176083dc16e15bf422347aae9008adc", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 10, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "one-shottoinstructthemodeltogeneratecitations.Forthe\nDatasetsDescription datagenerationqualitycontrolsetting,thethresholdforcita-\ntionrecallissetto0.8,andthethresholdofcitationprecision\nWe conducted an extensive experimental evaluation of two\nissetto0.8.\nshort-form QA datasets, one long-form QA dataset, and a\nfactverificationdataset.\nCategoricalComparisons\nNaturalQuestion (NQ) (Kwiatkowski et al. 2019) con-\ntains real user questions issued to the Google search and We differentiate our method from existing strong baseline", "metadata": {"id": "d89d8c3e34230ea522fec60bce26fd90ac385425", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 10, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "answersfoundfromWikipediabytheannotators.NQiscre- modelsbycategorizingandcomparingitacrossfivedimen-\natedtotrainandevaluateautomatedquestionansweringsys- sions,aspresentedinTable4.Asillustratedinthetable,our\ntems. methodcanstandoutinseveralkeyaspects.\nPopQA(Mallenetal.2023)isalarge-scaleopen-domain First, our SELF-REASONING method is the only end-to-\nquestionansweringdataset,consistingofentity-centricQA end framework among existing methods that can improve\npairs. Each question is made by converting a knowledge performance without relying on external models or tools.", "metadata": {"id": "b3463280fc15d04fb33b3df00ba262ff9bcb06f9", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 10, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "triplet retrieved from Wikidata using a template. In this Second, our method eliminates the need for external mod-\nwork, we use PopQA to evaluate performance in long-tail ulesduringboththetrainingandinferencephases.Inpracti-\nsettings. calapplications,ourframeworkdoesnotneedtocallmulti-\nASQA (Stelmakh et al. 2022) is a long-form factoid ple tools or modules. Third, our framework requires a sig-\ndataset,andmostquestionscanbeansweredbyWikipedia. nificantly smaller dataset for training the LLM compared", "metadata": {"id": "e75862c0ad4fc859c884b5c9b489cb576f1f4934", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 10, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "Each question originatesfrom AmbigQA (Min et al. 2020) to other methods, needing only 2,000 samples with self-\nand represents an ambiguous query that requires multiple reasoningtrajectories.Thisefficiencyintrainingdrastically\nshortanswerstocovervariousaspects.Thedatasetprovides\nalong-formanswerthatcontainsallshortanswers. 2Codesareavailableathttps://github.com/vllm-project/vllm", "metadata": {"id": "abffa419620db4637d96f2d12f88f8a2cb910f46", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 10, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "Externalmodule\nModels End-to-End TraindataforLLM\n(train) (inference) (data)\nSelf-Reasoning(Ours) Y N N NoNeed 2K\nSelf-RAG(Asaietal.2024) N N N NoNeed 145K(Generator)/46K(Critic)\nReAct(Yaoetal.2023) N N Y NoNeed NoNeed\nRECOMP(Xuetal.2024) N Y Y 152K NoNeed\nTable 4: Categorical comparisons with strong baseline models. External module (train) and External module (data) refer to\nwhethertheexternalmoduleneedstobetrainedandthenumberofsamplesrequired,respectively.Externalmodule(inference)\nindicateswhethertheexternalmoduleisneededduringtheinferencestage.TraindataforLLMindicatesthenumberoftraining", "metadata": {"id": "3123108c3cc67cd260a098334e934a0bc1d038a3", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 11, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "samplesneededtotrainwithLLMs.\nAlgorithm1:DataQualityControl the film. Second, in the evidence-aware selective process,\nthe model retrieved the first documents, which highlighted\nInput: theoriginself-reasoningdatasetD generatedbyGPT-4\n0\nOutput: filteredhighqualityself-reasoningdatasetD the original start date as January 2002, with filming com-\nsr\n1: InitializetheevaluationmetricsprogramP mencing in February 2002 (highlighted in green in the fig-\n2: InitializethecitationscoretoolsT (Gaoetal.2023b) ure). This information was crucial in establishing the time-", "metadata": {"id": "6151172aae7f5e5a701ed156a6d463b0cfec5a32", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 11, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "3: fori=1toN do lineforthefilm’sproduction.Themodelcanalsounderstand\n4: Evaluatewhethertheanswerofsampled i iscorrectusing ofthedifferencebetweentheproductiondateandtherelease\nprogramP dateinthethirdretrieveddocument(highlightedinredinthe\n5: ifTrueandd ∈Long-formQAdataset then\ni figure). In the trajectory analysis process, the correct time-\n6: Computecitationrecallscores forsampled\nr i linewasdeducedbypiecingtogetherself-generatedtrajecto-\n7: Computecitationprecisionscores forsampled\np i ries,leadingtotheconclusionthatthefilmCatchMeIfYou\n8: if s ≥δ ands ≥δ then", "metadata": {"id": "334fb30a3e4a5f38bba885202af26c29b5d8a4ce", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 11, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "p i ries,leadingtotheconclusionthatthefilmCatchMeIfYou\n8: if s ≥δ ands ≥δ then\nr r p p Can was indeed produced in 2002. As the case illustrated\n9: AddthetrainingsampletoD\nl\n10: endif above, by leveraging relevant documents and focusing on\n11: endif contextualevidence,our SELF-REASONING frameworkcan\n12: ifTrueandd ∈/Long-formQAdataset then achieve a precise and well-supported answer, highlighting\ni\n13: AddthetrainingsampletoD its utility and robustness in complex information retrieval\ns\n14: endif tasks.\n15: endfor\n16: D sr =D l ∪D s MoreAnalysisonAblationStudy\n17: returnD\nsr", "metadata": {"id": "9625be2ba9ba8b429204dd419a17ad7b096ea8d7", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 11, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "s\n14: endif tasks.\n15: endfor\n16: D sr =D l ∪D s MoreAnalysisonAblationStudy\n17: returnD\nsr\nEffectivenessofGradualLearning. Further,wevalidate\ntheeffectofgraduallearning.RatherthantraininganLLM\nwithastage-by-stageapproach,weinitiallyconcatenatethe\nlowers the resources and time needed, making our method\nreasoning trajectories from all stages and put them into the\nbothcost-effectiveandscalableforpracticalapplications.\nLLMforend-to-endtraining.AsshowninTable2,theper-\nformancedeclinecanbeobservedinthreedatasets,suggest-\nCaseStudy\ningthatgraduallearningcanhelpimproveperformance.", "metadata": {"id": "a2766b702d52a5088a275f9959b82610eacb740c", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 11, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "CaseStudy\ningthatgraduallearningcanhelpimproveperformance.\nInourcasestudy,asillustratedinFigure7,wecomparethe\nEffectiveness of Quality Control. The effect of quality\nresponses generated by the raw LLM, the standard RALM\ncontrolondatagenerationisalsoevaluatedinourwork.In-\n(e.g. LLaMA2 with retrievals), and our SELF-REASONING\nsteadofusingthefilteredhigh-qualitytrainingsamples,we\nmethod. The challenge involves reconciling information\nrandomly sampled 2,000 unfiltered training samples gener-\nfrom multiple retrieved documents to provide a correct an-", "metadata": {"id": "45588be296c09d76a58f0d116c2642ad4b3c521f", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 11, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "from multiple retrieved documents to provide a correct an-\natedbyGPT-4.AsshowninTable2,thesubstitutionofun-\nswerastheretrieveddocumentscontainednoisydata.\nfiltered training data leads to the degradation of the model\nThe response from the raw LLM (e.g., LLaMA2) sug- results.\ngested that the film was made in 2000, based on its inher-\nited knowledge. However, this answer is incorrect and is a HumanEvaluation\nhallucination generated by the LLM. The standard RALM\nWerandomlysample100examplesfromtheASQAdataset\napproach yielded 1989 as the production date. This answer", "metadata": {"id": "a7591b703e17ded461f4571859230d07861fe5e9", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 11, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "approach yielded 1989 as the production date. This answer\nandannotatetheoutputsofselectedmodels.Eachsampleis\nwasbasedonunrelateddetailsfromtheretrieveddocuments,\nthen assigned to two people for annotation. Each annotator\nshowingalackofcontext-specificunderstandingandrobust-\nisrequiredtoverifythecitationrecallandcitationprecision\nnessfornoisyretrieveddocuments.\nacorrdingtoourprovidedscheme.Theannotationschemeis\nOur SELF-REASONING framework provided a compre-\ninspiredby(Gaoetal.2023b)asfollows:\nhensiveapproachbyassessingtherelevanceandcontextof", "metadata": {"id": "3c3a766d2d42633d698da3f615fc7d7cfd326cd2", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 11, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "inspiredby(Gaoetal.2023b)asfollows:\nhensiveapproachbyassessingtherelevanceandcontextof\nretrieved documents. First, in the relevance-aware process, CitationRecall. Theannotatorsareshownthequestionq,\nthedocumentswereidentifiedasrelevantbasedontheircon- thestatements ,andallofitscitationsC ,andtheyassessif\ni i\ntent regarding the production dates and events surrounding thesetofcitationsfullysupportthestatement(recall=1)or", "metadata": {"id": "811c0c75322e5f152c1858eaeeee29730ca8f9d6", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 11, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "Models NQ ASQA\nLLaMA2 0.19 1.92\nSelf-RAG 2.08 2.38\nSELF-REASONING 2.19 2.32\nGPT-4 35.5 40.5\nTable 5: The latency experiment results on NQ and ASQA\ndatasets.Theaverageinferencelatencyperquestioniseval-\nuatedusing7B-parametermodels.\niftheydonotsupportalltheclaims(recall=0).Wecalculate\ntheoverallrecallscoreforthemodelbyaveragingtherecall\nscoresofallstatements.\nCitation Precision. The annotators are shown the ques-\ntion q and a statement s and one of its citation c(k) ∈ C .\ni i i\nWeasktheannotatorifthecitationfullysupports,partially\nsupports,ordoesnotsupportthegeneratedclaimsins .Ci-\ni", "metadata": {"id": "fa6dd72751f6ad471f82f3f6aa40f632ea89352c", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 12, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "supports,ordoesnotsupportthegeneratedclaimsins .Ci-\ni\ntationc(k)hasacitationprecisionof1ifs hasarecallof1,\ni i\nand c(k) fullyorpartially supportss .Finally,we calculate\ni i\nthe overall precision score for the model by averaging the\nprecisionscoresofallstatements.\nThePseudo-codeofDataQualityControl\nThe details and pseudo-code of data quality control are il-\nlustratedinAlgorithm1.Inthetable,D istheoriginself-\n0\nreasoningdatasetgeneratedbyGPT-4.P istheprogramto\nevaluate the answer according to the metrics. T is the off-\nthe-shelf tool (Gao et al. 2023b) to calculate the citation", "metadata": {"id": "fb53d1c07b72bf5f56579260cc7408db4338115a", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 12, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "the-shelf tool (Gao et al. 2023b) to calculate the citation\nscores. N is the number of total samples from the origin\ndataset,andd isthei-thtrainingsample.s ands arecita-\ni r c\ntionrecallandprecisionscoresforeachsamplerespectively.\nδ and δ are thresholds of citation recall and precision re-\nr p\nspectively. D is filtered trajectory data for long-form QA,\nl\nand D is filtered dataset for short-form QA and fact veri-\ns\nfication dataset. D is the final high quality self-reasoning\nsr\ntrainingdataset.\nLatencyAnalysis\nWeconductextraexperimentstomeasureinferencelatency", "metadata": {"id": "5889a28aa79862b7a4c40304f9938cf2a699d7df", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 12, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "sr\ntrainingdataset.\nLatencyAnalysis\nWeconductextraexperimentstomeasureinferencelatency\nacross our approach, Self-RAG, and GPT-4. The results\ndemonstrated that our method achieved better performance\nwhilemaintaininglatencycomparabletoSelf-RAG.There-\nsults of average inference latency per question for the NQ\nandASQAdatasetsareshowninTable5.", "metadata": {"id": "d5e9bebe94fc05e0b5d52d8b56d3a8e8c2348dfa", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 12, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "Instructions\n# Role\nYou are an experienced expert, skilled in answering various questions.\n# Task\nPlease answer the question according to the provided reference evidence as\nrequired.\n# Reference Evidence\n[1] Retrieved Document {{DOCUMENT 1}}\n[2] Retrieved Document {{DOCUMENT 2}}\n[3] Retrieved Document {{DOCUMENT 3}}\n[4] Retrieved Document {{DOCUMENT 4}}\n[5] Retrieved Document {{DOCUMENT 5}}\n# Requirements\n1. First, please judge whether the provided documents are relevant with the\nquestion, and put it in the relevant field. If the provided content is irrelevant to", "metadata": {"id": "14c1148f8d1bbab08edf3e1e45138c25e6483ef7", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 13, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "question, and put it in the relevant field. If the provided content is irrelevant to\nthe question, explain the reason in the relevant reason field, then you can give\nthe answer with your internal knowledge.\n2. If possible, answer the question in points and provide explanations.\n3. If the content in the answer comes from different pieces of evidence, you\nneed to cite the sequence number of the evidence at the end of the sentence.\nThe citation format is shown below: [1], [1,3].\n4. Place each cited piece of evidence in the cite_list field, cite content field to", "metadata": {"id": "1f8095894fbe173bef90fa5ae3c83fa601331336", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 13, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "4. Place each cited piece of evidence in the cite_list field, cite content field to\nstore each paragraph of cited content (omitted words can be replaced by ...),\ncite reason is used to store your thoughts and analysis of this content, how\nthis paragraph can answer the question.\n5. Put the long answer content in the analysis field, and put the short\nanswer(no more than 10 words) in the answer field.\n# Question\n{{QUESTION}}\nFigure5:TheinstructionsfortheGPT-4togeneratetheself-reasoningtrajectoriesforshort-formandlong-formQAtasks.", "metadata": {"id": "0d43dcc67153757cb874c064ed1d2929076be983", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 13, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "Instructions\n# Role\nYou are an experienced expert, skilled in answering various questions.\n# Task\nPlease answer the question according to the provided reference evidence as\nrequired.\n# Reference Evidence\n[1] Retrieved Document {{DOCUMENT 1}}\n[2] Retrieved Document {{DOCUMENT 2}}\n[3] Retrieved Document {{DOCUMENT 3}}\n[4] Retrieved Document {{DOCUMENT 4}}\n[5] Retrieved Document {{DOCUMENT 5}}\n# Requirements\n1. First, please judge whether the provided documents are relevant with the\nclaim, and put it in the relevant field. If the provided content is irrelevant to the", "metadata": {"id": "737d30f3a3ce5667aaa0ea07f55882cade695a82", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 14, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "claim, and put it in the relevant field. If the provided content is irrelevant to the\nquestion, explain the reason in the relevant reason field, then you can give the\nanswer with your internal knowledge.\n2. If possible, answer the question in points and provide explanations.\n3. If the content in the answer comes from different pieces of evidence, you\nneed to cite the sequence number of the evidence at the end of the sentence.\nThe citation format is shown below: [1], [1,3].\n4. Place put each cited piece of evidence in the list, use cite content field to", "metadata": {"id": "4710c4254b9c56d1302bcb502dc82e4ed344eff7", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 14, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "4. Place put each cited piece of evidence in the list, use cite content field to\nstore each paragraph of cited content (omitted words can be replaced by ...),\ncite reason is used to store your thoughts and analysis of this content, how this\nparagraph can answer the question.\n5. Put the long answer content in the analysis field, and put the short answer\n(SUPPORT/REFUTE/NOT ENOUGH INFO) in the answer field.\n# Question\nYou should check whether the given claim is supported or refuted by above the\nevidence. If supported，output SUPPORT, if controversial, output REFUTE; if", "metadata": {"id": "9a64f849d797cbd13037c6a1c05ffdd9be16d67f", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 14, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "evidence. If supported，output SUPPORT, if controversial, output REFUTE; if\nthe evidence provided is not enough to make a decision or the provided\nevidence is irrelevant, output NOT ENOUGH INFO.\n{{CLAIM}}\nFigure6:TheinstructionsfortheGPT-4togeneratetheself-reasoningtrajectoriesforthefactverificationtask.", "metadata": {"id": "67aebdcc50d8bf99f157d001a119e84e82c0d237", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 14, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "Question\nWhen was catch me if you can made?\nRetrieved document #1: The film due to her busy schedule. The original start date was\nJanuary 2002, but was pushed to February 7 in Los Angeles, California. Locations included\nBurbank, Downey, New York City, Quebec City and Montreal. …\nRetrieved document #2: eventually turned to him for help in catching other check forgers.\nDevelopment for the film started in 1980, but did not progress until 1997, when Spielberg's\nDreamWorks bought the film rights to Abagnale's book. David Fincher, Gore Verbinski, Lasse", "metadata": {"id": "af62bcf5bac54ab3c594d791ee6ec73d89bb87f0", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 15, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "DreamWorks bought the film rights to Abagnale's book. David Fincher, Gore Verbinski, Lasse\nHallström, Miloš Forman, and Cameron Crowe had all been possible candidates for director\nbefore Spielberg decided to direct. …\nRetrieved document #3: It was funded independently and had a budget of $800,000. The film\nwas Sommers's directorial debut and was written by him. When production finished, the studio\nthat had promised to distribute the movie had gone out of business, and it was eventually\nreleased by MCA Inc. in July 1989. The movie…\n(Other retrieved documents are omitted.)", "metadata": {"id": "04ee0b77996b478a00384168bf14cd7c033e75f3", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 15, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "released by MCA Inc. in July 1989. The movie…\n(Other retrieved documents are omitted.)\nRaw LLM Answer: 2000 ❌\nStandard RAG Answer: 1989 ❌\nSelf-Reasoning Answer:\n(Relevant Aware Process)\nRelevant: True\nRelevant Reason: The provided documents are relevant with question.\n(Evidence Aware Selective Process)\nCite content: [1] … the original start date was January 2002, but was pushed to February 7 in\nLos Angeles, …\nReason to cite: This piece provides information on the commencement and location of filming\nfor 'Catch Me If You Can', indicating that it started in April 2002", "metadata": {"id": "4cfd1cf8f447afb7107fd83fa429888a92ae4f20", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 15, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "for 'Catch Me If You Can', indicating that it started in April 2002\nCite content: [3] … When production finished, the studio that had promised to distribute the\nmovie had gone out of business, and it was eventually released by MCA Inc. in July 1989., …\nReason to cite: This piece provides information on the release date of 'Catch Me If You Can',\nindicating that it was eventually released by MCA Inc. in July.\n(Trajectory Analysis Process)\nAnalysis:（long answer）\nThe film 'Catch Me If You Can' was made in 2002. It started filming in April 2002 in Park Avenue,", "metadata": {"id": "f4e8221e59284a1e8ce7901d6298456f9d18bd54", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 15, "created_at": "2025-09-12T03:11:26Z"}}
{"content": "The film 'Catch Me If You Can' was made in 2002. It started filming in April 2002 in Park Avenue,\njust outside the Waldorf-Astoria Hotel, and moved to Orange, New Jersey, before returning to\nBrooklyn for bank and courthouse scenes [1].\nFinal Answer：2002 ✅ （short answer）\nFigure7:ACaseStudy.Wepresenttheself-reasoningtrajectoriesgeneratedbyourframeworkduringinferenceanddemonstrate\nhowtheycanlogicallygeneratethecorrectanswer.", "metadata": {"id": "00e8a361f2bceb6aa8097d29f8bb17466dd96f5c", "source": "Improving Retrieval Augmented Language Model with Self-Reasoning (1).pdf", "page": 15, "created_at": "2025-09-12T03:11:26Z"}}
