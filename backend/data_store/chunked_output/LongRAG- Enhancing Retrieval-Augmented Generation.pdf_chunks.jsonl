{"content": "LongRAG: Enhancing Retrieval-Augmented Generation\nwith Long-context LLMs\n♠Ziyan Jiang, ♠Xueguang Ma, ♠Wenhu Chen\n♠University of Waterloo\nziyanjiang528@gmail.com, {x93ma ,wenhuchen}@uwaterloo.ca\nProject Website: https://tiger-ai-lab.github.io/LongRAG/\nAbstract\nIn traditional RAG framework, the basic retrieval units are normally short. The common\nretrievers like DPR normally work with 100-word Wikipedia paragraphs. Such a design\nforces the retriever to search over a large corpus to find the “needle” unit. In contrast,", "metadata": {"id": "c2ecf5dce2e5eef7ed3ad1c0ca4277c5f3b555d8", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 1, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "forces the retriever to search over a large corpus to find the “needle” unit. In contrast,\nthe readers only need to generate answers from the short retrieved units. The imbalanced\n“heavy” retriever and “light” reader design can lead to sub-optimal performance. The\nloss of contextual information in the short, chunked units may increase the likelihood of\nintroducing hard negatives during the retrieval stage. Additionally, the reader might not\nfully leverage the capabilities of recent advancements in LLMs. In order to alleviate the", "metadata": {"id": "dc33a3ac1dc994eaa53f7c72045edff7a9a8c5fb", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 1, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "fully leverage the capabilities of recent advancements in LLMs. In order to alleviate the\nimbalance, we propose a new framework LongRAG, consisting of a “long retriever” and\na “long reader”. In the two Wikipedia-based datasets, NQ and HotpotQA, where the\naverage document size is less than 1K tokens, LongRAG processes the entire Wikipedia\ncorpus into 4K-token units by grouping related documents, making these units 30 times\nlonger than before. By increasing the unit size, we significantly reduce the total number", "metadata": {"id": "9315600b6049255ced415afe397cf5c783d436b0", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 1, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "longer than before. By increasing the unit size, we significantly reduce the total number\nof units from 22M to 600K. This greatly reduces the burden on the retriever, resulting in\nstrongretrievalperformancewithonlyafew(lessthan8)topunits. Comparedtotraditional\nRAG,whichmayrequirehundredsofshortunitstoachievesimilarretrievalperformance,our\napproach minimizes the likelihood of retrieving hard negatives while maintaining semantic\nintegrity of each unit. Then we feed these retrieved units (≈ 30K tokens) to an existing", "metadata": {"id": "143080a83abbef6a6f9fd7ec9c491d42d50e65e7", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 1, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "integrity of each unit. Then we feed these retrieved units (≈ 30K tokens) to an existing\nlong-context LLM to perform zero-shot answer generation. Without requiring any training,\nLongRAG achieves an EM of 62.7% on NQ and 64.3% on HotpotQA, which are on par\nwith the (fully-trained) SoTA model. Furthermore, we test on two non-Wikipedia-based\ndatasets,QasperandMultiFieldQA-en,wheretheaveragedocumentlengthisalreadyabove\n4Ktokens. LongRAGprocesseseachindividualdocumentasasingle(long)unitratherthan\nchunking them into smaller units. By doing so, we achieve an F1 score of 25.9% on Qasper", "metadata": {"id": "0b6179c70bb88851e33f7c07aa176878d91bd928", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 1, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "chunking them into smaller units. By doing so, we achieve an F1 score of 25.9% on Qasper\n(previously 22.5%) and 57.5% on MultiFieldQA-en (previously 51.2%). Our study offers\ninsights into the future roadmap for combining RAG with long-context LLMs.\n1 Introduction\nRetrieval-AugmentedGeneration(RAG)methodshavelongbeenemployedtoenhancelargelanguagemodels\n(LLMs) (Mialon et al., 2023). Knowledge in the form of natural language can be entirely offloaded from\nthe parametric knowledge of LLMs by leveraging a standalone retrieval component from an external corpus.", "metadata": {"id": "b4cfecacf38007db2ef74e727681575dcfb44b1b", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 1, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "The existing RAG framework tends to use short retrieval units, such as 100-word passages in popular open-\ndomainquestion-answeringtasks(Chenetal.,2017;Lewisetal.,2020;Karpukhinetal.,2020). Theretriever\nis tasked with finding the “needle” (i.e. the precise tiny retrieval unit) from the “haystack” (i.e. the massive\ncorpus with up to tens of millions of information units). Subsequently, the retrieved units are passed to\nthe reader to generate the final response. On the contrary, the reader only needs to extract answers from\n1\n4202\npeS\n1\n]LC.sc[\n3v91351.6042:viXra", "metadata": {"id": "eb628b27cce540630f65af211d6aeed7b666f0b2", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 1, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "Traditional RAG LongRAG\nRetrieval Ranker Reader Long Retrieval Long Reader\nFigure 1: Traditional RAG vs. LongRAG. (Up) Traditional RAG operates on short retrieval units, where\nthe retriever needs to scan over a massive amount of units to find the relevant piece. In contrast, LongRAG\noperatesonlongretrievalunits(30xlonger). TheretrieverofLongRAGhasasignificantlyreducedworkload,\nachieving strong retrieval quality by leveraging only a few top units without the need for additional ranking\nmechanismsorothercomplexcomponents. LongRAGcouldfullyexploittheabilityoflong-contextlanguage", "metadata": {"id": "768bd4a7709f207707d4506c8b2ffa607328f23d", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 2, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "mechanismsorothercomplexcomponents. LongRAGcouldfullyexploittheabilityoflong-contextlanguage\nmodels to achieve strong performance. (Down) QA performance compared with other methods on the NQ\ndataset and the HotpotQA dataset.\nthese retrievals, which is a fairly easy task. This kind of imbalanced design, with a “heavy” retriever and a\n“light” reader, puts too much pressure on the retriever. Therefore, existing RAG models (Izacard & Grave,\n2020b) have to recall huge amounts of units, such as the top-100/200, combined with additional re-ranker", "metadata": {"id": "65c4a8b70034958f1962b5289849d28f3e18c22e", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 2, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "to achieve the best performance. Moreover, short retrieval units can lead to semantic incompleteness due\nto document truncation. This can result in the loss of contextual information, which may ultimately harm\noverall performance. This design choice was made in an era when the reader models were heavily restricted\nby their ability to handle long and contexts. With the recent advances in long-context language models,\nthe reader can potentially handle up to 128K or even millions of tokens as input (Reid et al., 2024; Achiam", "metadata": {"id": "69621ca121ff5cfb61805fbc9ac480ab428341fb", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 2, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "et al., 2023). In this paper, we propose to revisit this design choice for open-domain question answering\nand propose the LongRAG framework as a solution to balance the workload between the retriever and the\nreader, as illustrated in Figure 1. There are three important designs in our novel framework:\nLongRetrievalUnit: Byusingentiredocumentsorgroupingmultiplerelateddocuments,wecanconstruct\nlong retrieval units with more than 4K tokens. This design could also significantly reduce the corpus size", "metadata": {"id": "3475c39ec2a2f658eec107d24719921e78659fd3", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 2, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "(number of retrieval units in the corpus). This makes the retriever’s task much easier by providing more\ncomplete information, allowing the retriever’s architecture to be simplified without the need for additional\nre-rankers or iterative retrieval.\nLongRetriever: Thelongretrieverwillidentifycoarserelevantinformationforthegivenquerybysearching\nthrough all the long retrieval units in the corpus. Only a few top retrieval units (1 to 8 retrieval units in the\nfourdatasetswetestedon),withoutre-ranking,areusedforthenextstep. Comparedtoretrievinghundreds", "metadata": {"id": "9344600efa576f7ce522edce521c163b171d6163", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 2, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "fourdatasetswetestedon),withoutre-ranking,areusedforthenextstep. Comparedtoretrievinghundreds\nof short units, the long retriever only needs to retrieve a few candidates, which significantly reduces the\nlikelihood to encounter hard negatives (it will confuse the reader).\nLong Reader: The long reader will further extract answers from the concatenation of retrievals, which is\nnormally around 30K tokens. We simply prompt an existing long-context LM (like Gemini or GPT4) with\nthe question to produce the answers in a zero-shot fashion.\n2", "metadata": {"id": "3b042cb4a582dbbb3b451ae98de69a3285254dc2", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 2, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "These three novel designs significantly boost the overall performance of RAG on open-domain question-\nanswering tasks like NQ (Kwiatkowski et al., 2019), HotpotQA (Yang et al., 2018), Qasper (Dasigi et al.,\n2021) and MultiFieldQA-en (Bai et al., 2023).\nIn our experiments, we adopt off-the-shelf retrievers like BGE (Xiao et al., 2023) and readers like Gemini-\n1.5-Pro (Reid et al., 2024) or GPT-4o (OpenAI, 2024) without any further tuning. To demonstrate the\ngeneralizability of our proposed framework, we tested it on four datasets from different scenarios. First,", "metadata": {"id": "2db64d62bb8743139f7f000677ab1100941d84f9", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 3, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "we evaluate it on NQ and HotpotQA, which are Wikipedia-based dataset. The corpus of both datasets are\ncomposed of relatively short (averaging less than 1K tokens) but vast Wikipedia documents. By forming\nlonger retrieval units through the grouping of multiple related documents, we reduce the NQ corpus size\nfrom22Mto600Kunits,whichimprovestheanswerrecall@1from52%(DPR)to71%. Similarly,wereduce\nthe HotpotQA corpus size from 5M to 500K, which improves the recall@2 from 47% (DPR) to 72%. By\nexploiting the long-context understanding ability of GPT-4o, LongRAG can achieve an EM of 62.7% on NQ", "metadata": {"id": "12771c1da828de5c813d16cbd1d31fc7ba85c119", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 3, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "and 64.3% on HotpotQA. These results could be comparable to the strongest fully trained RAG models like\nAtlas(Izacardetal.,2022)andMDR(Xiongetal.,2020b). Furthermore,wetestontwonon-Wikipedia-based\ndatasets, Qasper and MultiFieldQA-en, where the corpus consists of relatively long documents averaging\nmorethan4Ktokens. LongRAGprocesseseachentiredocumentasasingleunitratherthanchunkingthem\ninto smaller units. By doing so, we achieve an F1 score of 25.9% on Qasper (previously 22.5%) and 57.5%\non MultiFieldQA-en (previously 51.2%).", "metadata": {"id": "6ac29e3cb0ca391669b309368ab259c2ab46340e", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 3, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "on MultiFieldQA-en (previously 51.2%).\nWe perform ablation studies in subsection 3.5 to prove why longer retrieval units are necessary. Given a\nbudget of 40K recall tokens, with “short retriever units”, we can increase the number of recalled units to\nreach a marvelously high recall score (91% for recall@200). However, the end performance dips significantly\ndue to the huge amount of “hard negatives”, which confuses the reader. With “long retriever units”, we\nobserve an entirely different trend. As we recall more units (from 1 to 8 units), both the recall and end", "metadata": {"id": "f4cf1bc8a2d6f0e04112ae159814e8bed6e55bba", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 3, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "performance will increase or plateau. The impact of “hard negative” is much less severe in LongRAG. It\nshowsthatLongRAGcanbetterexploittheadvancesinthelong-contextLLMs(reader). Asthelong-context\nmethods evolve, the performance of LongRAG will continue to improve. Therefore, we believe the modern\nRAG systems should re-consider the granularity of their retrieval units to exploit the advantages of the\ncurrent long-context LLMs.\n2 LongRAG\nOur proposed LongRAG framework is comprised of two components: the Long Retriever and the Long", "metadata": {"id": "e1620d1f26a5fc8eb81e87c98817e7a88b121f2b", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 3, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "Our proposed LongRAG framework is comprised of two components: the Long Retriever and the Long\nReader. ComparedtotraditionalRAG,whichoperatesonalargenumberofshortretrievalunits,LongRAG\noperates on long retrieval units, with only a few (typically fewer than 10) being fed into the reader. An\nillustrative example is shown in Figure 2.\n2.1 Long Retriever\nThe traditional RAG framework employs smaller retrieval units and prioritizes retrieving the exact fine-\ngrained short context containing the answer. In contrast, our proposed LongRAG framework places greater", "metadata": {"id": "23f26122244a1f95b7e1a5ab20594273856c9c2b", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 3, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "emphasis on recall, aiming to retrieve relevant context with much coarse granularity. This design choice\nshifts more burden from the retriever to the reader to extract the exact answers from the relevant context.\nWe denote our corpus for retrieval as C ={d ,d ,...,d }, which is a collection of D documents. Formally\n1 2 D\nspeaking, the long context retriever is a function: F : (q,C) → C that takes as input a question q and a\nF\ncorpus C and returns a filtered set of texts C ⊂C. In traditional RAG, C is usually small which contains\nF F", "metadata": {"id": "e9556d678c27ef2e06888a7eaad6b43596253fdc", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 3, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "F F\nabouthundredoftokens,whichshouldcontainexactinformationrelatedtothequestionq. Inourframework,\nC isusuallymorethan4Ktokens,whichcontainsrelavantbutnotexactinformationrelatedtothequestion\nF\nq. The long retriever function F :(q,C) is then divided into three steps:\nFormulate long retrieval units A function is applied to the corpus to form M retrieval units: G(C) =\n{g ,g ,...,g }. In traditional RAG, the retrieval unit g is typically a short span of passage which is split\n1 2 M\nfrom the documents d, containing hundreds of tokens. In our framework, g could be as long as the whole\n3", "metadata": {"id": "f09e551e6989eebb78c2c5126753ea830539c2fd", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 3, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "Title: 2015 AFL Rising Star Title: Ben Simmons\n… The award was won by Jesse Simmons was selected with the\nHogan of Melbourne who polled number one overall pick in the 2016\n49 votes, beating Carlton 's NBA draft by the 76ers, becoming\nPatrick Cripps who finished on the third Melbourne-born number\n41 votes … one overall pick\n…\nTitle: 2016 AFL Rising Star Title: Andrew Bogut\nRetriever The NAB AFL Rising Star … is an Australian professional Reader\naward is given annually to a basketball player for the Los Angeles\nstand out young player in the Lakers of the National Basketball", "metadata": {"id": "7c1795402d8097d94c16dd7fbbd677b2752c2fd5", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 4, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "stand out young player in the Lakers of the National Basketball\nAustralian Football League… Association (NBA). The 7 ft center\nwas selected by … 7 ft\nWhat is the height of the\nplayer who won the\n2015 AFL Rising Star\nTitle : 2015 AFL Rising Star\naward? … The award was won by Jesse 1.95 m\nHogan of Melbourne who polled 49\nvotes, beating Carlton 's Patrick\nCripps who finished on 41 votes …\nTitle: Jesse Hogan\n… in the Australian Football League\n(AFL). A key forward, Hogan is 1.95\nm tall and weighs 100 kg …\nTitle: Australian Football League\nLong The Australian Football League (AFL) Long", "metadata": {"id": "75ffd811cf9fb9f941404ec95c16bc067a11b77b", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 4, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "Title: Australian Football League\nLong The Australian Football League (AFL) Long\nRetriever is the pre-eminent professional Reader\ncompetition of Australian rules\nfootball.\nFigure2: LongRAGexample. Weformthelongretrievalunitwhichisatleast4Ktokensbyusingtheentire\ndocument or grouping related documents, depending on the orinal docuemnt size. In this example, multiple\nWikipedia documents are grouped through hyperlinks. This approach enables even multi-hop question-\nanswering cases from HotpotQA to be addressed using only a few retrieval units, which are then fed into a", "metadata": {"id": "232b8f925e70fdd364811f52592bd505bc33f848", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 4, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "longreader. ComparedtotraditionalRAG,whichretrieveshundredsofshortunits,ourproposedLongRAG\nreduces the likelihood of retrieving hard negatives during the retrieval stage and more effectively leverages\nrecent advances in long-context LLMs.\ndocument or even a group of documents, resulting in much longer retrieval units. If the original document\nis already long (e.g., longer than 4K tokens), we treat the entire document as a single unit. If the original\ndocument is relatively short (e.g., shorter than 1K tokens), we group related documents together to form a", "metadata": {"id": "2332d28ab2473f6bc3f71ff9880fa1a7bc6fc822", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 4, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "single unit. We provide an example of a grouping algorithm in Appendix A.3.\nBy having a longer retrieval unit, there are two key advantages: First, only very few (e.g., 4 to 8 retrieval\nunits) are fed into the reader, which greatly reduces the likelihood of encountering hard negatives compared\nto traditional RAG, which may require hundreds of short units in its reader. Second, by retaining the entire\ndocument or even related documents within a single retrieval unit, the contextual information is preserved.", "metadata": {"id": "93d7f9afc292c7f4cf3fa8721139f3f4fe37c95e", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 4, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "Similarity search Weutilizeanencoder, denotedasE (·), tomaptheinputquestiontoad-dimensional\nQ\nvector. Additionally,weemployanotherencoder,E (·),tomaptheretrievalunittoad-dimensionalvector.\nC\nWe define the similarity between the question and the retrieval unit using the dot product of their vectors:\nsim(q,g)=E (q)TE (g)\nQ C\nInLongRAGsettings,E (g)ischallenginggiventhelengthofg,soweresorttoanapproximationasbelow.\nC\nsim(q,g)=E (q)TE (g)≈max(E (q)TE (g′))\nQ C Q C\ng′⊆g\nWe approximate it by maximizing the scores of all chunks g′ within the retrieval unit g, akin to the MaxP", "metadata": {"id": "a7570c2ed286686dd3f4733918343c8757490b0b", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 4, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "design in (Dai & Callan, 2019). We consider different levels of granularity of chunk g′, including 512 tokens,\n4Ktokens,andencodingtheentireg completely. TheempiricalstudyaboutthissettingsisinTable4. With\nthis similarity score setup, we will retrieve the top k retrieval units closest to the given query. For efficient\nretrieval, we precompute the embedding of each retrieval unit g′ and predict the exact inner product search\nindex in FAISS (Johnson et al., 2019).\nAggregate retrieval result We will concatenate the top k retrieval units into the long context as the", "metadata": {"id": "58c3b47744e6145dcfc3a5d198ca0122628600c6", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 4, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "retrieval result, denoted by C = Concat(g1,g2,...,gk). Depending on the selection of retrieval units, a\nF\n4", "metadata": {"id": "8693554f84ce78fb3e3212dcf84aaa8fed215639", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 4, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "larger retrieval unit size will result in a smaller value of k being used. For instance, in NQ dataset, if the\nretrieval unit is a passage, k is approximately above 100; if it’s a document, k is around 10; and for grouped\ndocuments as retrieval units, we typically set k to 4 to 8.\n2.2 Long Reader\nThe long reader operates straightforwardly. We feed the related instruction i, the question q, and the long\nretrieval result C into an LLM, enabling it to reason over the long context and generate the final output.\nF", "metadata": {"id": "c2cca327463c6f5f033fb017e3ef05dbdd4b7384", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 5, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "F\nIt’simportantthattheLLMusedinthelongreadercanhandlelongcontextsanddoesnotexhibitexcessive\nposition bias. We select Gemini-1.5-Pro (Reid et al., 2024) and GPT-4o (OpenAI, 2024) as our long reader\ngiven their strong ability to handle long context input. We utilize different approaches for short and long\ncontexts. For short contexts, typically containing fewer than 1K tokens, we instruct the reader to directly\ngeneratetheanswerfromtheprovidedcontextretrievedfromthecorpus. Forlongcontexts,typicallylonger", "metadata": {"id": "500af2cf3b4ac6fe950741a3d7249c263b560269", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 5, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "generatetheanswerfromtheprovidedcontextretrievedfromthecorpus. Forlongcontexts,typicallylonger\nthan 4K tokens, we empirically find that using a similar prompt as for short contexts, where the model\nextracts the final answer directly from the long context, often leads to decreased performance. Instead, the\nmost effective approach is to utilize the LLM as a chat model. Initially, it outputs a long answer, typically\nspanning a few words to a few sentences. Subsequently, we prompt it to generate a short answer by further", "metadata": {"id": "da282f1b9290d5acb1bae63f74fa9e1acb08457e", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 5, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "extracting it from the long answer. The prompt is provided in the Appendix A.1.\n3 Experiments\nInthissection,wewillfirstprovidedetaileddescriptionsofthefourdatasetsweuse,followedbyademonstra-\ntion of the retriever’s performance. Next, we will present the final question-answering performance. Finally,\nwe conduct detailed ablation studies to explain why operating on long retrieval units benefits performance.\n3.1 Dataset\nOur proposed methods were tested on four question-answering datasets. The basic statistics are shown in\nTable 1. Additionally, we have provided some examples in Appendix A.4.", "metadata": {"id": "e2d61ac39da571e7bfd87c2e33986e2d80431cba", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 5, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "Table 1. Additionally, we have provided some examples in Appendix A.4.\nAvg. Doc.\nDataset Corpus source # of Documents # of Text cases Metric\nLength\nNQ Wikipedia 800 3M 3,610 EM\nHotpotQA Wikipedia 130 5.2M 7,405 EM\nQasper Science 4.7K 416 371 F1\nMultiFieldQA-en Multi-field 6.9K 150 150 F1\nTable 1: An overview of the four datasets used in our experiments is provided. “Corpus source” refers to\nthe origin of the retrieval corpus. We selected NQ and HotpotQA from Wikipedia, Qasper from scientific\ndocuments, and MultifieldQA-en from multi-field documents. The two Wikipedia-based datasets utilize a", "metadata": {"id": "a8782528ad7acb93cb860343b87f270e4fb0c0fe", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 5, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "massive retrieval corpus containing millions of short documents. In contrast, the other two datasets employ\na smaller corpus consisting of hundreds of long documents.\nNatural Question(Kwiatkowskietal.,2019)isdesignedforend-to-endquestionanswering. Thequestions\nare mined from real Google search queries and the answers were spans in Wikipedia articles identified by\nannotators. This dataset contains 3,610 questions. For NQ, we use the Wikipedia dumps from December\n20, 2018, which contain approximately 3 million documents and 22 million passages.", "metadata": {"id": "be7d7111bf1284c651eab83c969a962e3600f6a5", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 5, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "20, 2018, which contain approximately 3 million documents and 22 million passages.\nHotpotQA (Yang et al., 2018) consists of two-hop questions over diverse topics. We focus on the fullwiki\nsetting in which two Wikipedia passages are required to answer the questions. Since the gold passages for\nthe test set are not available, we follow prior work (Xiong et al., 2020b) and evaluate on the development\nset, which has 7,405 questions. There are two main question types in HotpotQA: (1) comparison questions", "metadata": {"id": "e1db5eee6144a7d8600e34071a5b21bd198f9a7d", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 5, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "usually require contrasting two entities and (2) bridge questions can be answered by following a connecting\nentitythatlinksonedocumenttoanother. ForHotpotQA,weusetheabstractparagraphsfromtheOctober\n1, 2017 dump, which contain around 5 million documents.\n5", "metadata": {"id": "903f3b4ddb085ce8185caec657c9f36972849470", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 5, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "Qasper (Dasigi et al., 2021) is an information-seeking question answering dataset over academic research\npapers. Eachquestioniswrittenasafollowuptothetitleandabstractofaparticularpaper,andtheanswer,\nifpresent,isidentifiedintherestofthepaper. TheoriginalQasperdatasetisasingle-documentQAdataset.\nWe refactor it into a RAG task, where the system first retrieves the necessary document and then answers\nthe given question, following a design similar to LoCoV1 (Saad-Falcon et al., 2024).\nMultifieldQA-en (Bai et al., 2023) is a question-answering dataset based on long documents from diverse", "metadata": {"id": "7e5b13192b30bdd5348aa403cd5bc4d295b60678", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 6, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "sources, including legal documents, government reports, encyclopedias, and academic papers. The original\nMultifieldQA-en is a single-document QA dataset. We refactor the dataset into a RAG task, where the\nsystemfirstretrievesthenecessarydocumentandthenanswersthegivenquestion,followingadesignsimilar\nto LoCoV1 (Saad-Falcon et al., 2024).\n3.2 Retrieval Performance\nIn this section, we present the retrieval performance on two extractive QA datasets, NQ and HotpotQA,\nto demonstrate that comparable retrieval performance can be achieved using only a few long retrieval units", "metadata": {"id": "0520b936712520a3d5ea32c5740011813bc1d933", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 6, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "(such as 4 to 8). This approach contrasts with the use of hundreds of short retrieval units, which may lead\nto information loss and the introduction of hard negatives that can confuse the reader and prevent the full\nutilization of long-context LLMs. For the other two datasets, it’s not straightforward to compare retrieval\nperformance at different granularities since they are not extractive QA tasks. Therefore, we will directly\ndiscuss the final QA results in the next section.\nMetrics Retrieval performance is measured using Answer Recall (AR) and Recall (R). For NQ, we use", "metadata": {"id": "a34337a3a3efc29983b8e6a186f7b1d1c48e3539", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 6, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "Metrics Retrieval performance is measured using Answer Recall (AR) and Recall (R). For NQ, we use\nonlyanswerrecall,whileforHotpotQA,weusebothmetrics. AnswerRecallistherecalloftheanswerstring\nin all the retrieved documents that we plan to use in the reader. For example, if the retrieval unit is at the\n“passage” level and the number of retrieval units is 100, answer recall measures whether the answer string\nis present in these 100 passages. For HotpotQA, we compute AR only for questions with span answers,", "metadata": {"id": "5de725732a7fd281b235ffd842268214d26123a2", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 6, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "is present in these 100 passages. For HotpotQA, we compute AR only for questions with span answers,\nspecifically the “bridge” type questions, while ignoring yes/no and comparison questions, following previous\nwork(Khalifaetal.,2022). RecallusedforHotpotQAmeasureswhetherthetwogolddocumentsarepresent\nin all the retrieved results. For example, if the retrieval unit is at the “document” level and the number of\nretrieval units is 10, recall measures whether both gold documents are present among the 10 retrieval.", "metadata": {"id": "359a6cbdb11fe7bee6f7c6c7eefd8b66e00de24f", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 6, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "Experiment Setup We leverage open-sourced dense retrieval toolkit, Tevatron (Gao et al., 2022), for\nall our retrieval experiments. The base embedding model we used is bge-large-en-v1.5, a general-purpose\nembeddings model that isn’t specifically trained on our test data.\nAverageNumofTokens\nRetrievalUnit CorpusSize NumofRetrievalUnits AnswerRecall(AR)\nCorpus TestSet\n1 120 130 52.24\nPassage 22M 100 12K 14K 89.92\n200 24K 28K 91.30\n1 820 4K 69.45\nDocument 3M 5 4K 18K 85.37\n10 8K 34K 88.12\n1 4K 6K 71.69\nGroupedDocuments 600K 4 16K 25K 86.30\n8 32K 50K 88.53", "metadata": {"id": "c3dba5cbfaec64f9d88c0414fc84a4b2be900fda", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 6, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "10 8K 34K 88.12\n1 4K 6K 71.69\nGroupedDocuments 600K 4 16K 25K 86.30\n8 32K 50K 88.53\nTable 2: The table illustrates the retrieval performance on NQ. Employing a long-context retriever (with an\naveragenumberoftokensforeachretrievalunitupto6K)compressesthecorpussizebyupto30times(from\n22Mto600K),enhancingtop-1answerrecallbyapproximately20points(from52.24to71.69). Furthermore,\nlong-context retriever requires significantly fewer retrieval units (10x fewer) to achieve comparable results.\nTherefore, integrating long-context retrieval significantly alleviates the burden of retriever.", "metadata": {"id": "f3bc3caa5874486d51071bbb5079718fedc038fe", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 6, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "Therefore, integrating long-context retrieval significantly alleviates the burden of retriever.\nTable 2 and Table 3 have shown the retrieval results on NQ and HotpotQA. In the NQ dataset, we utilize\nthree different retrieval units, ranging from shorter to longer: passage, document, and grouped documents.\n6", "metadata": {"id": "1823580ce2a99d8fe8c5c2a675e4662014876688", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 6, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "AverageNumofTokens Recall AnswerRecall\nRetrievalUnit CorpusSize NumofRetrievalUnits\n(R) (AR)\nCorpus TestSet\n2 130 200 30.01 47.75\nDocument 5.2M 100 6.5K 10K 74.84 84.67\n200 13K 20K 79.68 88.34\n2 1K 8K 56.30 72.49\nGroupedDocuments 500K 8 4K 29K 74.71 84.40\nTable 3: The table illustrates the retrieval performance on HotpotQA. Similar to the findings on NQ, a\nlong-context retrieval could significantly alleviate the burden on the retriever component.\nIn the table, we have mentioned two kinds of average number of tokens in each retrieval unit: one for the", "metadata": {"id": "d60c8fb7b4ac0e11cfa8940b363ff42a556e5138", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 7, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "entire corpus and one for each test set. The retrieval units for each test case can sometimes be much longer\nthan the average size across the whole corpus, as the corpus might include some Wikipedia pages with very\nfew words, while the test cases may focus more on longer documents. Generally, our long-context retriever\n(atthedocumentlevelandgroupeddocumentlevel)usesretrievalunitscontaininganaverageof6Ktokens.\nBy using longer retrieval units, there are several advantages: 1) It will significantly alleviate the burden on", "metadata": {"id": "f8bf1d55adef5e469ce3786a4b4806943f97db3e", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 7, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "theretrieverbycompressingthecorpussizebyapproximately30times,from22Mto600K.Thetop-1answer\nrecall improves by about 20 points, from 52.24 to 71.69. We could use significantly fewer retrieval units to\nachieve comparable retrieval performance. For instance, 8 retrieval units at the grouped document level can\nachieve similar recall as 100 retrieval units at the passage level. 2) It could provide more comprehensive\ninformation to the reader. In the original passage-level RAG setup, information might be incomplete due to", "metadata": {"id": "cfa355258a461b07dd4890498e5ac57f1f5bc45d", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 7, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "thechunkingoperation. IntheHotpotQAdataset, weobservesimilarresults. Onenotabledifferenceisthat\nin HotpotQA, the retrieval units are only at the document level and grouped document level, as HotpotQA\nuses only abstract paragraphs from each Wikipedia page.\nModel Granularity AR@1\nBGE-Large 512-tokens chunk 71.7%\nE5-Mistral-7B 4000-tokens chunk 54.2%\nE5-Mistral-7B entire grouped retrieval unit 23.4%\nTable4: Differentmethodstoencodethelongretrievalunitinthelongretriever. Usingageneralembedding\nmodel and approximating by maximizing the similarity scores between the query and all chunks within the", "metadata": {"id": "f33a788fe0e37f61920d832cc4214732b08b30f0", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 7, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "retrieval unit is better than using the existing long embedding model to encode the entire context.\nEncode the long retrieval unit As discussed in Section 2.2, it’s very challenging to employ an\nencoder, E (·), to map the retrieval unit g to a d-dimensional vector when g is very long. There-\nC\nfore, we use an approximation in our proposed system. Table 4 demonstrates that our approximation,\nsim(q,g)=E (q)TE (g)≈max (E (q)TE (g′)), is much more effective than encoding the entire long\nQ C g′⊆g Q C", "metadata": {"id": "3c331c4b97e195314042578d9f646ac4232235e3", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 7, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "Q C g′⊆g Q C\ncontext directly. We compare three methods: 1) Using the general embedding model “bge-large-en-v1.5”\n(Xiao et al., 2023), with g′ selected as text of 512-token size. 2) Using long embedding model “E5-Mistral-\n7B” (Zhu et al., 2024a), with g′ selected as the whole document, which has an average size of 4K tokens.\n3) Using long embeddings model “E5-Mistral-7B”, with no approximation, we encode the entire g, which\nis composed of multiple documents, directly. The average size of g is 6K tokens. We can notice from the", "metadata": {"id": "02b81926c9c9f0541c42b111dae77859cc605065", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 7, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "table that our approximation by taking the maximum score between the query and each text piece from the\nlong context produces much better results than encoding them directly using the long embedding model.\nWe believe that future advancements in long embedding models, which focus on encoding long contexts or\nmultiple documents, will further enhance our framework and reduce memory consumption.\n3.3 Full QA Performance on Wikipedia-based Datasets\nWe leverage Gemini-1.5-Pro and GPT-4o as the reader in our LongRAG framework. The prompt we use for", "metadata": {"id": "62e6edd1e33296e70881acb5403d401f904ac58c", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 7, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "We leverage Gemini-1.5-Pro and GPT-4o as the reader in our LongRAG framework. The prompt we use for\nour experiments are in Table 7. For Wiki-based datasets, such as NQ and HotpotQA, which generate short\nanswerstypicallylessthan5tokens, weuseEM(ExactMatchrate)astheevaluationmetric. Wealsorefine\n7", "metadata": {"id": "27838c84633468c784a5a2f0ecf94ae79eeab15a", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 7, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "NQ EM HotpotQA EM\nClosed-Book Closed-Book\nGPT-4-Turbo (Achiam et al., 2023) 41.2 Claude-3-Opus (Anthropic, 2024) 32.8\nGemini-1.5-Pro (Reid et al., 2024) 47.8 Gemini-1.5-Pro (Reid et al., 2024) 33.9\nClaude-3-Opus (Anthropic, 2024) 49.2 GPT-4-Turbo (Achiam et al., 2023) 42.4\nFully-supervised RAG Fully-supervised RAG\nREALM (Guu et al., 2020) 40.4 DrKIT (Dhingra et al., 2020) 42.1\nDPR (Karpukhin et al., 2020) 41.5 Transformer-XH (Zhao et al., 2019) 51.6\nRAG (Lewis et al., 2020) 44.5 QAMAT+ (Chen et al., 2023b) 57.6\nRETRO (Borgeaud et al., 2022) 45.5 HGN (Fang et al., 2019) 59.7", "metadata": {"id": "922bf8ab62b967d22c73875c5c7b849ecf15a68e", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 8, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "RETRO (Borgeaud et al., 2022) 45.5 HGN (Fang et al., 2019) 59.7\nRePAQ (Lewis et al., 2021) 47.8 PathRetriever (Asai et al., 2019) 60.0\nFID (Izacard & Grave, 2020b) 51.4 HopRetrieve (Li et al., 2021) 62.1\nEMDR2 (Singh et al., 2021) 52.5 MDR (Xiong et al., 2020b) 62.3\nFID-KD (Izacard & Grave, 2021) 54.7 HopRetrieve-plus (Li et al., 2021) 66.5\nR2-D2 (Fajcik et al., 2021) 55.9 AISO (Zhu et al., 2021) 68.1\nAtlas (Izacard et al., 2022) 64.0 COS (Ma et al., 2023) 68.2\nNo Fine-tuning RAG No Fine-tuning RAG\nREPLUG (Shi et al., 2023) 44.7 DSP (Khattab et al., 2022) 51.4", "metadata": {"id": "337c90219ca39b6b38cc1571ffc9cccd7830c03b", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 8, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "REPLUG (Shi et al., 2023) 44.7 DSP (Khattab et al., 2022) 51.4\nREPLUG + LSR (Shi et al., 2023) 45.5 PromptRank (Khalifa et al., 2023) 55.7\nLongRAG (Gemini-1.5-Pro; Recall 4 units) 58.6 LongRAG (Gemini-1.5-Pro; Recall 8 units) 57.5\nLongRAG (GPT-4o; Recall 4 units) 62.7 LongRAG (GPT-4o; Recall 8 units) 64.3\nTable 5: The tables show the QA results on the NQ test dataset (left) and Hotpot-QA dev set (right). We\ncompare the results with three groups of baselines: closed-book, which involves directly prompting state-of-", "metadata": {"id": "aa7ee4168a5df8a89db825e9f254ea45047a07bb", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 8, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "the-art LLMs with 16-shot in-context examples; fully-supervised RAG, where the RAG framework is used\nandthemodelisfullysupervisedandtrainedonthetrainingdata;andNoFine-tuningRAG,whichemploys\nthe RAG framework without any tuning.\nthe standard exact match rate definition to more fairly evaluate LongRAG’s performance. More details can\nbe found in Section A.2.\nFor NQ and HotpotQA, we compare our model with several groups of strong previous models as baselines.\nThefirstgroupis“Closed-Book”: Thesebaselinesmeanthatnoretrievalcomponentisused;instead,state-", "metadata": {"id": "6740358cf3fd37050e83b88e14c17e226d52266b", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 8, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "Thefirstgroupis“Closed-Book”: Thesebaselinesmeanthatnoretrievalcomponentisused;instead,state-\nof-the-art LLMs are employed to directly obtain the final result. We evaluate our results on Gemini-1.5-pro\n(Reid et al., 2024), Claude-3-Opus (Anthropic, 2024) and GPT-4-Turbo (Achiam et al., 2023). All models\nareevaluatedon16-shotin-contextlearningwithdirectprompting;Thesecondgroupis“Fully-supervised\nRAG”, and these baselines involve full-supervised fine-tuning on the training dataset. The third group is", "metadata": {"id": "ce8693290805ff0623585190e3033bde20619993", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 8, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "“No Fine-tuning RAG”, and these baselines doesn’t involve any supervised fine-tuning. The QA results\non NQ and HotpotQA are presented in Table 5. On the NQ dataset, LongRAG achieves a 62.7 exact match\nrate,whichisonparofthestrongestfine-tunedRAGmodellikeAtlas. OntheHotpotQAdataset,LongRAG\nachieves a 64.3 exact match rate, which is also close to the SoTA fully-supervised RAG frameworks.\n3.4 Full QA Performance on non-Wikipedia-based Datasets\nFor datasets that generate long answers, such as Qasper and MultifieldQA-en, we use the token-level F1", "metadata": {"id": "62a77008af4fbbaeaefb655f8350172a6c44b11c", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 8, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "score (F1) as the evaluation metric. For Qasper and MultifieldQA-en, since we repurpose the datasets from\nsingle-document QA to a RAG task, we do not directly compare the results with previous models. Instead,\nwecomparetheperformanceoftraditionalRAG,whichoperateson200-tokenpassages,withourLongRAG,\nwhich operates on entire documents ranging from 4K to 6K tokens. The results are shown in Table 6. We\nobserve that using long retrieval units at the whole document level performs better than using hundreds of", "metadata": {"id": "88611f26a467d864ed3abcf67c196c881bbfe90e", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 8, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "short chunked retrieval units. On the Qasper dataset, gathering 100 short retrieval units of 200 tokens each\ninto the reader achieves a 22.6% F1 score, while using a single long retrieval unit of 5K tokens achieves a\n26.3% F1 score. Similarly, on the MultifieldQA-en dataset, gathering 100 short retrieval units of 200 tokens\n8", "metadata": {"id": "bbb59da5a617f29f5235f526f717d0f642f2dc93", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 8, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "each into the reader results in a 51.3% F1 score, whereas using five long retrieval units of 7K tokens each\nresults in a 57.5% F1 score.\nRetrieval Unit Num of Retrieval Units Qasper MutilfieldQA-en\n1 15.5 38.9\n10 20.6 47.3\nPassage\n100 22.6 51.3\n200 21.9 50.9\n1 26.3 49.4\n2 25.9 50.2\nDocument\n5 23.9 57.5\n10 21.6 56.8\nTable 6: This table presents the QA results on two non-Wiki datasets: Qasper and MultifieldQA-en. The\nresults are evaluated based on token-level F1. Both datasets contain long documents, averaging at least 4K", "metadata": {"id": "e431ec8ebc3708af9e192ed1f265f4f868c28da5", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 9, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "tokens. The results demonstrate that our LongRAG, which operates on long retrieval units, achieves better\nperformance compared to traditional RAG, which operates on short retrieval units.\n3.5 Ablation Studies\nWeperformseveralin-depthablationtounderstandwhataretheimportantfactorsinourLongRAGsystem\nincluding “unit size” and “reader variant”.\nNQ\nHotpotQA\nFigure 3: This figure compares different settings of LongRAG, using 200 test cases from the test set to\nevaluate various retrieval unit selections, demonstrating the effectiveness of our LongRAG design. The", "metadata": {"id": "deee7c63888c1eef49e8d11efb5a172f70a5608e", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 9, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "upper part of the figure shows the NQ dataset, while the lower part displays the HotpotQA dataset. On\nthe left, it illustrates how the overall performance changes with different settings of retrieval unit size and\nthe number of units fed into the reader; on the right, it shows that the end performance does not increase\nmonotonically with the recall score, and LongRAG is more robust to the influence of “hard negatives” as\nthe context length of the reader increases.\n9", "metadata": {"id": "bfba354803f238af016085eb7182920bd450aee6", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 9, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "65\n60\n55\n50\n45\nGe\nm\n4\nin\n0 i-1.5-pro GPT-4-Turbo GPT-4\nC\no laude-3-\nC\nO\nl\np\na\nu\nu\ns de-3.5-S\nD\non\nee\nn\np\ne\nS\nt eek-V2-Chat\n)%(\nhctaM\ntcaxE\n61.5% 62% 61%\n59.5% 60%\n51%\nFigure 4: This figure compares different readers of LongRAG on the NQ dataset. This table leverages 200\ntest cases from the test set to help compare performance using different readers.\nRetrieval Unit Selection Figure 3 compare different retrieval unit settings of LongRAG, specifically\nfocusing on the selection of retrieval unit granularity and the optimal number of retrieval units used in", "metadata": {"id": "fba38255136cfdcf0aa574b580ccd8e8ecf62a7d", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 10, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "the reader. We have two observations: First, regardless of which retrieval unit is selected, there will be a\nturning point where feeding more retrieval units into the reader becomes detrimental. This is due to the\nexcessive burden placed on the reader, preventing it from effectively understanding and extracting relevant\ninformation from the long context. Taking NQ as an example: for passage-level retrieval units, the turning\npoint occurs between 100 and 200; for document-level retrieval units, the turning point is between 5 and", "metadata": {"id": "d26e060c6ab875a3d701dc4ea206d874759a2147", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 10, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "10; and for grouped documents level, the turning point is between 4 and 8. In general, the most suitable\ncontext length fed into the reader is around 30K tokens. Second, using long retrieval units shows improved\nperformance when comparing passage-level retrieval units with document-level or grouped document-level\nretrieval units.\nRecall vs. EM In Figure 3, we compare the relationship between retrieval recall and end performance\nacross varying context lengths for different retrieval unit selections. We observe that using fewer retrieval", "metadata": {"id": "bade187ae490f0e1d459253b6f9ae927c7a1b26f", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 10, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "unitsinthereaderwithlongerretrievalunitsdesignreducestheintroductionofdistractorsorhardnegatives\nunder a given length budget. Consequently, the end performance does not increase monotonically with the\nrecall score. In the future, with advancements in long embedding models and improved retrieval recall for\nlong retrieval units, we can expect better end performance.\nReader Model In Figure 4, we compare the performance of six different readers: Gemini-1.5-pro, GPT-\n4-Turbo, GPT-4o, Claude-3-Opus, Claude-3.5-Sonnet and DeepSeek-V2-Chat. The results indicate that", "metadata": {"id": "6320860537a6897fb098f51f297d113edb33c62e", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 10, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "4-Turbo, GPT-4o, Claude-3-Opus, Claude-3.5-Sonnet and DeepSeek-V2-Chat. The results indicate that\nGPT-4o achieves the highest exact match score on the 200 test questions of the NQ dataset among the\nthree models. This suggests that GPT-4o is the most effective in the role of a long reader in the LongRAG\nframework. The enhanced performance of GPT-4o can be attributed to its superior ability to process and\ncomprehendlengthycontexts,ensuringthatcrucialinformationisaccuratelyextracted. Therefore,wemainly", "metadata": {"id": "a66ccda0f209a13922021bdfb4c72716c2f75f36", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 10, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "comprehendlengthycontexts,ensuringthatcrucialinformationisaccuratelyextracted. Therefore,wemainly\nreport the GPT-4o results in our main table. Besides, Gemini-1.5-pro, GPT-4-Turbo, Claude-3-Opus, and\nClaude-3.5-Sonnetcouldachieveverysimilarresults. Thesestate-of-the-artblackboxLLMsarealsoeffective\nreaders within the LongRAG framework. Deepseek-V2-Chat is one of the best open-source LLMs, but its\nperformance degrades significantly compared to the previous five black-box LLMs. The above experiments", "metadata": {"id": "1693062930a4ad22130fbee601fcd5904b5c46bf", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 10, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "demonstrate that our current framework depends on the long-context understanding ability of LLMs, and\nwe still have a long way to go in harnessing open-source LLMs within our framework.\n10", "metadata": {"id": "2b35a0698e3f891eb4b4b6a013f63bcf282feecd", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 10, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "4 Related Work\n4.1 Retrieval-Augmented Generation.\nAugmentinglanguagemodelswithinformationretrievedfromlargecorporahasbecomeapopularandeffec-\ntiveapproachforknowledge-intensivetasks,particularlyopen-domainquestionanswering. Thepredominant\narchitecture follows a retriever-reader style (Chen et al., 2017; Guu et al., 2020), where the input query re-\ntrievesinformationfromacorpus,andalanguagemodelusesthisinformationasadditionalcontexttomake\na final prediction. Recent work has focused on improving the retriever (Karpukhin et al., 2020; Xiong et al.,", "metadata": {"id": "abdcacc64dea0aa8ee7dba5bb0b3d95863bb747c", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 11, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "2020a; Qu et al., 2020; Xiong et al., 2020b; Khalifa et al., 2023), enhancing the reader (Izacard & Grave,\n2020b;Chengetal.,2021;Yuetal.,2021;Borgeaudetal.,2022), fine-tuningtheretrieverandreaderjointly\n(Yu,2022;Izacardetal.,2022;Singhetal.,2021;Izacard&Grave,2020a),andintegratingtheretrieverwith\nthe black-box language model (Yu et al., 2023; Shi et al., 2023; Trivedi et al., 2022). However, the impact\nof document granularity on the effectiveness and efficiency of the retrieval-augmented generation pipeline\nremains underexplored.\n4.2 Long Context Large Language Models.", "metadata": {"id": "1eb66ec679eb8ab07e8faaf2f114dcd4bc06f13c", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 11, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "remains underexplored.\n4.2 Long Context Large Language Models.\nThe effectiveness of Transformer-based models is hindered by the quadratic increase in computational cost\nrelative to sequence length, especially when dealing with long context inputs. In order to solve this issue,\ndifferentapproacheshavebeenproposedtomitigatecomputationalissues,includingslidingmemorywindow\nand chunk segmentation (Hao et al., 2022; Ratner et al., 2023; Zhu et al., 2024b). FlashAttention (Dao\net al., 2022) has also been a pivotal strategy to significantly reduce the memory footprint to almost linear", "metadata": {"id": "c4b84041b19cb5d4b926cc25f9cbbea6b130557c", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 11, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "w.r.t sequence length.\nTo enable length extrapolation, RoPE (Su et al., 2021) and AliBI (Press et al., 2021) position encodings\nhave shown potential to enable length extrapolation, which have been widely used in the literature. Recent\nendeavors have explored diverse strategies to tackle this challenge, which is mainly Position reorganization\n(Jin et al., 2024; An et al., 2024), Position interpolation (Chen et al., 2023a; Peng et al., 2023; Liu et al.,\n2024). Furthermore, alternative architectures beyond the Transformer have been explored to handle long", "metadata": {"id": "492c35670dd628b22149d52517e127cd3e854f5f", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 11, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "inputs more naturally. These diverse approaches claim that they can enhance the capabilities of LLMs in\nprocessing long context inputs more efficiently.\n4.3 Long Context Embedding\nRecenteffortsalsoincreasedthecontextlengthforembeddingmodels,extendingthesupportedtextsnippet\nlengthfromalimitof512tokensto32ktokens. Typically,thedevelopmentoflong-contextembeddingmodels\ninvolvesfirstobtainingalong-contextbackbonemodel. Thiscanbeachievedeitherbypre-trainingwithlong\ninputs from scratch (Günther et al., 2023; Nussbaum et al., 2024; Chen et al., 2024) or by utilizing existing", "metadata": {"id": "c962bc2ab79aec02998d44bfb1e6bfbe8956f14f", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 11, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "large language models that support longer context (Wang et al., 2023). Additionally, some works extend\nthe capabilities of existing embedding models to handle long contexts by applying LLM content window\nextension methods on embedding models (Zhu et al., 2024a; Peng & Quesnelle, 2023), or by employing\nstate-space encoder models (Saad-Falcon et al., 2024).\n5 Conclusion\nIn this paper, we propose a new framework, LongRAG, to alleviate the imbalance between the burden of\ntheretriever. TheLongRAGframeworkconsistsofa“longretriever”anda“longreader”componentontop", "metadata": {"id": "f32a02cd387335a287719b418c55a66ca9746fd5", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 11, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "theretriever. TheLongRAGframeworkconsistsofa“longretriever”anda“longreader”componentontop\nof the 4K-token retrieval units. Our proposed framework can significantly reduce the corpus size, enabling\nstrongretrievalrecallusingonlyafewtopunits,therebyminimizingnoisefromhardnegatives. Ontheother\nhand, the long retrieval unit preserves the semantic integrity of each document. We test our framework on\nfour end-to-end question answering tasks and demonstrate its superior performance without any training.\nWe believe LongRAG can pave the road for the modern RAG system design.\n11", "metadata": {"id": "c11bfbb690825e990a37dd49b0b9f45a57b3d3be", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 11, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "References\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo\nAlmeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv\npreprint arXiv:2303.08774, 2023.\nChenxinAn,FeiHuang,JunZhang,ShansanGong,XipengQiu,ChangZhou,andLingpengKong. Training-\nfree long-context scaling of large language models. arXiv preprint arXiv:2402.17463, 2024.\nAnthropic. Introducing the next generation of claude. 2024.\nAkari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caiming Xiong. Learning to", "metadata": {"id": "1a6634948400869ba0443429f045da5b9fae236a", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 12, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caiming Xiong. Learning to\nretrieve reasoning paths over wikipedia graph for question answering. arXiv preprint arXiv:1911.10470,\n2019.\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu,\nAohanZeng,LeiHou,etal. Longbench: Abilingual,multitaskbenchmarkforlongcontextunderstanding.\narXiv preprint arXiv:2308.14508, 2023.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican,", "metadata": {"id": "0e8b853a7f623177b57ce86642dd6d8ad7bbfd7b", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 12, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican,\nGeorge Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving\nlanguage models by retrieving from trillions of tokens. In International conference on machine learning,\npp. 2206–2240. PMLR, 2022.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading wikipedia to answer open-domain\nquestions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pp. 1870–1879, 2017.", "metadata": {"id": "813a7c137d7694d519f8dfbeddf37f77f3640bc8", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 12, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "(Volume 1: Long Papers), pp. 1870–1879, 2017.\nJianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. Bge m3-embedding: Multi-\nlingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation. arXiv\npreprint arXiv:2402.03216, 2024.\nShouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large\nlanguage models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023a.\nWenhu Chen, Pat Verga, Michiel de Jong, John Wieting, and William Cohen. Augmenting pre-trained lan-", "metadata": {"id": "ae490616a505ac2ce372dba67bb6b8a461db95ed", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 12, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "guagemodelswithqa-memoryforopen-domainquestionanswering. InProceedings of the 17th Conference\nof the European Chapter of the Association for Computational Linguistics, pp. 1597–1610, 2023b.\nHao Cheng, Yelong Shen, Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Unitedqa: A\nhybrid approach for open domain question answering. arXiv preprint arXiv:2101.00178, 2021.\nZhuyunDaiandJamieCallan.Deepertextunderstandingforirwithcontextualneurallanguagemodeling.In\nProceedings of the 42nd international ACM SIGIR conference on research and development in information\nretrieval, pp. 985–988, 2019.", "metadata": {"id": "d75ff942e5cf78d7bd571c8547d59ea5c2fb9ea6", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 12, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "retrieval, pp. 985–988, 2019.\nTriDao,DanFu,StefanoErmon,AtriRudra,andChristopherRé.Flashattention: Fastandmemory-efficient\nexact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344–16359,\n2022.\nPradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A Smith, and Matt Gardner. A dataset of\ninformation-seekingquestionsandanswersanchoredinresearchpapers. arXiv preprint arXiv:2105.03011,\n2021.\nBhuwan Dhingra, Manzil Zaheer, Vidhisha Balachandran, Graham Neubig, Ruslan Salakhutdinov,", "metadata": {"id": "67086507ec92410972c898ebf7414b9bf4b3557b", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 12, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "2021.\nBhuwan Dhingra, Manzil Zaheer, Vidhisha Balachandran, Graham Neubig, Ruslan Salakhutdinov,\nand William W Cohen. Differentiable reasoning over a virtual knowledge base. arXiv preprint\narXiv:2002.10640, 2020.\nMartin Fajcik, Martin Docekal, Karel Ondrej, and Pavel Smrz. R2-d2: A modular baseline for open-domain\nquestion answering. In Findings of the Association for Computational Linguistics: EMNLP 2021, pp.\n854–870, 2021.\n12", "metadata": {"id": "91ed4b39df1acd49713ab24efaeddba3d8a5d71b", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 12, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "YuweiFang, SiqiSun,ZheGan,RohitPillai, ShuohangWang,andJingjingLiu. Hierarchicalgraphnetwork\nfor multi-hop question answering. arXiv preprint arXiv:1911.03631, 2019.\nLuyu Gao, Xueguang Ma, Jimmy J. Lin, and Jamie Callan. Tevatron: An efficient and flexible toolkit for\ndense retrieval. ArXiv, abs/2203.05765, 2022.\nMichael Günther, Jackmin Ong, Isabelle Mohr, Alaeddine Abdessalem, Tanguy Abel, Mohammad Kalim\nAkram, Susana Guzman, Georgios Mastrapas, Saba Sturua, Bo Wang, et al. Jina embeddings 2: 8192-", "metadata": {"id": "296b50efdff8efa226804d76d86d52e7b1b9f8a3", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 13, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "Akram, Susana Guzman, Georgios Mastrapas, Saba Sturua, Bo Wang, et al. Jina embeddings 2: 8192-\ntoken general-purpose text embeddings for long documents. arXiv preprint arXiv:2310.19923, 2023.\nKelvinGuu,KentonLee,ZoraTung,PanupongPasupat,andMingweiChang.Retrievalaugmentedlanguage\nmodel pre-training. In International conference on machine learning, pp. 3929–3938. PMLR, 2020.\nYaru Hao, Yutao Sun, Li Dong, Zhixiong Han, Yuxian Gu, and Furu Wei. Structured prompting: Scaling\nin-context learning to 1, 000 examples. ArXiv, abs/2212.06713, 2022.", "metadata": {"id": "9e5826d02d359d808390f8a6cf49b169397fcf1c", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 13, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "in-context learning to 1, 000 examples. ArXiv, abs/2212.06713, 2022.\nGautier Izacard and Edouard Grave. Distilling knowledge from reader to retriever for question answering.\narXiv preprint arXiv:2012.04584, 2020a.\nGautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models for open domain\nquestion answering. arXiv preprint arXiv:2007.01282, 2020b.\nGautier Izacard and Edouard Grave. Distilling knowledge from reader to retriever for question answering.\nIn ICLR 2021-9th International Conference on Learning Representations, 2021.", "metadata": {"id": "5107cc217acf75f2647368e3ffe2adcdefa9e0d2", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 13, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "In ICLR 2021-9th International Conference on Learning Representations, 2021.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane A. Yu, Ar-\nmandJoulin, SebastianRiedel, andEdouardGrave. Few-shotlearningwithretrievalaugmentedlanguage\nmodels. ArXiv, abs/2208.03299, 2022.\nHongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng Jiang, Zirui Liu, Chia-Yuan Chang, Huiyuan Chen,\nand Xia Hu. Llm maybe longlm: Self-extend llm context window without tuning. arXiv preprint\narXiv:2401.01325, 2024.", "metadata": {"id": "8a041764381aa7e366097abfad14a49911f71d69", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 13, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "arXiv:2401.01325, 2024.\nJeffJohnson,MatthijsDouze,andHervéJégou. Billion-scalesimilaritysearchwithgpus. IEEETransactions\non Big Data, 7(3):535–547, 2019.\nVladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen,\nand Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint\narXiv:2004.04906, 2020.\nMuhammadKhalifa, LajanugenLogeswaran, MoontaeLee, HonglakLee, andLuWang. Few-shotreranking\nfor multi-hop qa via language model prompting. arXiv preprint arXiv:2205.12650, 2022.", "metadata": {"id": "0f021f4b07f81cbae67ed21e42055d99914f7e15", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 13, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "for multi-hop qa via language model prompting. arXiv preprint arXiv:2205.12650, 2022.\nMuhammadKhalifa, LajanugenLogeswaran, MoontaeLee, HonglakLee, andLuWang. Few-shotreranking\nfor multi-hop qa via language model prompting. arXiv preprint arXiv:2205.12650, 2023.\nOmar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei\nZaharia. Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive\nnlp. arXiv preprint arXiv:2212.14024, 2022.", "metadata": {"id": "04bd47478153a193123f4d70d407c55efb4412bd", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 13, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "nlp. arXiv preprint arXiv:2212.14024, 2022.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a benchmark for\nquestion answering research. Transactions of the Association for Computational Linguistics, 7:453–466,\n2019.\nPatrickLewis,EthanPerez,AleksandaraPiktus,FabioPetroni,VladimirKarpukhin,NamanGoyal,Heinrich\nKuttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. Retrieval-", "metadata": {"id": "d08bd9d4ad1be6a991e3a72559851e62dee51c48", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 13, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "Kuttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. Retrieval-\naugmented generation for knowledge-intensive nlp tasks. ArXiv, abs/2005.11401, 2020.\n13", "metadata": {"id": "19788ef39679d30c09c92f64891f7f88eada3b40", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 13, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "Patrick Lewis, Yuxiang Wu, Linqing Liu, Pasquale Minervini, Heinrich Küttler, Aleksandra Piktus, Pontus\nStenetorp, and Sebastian Riedel. Paq: 65 million probably-asked questions and what you can do with\nthem. Transactions of the Association for Computational Linguistics, 9:1098–1115, 2021.\nShaobo Li, Xiaoguang Li, Lifeng Shang, Xin Jiang, Qun Liu, Chengjie Sun, Zhenzhou Ji, and Bingquan\nLiu. Hopretriever: Retrievehopsoverwikipediatoanswercomplexquestions. InProceedings of the AAAI\nconference on artificial intelligence, volume 35, pp. 13279–13287, 2021.", "metadata": {"id": "2107d1408fb57365cabad2eee700586069891ccd", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 14, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "conference on artificial intelligence, volume 35, pp. 13279–13287, 2021.\nJiahengLiu,ZhiqiBai,YuanxingZhang,ChenchenZhang,YuZhang,GeZhang,JiakaiWang,HaoranQue,\nYukangChen,WenboSu,etal. Eˆ2-llm: Efficientandextremelengthextensionoflargelanguagemodels.\nFindings of ACL 2024, 2024.\nKaixin Ma, Hao Cheng, Yu Zhang, Xiaodong Liu, Eric Nyberg, and Jianfeng Gao. Chain-of-skills: A\nconfigurable model for open-domain question answering. In The 61st Annual Meeting Of The Association\nFor Computational Linguistics, 2023.", "metadata": {"id": "0bbb9d65bb8b9f580c3bc9c93e13feb3cdfa172f", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 14, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "For Computational Linguistics, 2023.\nGrégoireMialon, RobertoDessì, MariaLomeli, ChristoforosNalmpantis, RamPasunuru, RobertaRaileanu,\nBaptiste Rozière, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, et al. Augmented language models: a\nsurvey. arXiv preprint arXiv:2302.07842, 2023.\nZach Nussbaum, John X Morris, Brandon Duderstadt, and Andriy Mulyar. Nomic embed: Training a\nreproducible long context text embedder. arXiv preprint arXiv:2402.01613, 2024.\nOpenAI. Hello gpt4-o. 2024.\nBowen Peng and Jeffrey Quesnelle. Ntk-aware scaled rope allows llama models to have extended (8k+)", "metadata": {"id": "2d22f7807f4a4f2ace7e7513258ce0b135dea69c", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 14, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "Bowen Peng and Jeffrey Quesnelle. Ntk-aware scaled rope allows llama models to have extended (8k+)\ncontext size without any fine-tuning and minimal perplexity degradation. https://www.reddit.com/r/\nLocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have, 2023.\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension\nof large language models. arXiv preprint arXiv:2309.00071, 2023.\nOfir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input", "metadata": {"id": "12db305662e55d8f7836db197311895eda18de8e", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 14, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "length extrapolation. In International Conference on Learning Representations, 2021.\nYingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and\nHaifeng Wang. Rocketqa: An optimized training approach to dense passage retrieval for open-domain\nquestion answering. arXiv preprint arXiv:2010.08191, 2020.\nNir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Inbal Magar, Omri Abend, Ehud Karpas, Amnon\nShashua,KevinLeyton-Brown,andYoavShoham. Parallelcontextwindowsforlargelanguagemodels. In", "metadata": {"id": "97426dad1c0b019c51d57cdfea7d75c50cb30bf7", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 14, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "Shashua,KevinLeyton-Brown,andYoavShoham. Parallelcontextwindowsforlargelanguagemodels. In\nAnnaRogers,JordanBoyd-Graber,andNaoakiOkazaki(eds.),Proceedings of the 61st Annual Meeting of\ntheAssociationforComputationalLinguistics(Volume1: LongPapers),pp.6383–6402,Toronto,Canada,\nJuly 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.352.\nMachelReid,NikolaySavinov,DenisTeplyashin,DmitryLepikhin,TimothyLillicrap,Jean-baptisteAlayrac,\nRadu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multi-", "metadata": {"id": "dd68ac369bfc5f84114a197b4fac4abf641fce17", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 14, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "modal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024.\nJon Saad-Falcon, Daniel Y Fu, Simran Arora, Neel Guha, and Christopher Ré. Benchmarking and building\nlong-context retrieval models with loco and m2-bert. arXiv preprint arXiv:2402.07440, 2024.\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and\nWen-tauYih. Replug: Retrieval-augmentedblack-boxlanguagemodels. arXiv preprint arXiv:2301.12652,\n2023.\nDevendraSingh,SivaReddy,WillHamilton,ChrisDyer,andDaniYogatama. End-to-endtrainingofmulti-", "metadata": {"id": "14ee2d4506ed060f857884380a5778f3faf7b7cb", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 14, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "2023.\nDevendraSingh,SivaReddy,WillHamilton,ChrisDyer,andDaniYogatama. End-to-endtrainingofmulti-\ndocument reader and retriever for open-domain question answering. Advances in Neural Information\nProcessing Systems, 34:25968–25981, 2021.\n14", "metadata": {"id": "abd6f89e2e098a4e25daa40f453465ab2d6dd5f1", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 14, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced\ntransformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021.\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Interleaving retrieval with\nchain-of-thoughtreasoningforknowledge-intensivemulti-stepquestions. arXivpreprintarXiv:2212.10509,\n2022.\nLiang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. Improving text\nembeddings with large language models. arXiv preprint arXiv:2401.00368, 2023.", "metadata": {"id": "d457a701528a68e246901881b3c445c443ce63a6", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 15, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "embeddings with large language models. arXiv preprint arXiv:2401.00368, 2023.\nShitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. C-pack: Packaged resources to advance\ngeneral chinese embedding. arXiv:2309.07597, 2023.\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold\nOverwijk. Approximate nearest neighbor negative contrastive learning for dense text retrieval. arXiv\npreprint arXiv:2007.00808, 2020a.\nWenhan Xiong, Xiang Lorraine Li, Srini Iyer, Jingfei Du, Patrick Lewis, William Yang Wang, Yashar", "metadata": {"id": "e3796702345760ea6b8786708b6d4db1151a2f36", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 15, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "Wenhan Xiong, Xiang Lorraine Li, Srini Iyer, Jingfei Du, Patrick Lewis, William Yang Wang, Yashar\nMehdad, Wen-tau Yih, Sebastian Riedel, Douwe Kiela, et al. Answering complex open-domain questions\nwith multi-hop dense retrieval. arXiv preprint arXiv:2009.12756, 2020b.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and\nChristopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering.\narXiv preprint arXiv:1809.09600, 2018.\nDonghan Yu, Chenguang Zhu, Yuwei Fang, Wenhao Yu, Shuohang Wang, Yichong Xu, Xiang Ren, Yiming", "metadata": {"id": "35bceac212434a1449c29b643759c1dcc61033c1", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 15, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "Donghan Yu, Chenguang Zhu, Yuwei Fang, Wenhao Yu, Shuohang Wang, Yichong Xu, Xiang Ren, Yiming\nYang, andMichaelZeng. Kg-fid: Infusingknowledgegraphinfusion-in-decoderforopen-domainquestion\nanswering. arXiv preprint arXiv:2110.04330, 2021.\nWenhao Yu. Retrieval-augmented generation across heterogeneous knowledge. In Proceedings of the 2022\nConference of the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies: Student Research Workshop, pp. 52–58, 2022.\nWenhaoYu,ZhihanZhang,ZhenwenLiang,MengJiang,andAshishSabharwal. Improvinglanguagemodels", "metadata": {"id": "fd3803f18171a2e18c754720d849827f2d7e9e19", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 15, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "WenhaoYu,ZhihanZhang,ZhenwenLiang,MengJiang,andAshishSabharwal. Improvinglanguagemodels\nvia plug-and-play retrieval feedback. arXiv preprint arXiv:2305.14002, 2023.\nChen Zhao, Chenyan Xiong, Corby Rosset, Xia Song, Paul Bennett, and Saurabh Tiwary. Transformer-xh:\nMulti-evidence reasoning with extra hop attention. In International Conference on Learning Representa-\ntions, 2019.\nDawei Zhu, Liang Wang, Nan Yang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li. Longembed:\nExtending embedding models for long context retrieval. arXiv preprint arXiv:2404.12096, 2024a.", "metadata": {"id": "d280e28fc2f62721cac46f329e90b3825a3c3f13", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 15, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "Extending embedding models for long context retrieval. arXiv preprint arXiv:2404.12096, 2024a.\nDaweiZhu,NanYang,LiangWang,YifanSong,WenhaoWu,FuruWei,andSujianLi. PoSE:Efficientcon-\ntextwindowextensionofLLMsviapositionalskip-wisetraining. InThe Twelfth International Conference\non Learning Representations, 2024b.\nYunchang Zhu, Liang Pang, Yanyan Lan, Huawei Shen, and Xueqi Cheng. Adaptive information seeking for\nopen-domainquestionanswering. InProceedings of the 2021 Conference on Empirical Methods in Natural\nLanguage Processing, pp. 3615–3626, 2021.\n15", "metadata": {"id": "1ece060f17abd0f4f7d0df085deb1d6ee4632060", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 15, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "A Appendix\nA.1 Prompts Template for Long Context Reader\nWe have put out prompts used for the experiments in Table 7. For the closed-book method, we use 16-shot\nin-context examples. For LongRAG, we use a two-turn approach to extract the final answer. In the first\nturn,thelongretrievedcontextandthequestionareconcatenatedasinput,andwedonotuseanyin-context\nexamples here due to the context being around 30K tokens. Empirically, we found it beneficial to let the\nreadergeneratealongeranswerinitially,typicallyrangingfromafewwordstoafewsentences. Inthesecond", "metadata": {"id": "d377566b65413d0255bf3f68ca876267007e50cb", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 16, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "readergeneratealongeranswerinitially,typicallyrangingfromafewwordstoafewsentences. Inthesecond\nturn, we use 8-shot in-context examples to guide the reader in further extracting the most important part\nof the long answer as the short answer, which is typically just a few words.\nMethod Prompt\nClosed- Herearesomeexamplesofquestionsandtheircorrespondinganswer,eachwith\nBook a “Question” field and an “Answer” field. Answer the question directly and\ndon’t output other thing.\n“Question”: ...“Answer”: ...\n“Question”: ...“Answer”: ...\n...\n“Question”: ...“Answer”: ...\nAnswer the following question.", "metadata": {"id": "c258174c8b84b29992f90908e460cadcf5ad3373", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 16, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "“Question”: ...“Answer”: ...\n...\n“Question”: ...“Answer”: ...\nAnswer the following question.\n“Question”: who is the owner of reading football club “Answer”:\nLongRAG Turn 1: Go through the following context and then answer the question. The\ncontext is a list of Wikipedia documents, ordered by title: ....\nEach Wikipedia document contains a title field and a text field. The context\nis:\n“Title”: ...“Text”: ...\n“Title”: ...“Text”: ...\n...\n“Title”: ...“Text”: ...\nFind the useful documents from the context, then answer the question: when", "metadata": {"id": "05a9730f769bc74abc220464d0c63f70c931d484", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 16, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "“Title”: ...“Text”: ...\nFind the useful documents from the context, then answer the question: when\ndid the philadelphia eagles play in the super bowl last. Answer the question\ndirectly. Your response should be very concise.\nTurn 2: You have been provided with a question and its long answer. Your\ntask is to derive a very concise short answer from the given long answer. It’s\nimportanttoensurethattheoutputshortanswerremainsassimpleaspossible.\nHere a few examples:\n“Question”: ...“Long Answer”: ...“Short Answer”: ...\n“Question”: ...“Long Answer”: ...“Short Answer”: ...\n...", "metadata": {"id": "f425763b9fc4c3d8ac1adb99b323e98bfdd94927", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 16, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "“Question”: ...“Long Answer”: ...“Short Answer”: ...\n...\n“Question”: ...“Long Answer”: ...“Short Answer”: ...\nExtract the short answer of the following question and long answer:\n“Question”: whendidthephiladelphiaeaglesplayinthesuperbowllast“Long\nAnswer”: The Philadelphia Eagles last played in the Super Bowl on February\n4, 2018, in Super Bowl LII. “Short Answer”:\nTable 7: Here are the prompts we used for all the experiments. For the closed-book method, we use 16-shot\nin-context examples. For LongRAG, we use a two-turn approach to extract the final answer. The first turn", "metadata": {"id": "ff41736cbf20222f6ccc0c6f1e521923acdb6aae", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 16, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "doesn’t require any in-context examples and generate a longer answer, typically ranging from a few words\nto a few sentences. In the second turn, we use 8-shot in-context examples to calibrate and extract the exact\nshort answer, which is typically just a few words.\n16", "metadata": {"id": "f1c85ed5720b3fbebbd43b04f9be73666d7f1dc4", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 16, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "A.2 Refined Metric\nThe most standard metric used in open-domain extractive question answering tasks is EM (Exact Match),\nsince the correct answer must be a substring within the corpus. In our framework, since the long retrieved\ncontext, which contains multiple highly-related documents to the given query, is fed into the reader, there is\na much higher possibility that an alias of the ground truth exists in the context and can be extracted by the\nreader. As shown in Table 8, although LongRAG’s prediction doesn’t exactly match the ground truth, it’s", "metadata": {"id": "6752c02e62c65963634bbfc4cd75ff45a2f02c38", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 17, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "obvious that LongRAG’s prediction is correct. To better and more fairly evaluate LongRAG’s performance,\nwe have refined the EM metric slightly. We recognize it as an exact match if the prediction is less than five\ntokens(indicatingthattheshortanswerissuccessfullyextractedasdescribedinSectionA.1)andtheground\ntruth is a substring of the prediction or vice versa. We have also manually verified that this refined metric\nindeed captures aliases or other forms of the ground truth. For the fully-supervised RAG baselines used in", "metadata": {"id": "18697556e0be7372d457597505747bb706d24b40", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 17, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "our paper, given that they are fine-tuned on the training data and the retrieval unit is a small snippet, we\nbelieve that the difference won’t be significant when using the refined EM.\nQuestion Ground truth LongRAG prediction\nwhere does the bob and tom show broadcast from Indianapolis , Indiana Indianapolis\nwho has given the theory of unbalanced economic growth Hirschman Albert O. Hirschman\nwhen does season 6 of the next step start 2018 September 29, 2018\nwhat was the precursor to the present day internet the ARPANET project ARPANET", "metadata": {"id": "9a5e3b48b057bbecc5bb4ba380649d256acd26e3", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 17, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "what was the precursor to the present day internet the ARPANET project ARPANET\nTable 8: Some examples demonstrate that LongRAG has extracted aliases or different forms of the ground\ntruth.\n17", "metadata": {"id": "f223c3988951504c884f73d1a828b5fbcec60c92", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 17, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "A.3 Group Documents Algorithm\nIn this section, we provide an example algorithm used to formulate long retrieval units by grouping multiple\nshort documents, which we applied in the NQ and HotpotQA experiments in our paper. In the algorithm,\nwhether two documents are related can be determined by any reasonable function, such as hyperlinks, word\nfrequency,orstructuralinformationfromthedataset. InthetwoWikipedia-relatedquestion-answeringtasks\nin our paper, NQ and HotpotQA, we use the hyperlinks embedded in the text to describe the relationships\nbetween documents.", "metadata": {"id": "c8da470926b2f502057e843360e94c573aa2beba", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 18, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "between documents.\nAlgorithm 1 Example Group Documents Algorithm\nInput: S (max number of tokens per group), D (list of documents), adj[d] (related documents for each\ndocument d), deg(d) (number of related documents for each document d)\nOutput: G (set of groups)\nSort D from low degree to high degree based on deg(d)\nInitialize an empty set of groups G\nfor each document d in D do\nrelated_groups←∅\nfor each related document r in adj[d] do\nfor each group g in G do\nif r ∈g then\nrelated_groups←related_groups∪{g}\nend if\nend for\nend for\nCreate a new group g ={d}\nnew\nSort related_groups by their size", "metadata": {"id": "fa2e3cfc8d41a3e87ee048e303ed5182b08ddef2", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 18, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "end if\nend for\nend for\nCreate a new group g ={d}\nnew\nSort related_groups by their size\nfor each group g in related_groups do\nif |g |+|g|≤S then\nnew\ng ←g ∪g\nnew new\nRemove g from G\nend if\nend for\nAdd g to G\nnew\nend for\nreturn G\n18", "metadata": {"id": "7a5129e8dbd8406c765fe764c4d5f8c11855fe79", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 18, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "A.4 Dataset Examples\nHere, we present a few examples from the four datasets we experiment with.\nMethod Prompt\nNQ Question: how many episodes are in series 7 game of thrones\nAnswer: seven\nHotpotQA Question: What government position was held by the woman who portrayed\nCorliss Archer in the film Kiss and Tell?\nAnswer: Chief of Protocol\nQasper Question: Inthepaper’End-to-EndTrainableNon-CollaborativeDialogSys-\ntem’, How is intent annotated?\nAnswer: using a role-playing task on the Amazon Mechanical Turk platform\nand collecting typed conversations", "metadata": {"id": "530d866e9376984eaf21a45cf49a7c8cb41ada12", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 19, "created_at": "2025-09-12T03:11:38Z"}}
{"content": "and collecting typed conversations\nMultifieldQA- Question: What is the name of the most active fan club?\nen Answer: South West Ultras fan club.\nTable 9: Here are some examples from the four datasets used in our experiments.\n19", "metadata": {"id": "5304955c0b40c611fa1592a95097763cb9dba927", "source": "LongRAG- Enhancing Retrieval-Augmented Generation.pdf", "page": 19, "created_at": "2025-09-12T03:11:38Z"}}
