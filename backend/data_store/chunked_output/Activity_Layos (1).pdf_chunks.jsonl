{"content": "1. The trial with the highest last average 50 reward is the number B test using an eps\nstart of 1 and eps end of 0.05 with alpha 0.15, wherein I got 4.64\n2. When training finishes, the Q-values at the start state tell the agent how good each\nmove (U, D, L, R) looks in terms of expected future reward. Among these, the\nQ-value for moving right (R) becomes the largest, so it is the first greedy move in all\ntest cases A, B, and C because going right is the shortest path toward the nearby\ndirt. The agent received an early +5 reward after only two steps. Since greedy action", "metadata": {"id": "1a1fe9bdfdb2bd072c19a0cacbc22775db198063", "source": "Activity_Layos (1).pdf", "page": 1, "created_at": "2025-09-08T09:04:26Z"}}
{"content": "dirt. The agent received an early +5 reward after only two steps. Since greedy action\nselection always picks the move with the largest Q-value, the agent’s first greedy\nmove is R.\n3. It is much lower because it is a farther starting point than others; therefore, the tiles\nneed much more learning to improve them. I can add a slight alpha increase.\n4. Bumping the learning rate (α) from 0.1 -> 0.2 makes each Q-update twice as\naggressive, so the curve typically climbs faster and plateaus sooner, but it also gets a", "metadata": {"id": "1997c6c6194e38c0563bea4b74cbb43fcd33a99f", "source": "Activity_Layos (1).pdf", "page": 1, "created_at": "2025-09-08T09:04:26Z"}}
{"content": "aggressive, so the curve typically climbs faster and plateaus sooner, but it also gets a\nbit noisier/jittery; here it improves the last-50 average sooner (faster convergence).\n5. In practice, I would prefer the slightly higher alpha because it offers speeds, though it\ncould make the steps jittery; it still accomplishes its job.", "metadata": {"id": "baac977749caa1a3776f6ac3ab2e43ec648b260e", "source": "Activity_Layos (1).pdf", "page": 1, "created_at": "2025-09-08T09:04:26Z"}}
