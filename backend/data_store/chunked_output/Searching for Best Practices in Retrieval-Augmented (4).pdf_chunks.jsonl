{"content": "Searching for Best Practices in Retrieval-Augmented\nGeneration\nXiaohuaWang, ZhenghuaWang, XuanGao, FeiranZhang,\nYixinWu, ZhiboXu, TianyuanShi, ZhengyuanWang, ShizhengLi,\nQiQian, RuichengYin, ChangzeLv, XiaoqingZheng∗, XuanjingHuang\nSchoolofComputerScience,FudanUniversity,Shanghai,China\nShanghaiKeyLaboratoryofIntelligentInformationProcessing\n{xiaohuawang22,zhenghuawang23}@m.fudan.edu.cn\n{zhengxq,xjhuang}@fudan.edu.cn\nAbstract\nRetrieval-augmented generation (RAG) techniques have proven to be effective\nin integrating up-to-date information, mitigating hallucinations, and enhancing", "metadata": {"id": "c34c3223223c82819ad159d5000c75120277921f", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 1, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "in integrating up-to-date information, mitigating hallucinations, and enhancing\nresponsequality,particularlyinspecializeddomains.WhilemanyRAGapproaches\nhavebeenproposedtoenhancelargelanguagemodelsthroughquery-dependent\nretrievals, these approaches still suffer from their complex implementation and\nprolongedresponsetimes.Typically,aRAGworkflowinvolvesmultipleprocessing\nsteps, each of which can be executed in various ways. Here, we investigate\nexisting RAG approaches and their potential combinations to identify optimal\nRAG practices. Through extensive experiments, we suggest several strategies", "metadata": {"id": "444df15bfb902f706baedaa59631d6eb3f14616e", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 1, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "RAG practices. Through extensive experiments, we suggest several strategies\nfor deploying RAG that balance both performance and efficiency. Moreover,\nwe demonstrate that multimodal retrieval techniques can significantly enhance\nquestion-answeringcapabilitiesaboutvisualinputsandacceleratethegeneration\nofmultimodalcontentusinga“retrievalasgeneration”strategy. Resourcesare\navailableathttps://github.com/FudanDNN-NLP/RAG.\n1 Introduction\nGenerativelargelanguagemodelsarepronetoproducingoutdatedinformationorfabricatingfacts,", "metadata": {"id": "bccf64ec9000551f76ff99aae1d3ade44770d6ca", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 1, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "Generativelargelanguagemodelsarepronetoproducingoutdatedinformationorfabricatingfacts,\nalthoughtheywerealignedwithhumanpreferencesbyreinforcementlearning[1]orlightweight\nalternatives[2–5]. Retrieval-augmentedgeneration(RAG)techniquesaddresstheseissuesbycom-\nbiningthestrengthsofpretrainingandretrieval-basedmodels,therebyprovidingarobustframework\nforenhancingmodelperformance[6]. Furthermore,RAGenablesrapiddeploymentofapplications\nforspecificorganizationsanddomainswithoutnecessitatingupdatestothemodelparameters,as\nlongasquery-relateddocumentsareprovided.", "metadata": {"id": "5299c301095c5e43ac02de95c181626a5fb8c428", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 1, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "longasquery-relateddocumentsareprovided.\nMany RAG approaches have been proposed to enhance large language models (LLMs) through\nquery-dependentretrievals[6–8]. AtypicalRAGworkflowusuallycontainsmultipleintervening\nprocessingsteps: queryclassification(determiningwhetherretrievalisnecessaryforagiveninput\nquery), retrieval (efficiently obtaining relevant documents for the query), reranking (refining the\norder of retrieved documents based on their relevance to the query), repacking (organizing the\nretrieved documents into a structured one for better generation), summarization (extracting key", "metadata": {"id": "0af35eb093accefc7a8b16ee5ce936bf8f4d5408", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 1, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "retrieved documents into a structured one for better generation), summarization (extracting key\ninformation for response generation from the repacked document and eliminating redundancies)\nmodules. ImplementingRAGalsorequiresdecisionsonthewaystoproperlysplitdocumentsinto\nchunks,thetypesofembeddingstouseforsemanticallyrepresentingthesechunks,thechoiceof\n∗CorrespondingAuthor.\nPreprint.Underreview.\n4202\nluJ\n1\n]LC.sc[\n1v91210.7042:viXra", "metadata": {"id": "0e32e5959f5a60d2cce072584d94c5339fb1dee6", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 1, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "Evaluation\nLarge Language Model\n• G eneral Performance\n• Sp ecific Domains\n• R e trieval Capability Retrieval Source\nQuery Classification Chunking\nFine-tune\n• C hunking Size\n• D isturb • Sm all2big\n• R andom • Sl iding Windows\n• N o r mal Retrieval • C hunk Metadata\n• O riginal Query\nSummarization • BM 25 Embedding\n• C ontriever\n• Ex tractive • LL M-Embedder • LL M-Embedder\n• R ecomp • Q uery Rewriting • in tfloat/e5\n• BM 25 • Q uery Decomposition • BA AI/bge\n• Co ntriever • H yDE • Jin a-embeddings-v2\n• A bstractive • H ybrid Search • G te", "metadata": {"id": "45ae9e45c6f9f4ea59cac50989a3b57713984a78", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 2, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "• Co ntriever • H yDE • Jin a-embeddings-v2\n• A bstractive • H ybrid Search • G te\n• Lo ngLLMlingua • H y DE+Hybrid Search • al l - m pnet-base-v2\n• Se lectiveContext\n• Re comp\nReranking Vector Database\nRepacking • D LM-based • M ilvus\n• m onoT5 • Fa iss\n• Si des • m onoBERT • W eaviate\n• Fo rward • R ankLLaMA • Q drant\n• R e verse • TI LDE • C hroma\nFigure1: Retrieval-augmentedgenerationworkflow. Thisstudyinvestigatesthecontributionof\neachcomponentandprovidesinsightsintooptimalRAGpracticesthroughextensiveexperimentation.", "metadata": {"id": "6d0a7da643fd80397fe7e838f36c6b5113e56213", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 2, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "eachcomponentandprovidesinsightsintooptimalRAGpracticesthroughextensiveexperimentation.\nTheoptionalmethodsconsideredforeachcomponentareindicatedinboldfonts,whilethemethods\nunderlinedindicatethedefaultchoiceforindividualmodules. Themethodsindicatedinbluefont\ndenotethebest-performingselectionsidentifiedempirically.\nvectordatabasestoefficientlystorefeaturerepresentations,andthemethodsforeffectivelyfine-tuning\nLLMs(seeFigure1).\nWhataddscomplexityandchallengeisthevariabilityinimplementingeachprocessingstep. For", "metadata": {"id": "a678e01ffe980500e26d032a046878e5f678f38a", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 2, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "Whataddscomplexityandchallengeisthevariabilityinimplementingeachprocessingstep. For\nexample, in retrieving relevant documents for an input query, various methods can be employed.\nOne approach involves rewriting the query first and using the rewritten queries for retrieval [9].\nAlternatively, pseudo-responses to the query can be generated first, and the similarity between\nthesepseudo-responsesandthebackenddocumentscanbecomparedforretrieval[10]. Another\noption is to directly employ embedding models, typically trained in a contrastive manner using", "metadata": {"id": "531cfd858c43c639e571523093707440ce3f50a3", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 2, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "option is to directly employ embedding models, typically trained in a contrastive manner using\npositiveandnegativequery-responsepairs[11,12]. Thetechniqueschosenforeachstepandtheir\ncombinationssignificantlyimpactboththeeffectivenessandefficiencyofRAGsystems. Tothebest\nofourknowledge,therehasbeennosystematicefforttopursuetheoptimalimplementationofRAG,\nparticularlyfortheentireRAGworkflow.\nInthisstudy,weaimtoidentifythebestpracticesforRAGthroughextensiveexperimentation. Given\ntheinfeasibilityoftestingallpossiblecombinationsofthesemethods,weadoptathree-stepapproach", "metadata": {"id": "bd30a4c720094d5a666453dfb84ebec807d7bd51", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 2, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "theinfeasibilityoftestingallpossiblecombinationsofthesemethods,weadoptathree-stepapproach\ntoidentifyoptimalRAGpractices. First,wecomparerepresentativemethodsforeachRAGstep(or\nmodule)andselectuptothreeofthebest-performingmethods. Next,weevaluatetheimpactofeach\nmethodontheoverallRAGperformancebytestingonemethodatatimeforanindividualstep,while\nkeepingtheotherRAGmodulesunchanged. Thisallowsustodeterminethemosteffectivemethod\nforeachstepbasedonitscontributionandinteractionwithothermodulesduringresponsegeneration.", "metadata": {"id": "8b92a4e92b71253852f3ca755e48b163a4b08cac", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 2, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "foreachstepbasedonitscontributionandinteractionwithothermodulesduringresponsegeneration.\nOnce the best method is chosen for a module, it is used in subsequent experiments. Finally, we\nempiricallyexploreafewpromisingcombinationssuitablefordifferentapplicationscenarioswhere\nefficiencymightbeprioritizedoverperformance,orviceversa. Basedonthesefindings,wesuggest\nseveralstrategiesfordeployingRAGthatbalancebothperformanceandefficiency.\nThecontributionsofthisstudyarethree-fold:\n• Throughextensiveexperimentation,wethoroughlyinvestigatedexistingRAGapproachesandtheir", "metadata": {"id": "6e3e87405f9cfdb552849f86e8227da95f705b2d", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 2, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "• Throughextensiveexperimentation,wethoroughlyinvestigatedexistingRAGapproachesandtheir\ncombinationstoidentifyandrecommendoptimalRAGpractices.\n2", "metadata": {"id": "5e9edda5473db583df9d675924f3d9bf12ded80a", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 2, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "• We introduce a comprehensive framework of evaluation metrics and corresponding datasets to\ncomprehensively assess the performance of retrieval-augmented generation models, covering\ngeneral,specialized(ordomain-specific),andRAG-relatedcapabilities.\n• Wedemonstratethattheintegrationofmultimodalretrievaltechniquescansubstantiallyimprove\nquestion-answeringcapabilitiesonvisualinputsandspeedupthegenerationofmultimodalcontent\nthroughastrategyof“retrievalasgeneration”.\n2 RelatedWork\nEnsuringthe accuracy ofresponses generatedby Large LanguageModels (LLMs) suchas Chat-", "metadata": {"id": "b499eb8f811c9b9a7889285295a06a5621abc4d5", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 3, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "2 RelatedWork\nEnsuringthe accuracy ofresponses generatedby Large LanguageModels (LLMs) suchas Chat-\nGPT[13]andLLaMA[14]isessential.However,simplyenlargingmodelsizedoesnotfundamentally\naddresstheissueofhallucinations[15,16],especiallyinknowledge-intensivetasksandspecialized\ndomains. Retrieval-augmentedgeneration(RAG)addressesthesechallengesbyretrievingrelevant\ndocumentsfromexternalknowledgebases,providingaccurate,real-time,domain-specificcontextto\nLLMs[6]. PreviousworkshaveoptimizedtheRAGpipelinethroughqueryandretrievaltransfor-", "metadata": {"id": "3dd1f29011258d0bfc51cc2f8e899e934a3a4727", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 3, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "LLMs[6]. PreviousworkshaveoptimizedtheRAGpipelinethroughqueryandretrievaltransfor-\nmations,enhancingretrieverperformance,andfine-tuningboththeretrieverandgenerator. These\noptimizationsimprovetheinteractionbetweeninputqueries,retrievalmechanisms,andgeneration\nprocesses,ensuringtheaccuracyandrelevanceofresponses.\n2.1 QueryandRetrievalTransformation\nEffectiveretrievalrequiresqueriesaccurate,clear,anddetailed. Evenwhenconvertedintoembed-\ndings,semanticdifferencesbetweenqueriesandrelevantdocumentscanpersist. Previousworkshave", "metadata": {"id": "9d35dde0bdcb2e53b474209154902a1597a179d2", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 3, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "dings,semanticdifferencesbetweenqueriesandrelevantdocumentscanpersist. Previousworkshave\nexploredmethodstoenhancequeryinformationthroughquerytransformation,therebyimproving\nretrievalperformance. Forinstance,Query2Doc[17]andHyDE[10]generatepseudo-documents\nfrom original queries to enhance retrieval, while TOC [18] decomposes queries into subqueries,\naggregatingtheretrievedcontentforfinalresults.\nOtherstudieshavefocusedontransformingretrievalsourcedocuments. LlamaIndex[19]providesan\ninterfacetogeneratepseudo-queriesforretrievaldocuments,improvingmatchingwithrealqueries.", "metadata": {"id": "aeec58b8a9c701c1a8e04a9cfff3499aeb4b7f20", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 3, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "interfacetogeneratepseudo-queriesforretrievaldocuments,improvingmatchingwithrealqueries.\nSomeworksemploycontrastivelearningtobringqueryanddocumentembeddingscloserinsemantic\nspace [12, 20, 21]. Post-processing retrieved documents is another method to enhance generator\noutput, with techniques like hierarchical prompt summarization [22] and using abstractive and\nextractivecompressors[23]toreducecontextlengthandremoveredundancy[24].\n2.2 RetrieverEnhancementStrategy\nDocumentchunkingandembeddingmethodssignificantlyimpactretrievalperformance. Common", "metadata": {"id": "b569266f58ab4443fb16a13940655c85cd7520d8", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 3, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "Documentchunkingandembeddingmethodssignificantlyimpactretrievalperformance. Common\nchunking strategies divide documents into chunks, but determining optimal chunk length can be\nchallenging. Smallchunksmayfragmentsentences, whilelargechunksmightincludeirrelevant\ncontext. LlamaIndex [19] optimizes the chunking method like Small2Big and sliding window.\nRetrieved chunks can be irrelevant and numbers can be large, so reranking is necessary to filter\nirrelevant documents. A common reranking approach employs deep language models such as", "metadata": {"id": "75454e151b04d2f59e371bb75b5a78caa5d0ab4c", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 3, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "irrelevant documents. A common reranking approach employs deep language models such as\nBERT[25],T5[26],orLLaMA[27],whichrequiresslowinferencestepsduringrerankingbutgrants\nbetterperformance. TILDE[28,29]achievesefficiencybyprecomputingandstoringthelikelihood\nofqueryterms,rankingdocumentsbasedontheirsum.\n2.3 RetrieverandGeneratorFine-tuning\nFine-tuningwithintheRAGframeworkiscrucialforoptimizingbothretrieversandgenerators. Some\nresearch focuses on fine-tuning the generator to better utilize retriever context [30–32], ensuring", "metadata": {"id": "cdf3b2f7b19bdf8e5954ea48eacdefedb02b9b98", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 3, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "research focuses on fine-tuning the generator to better utilize retriever context [30–32], ensuring\nfaithfulandrobustgeneratedcontent. Othersfine-tunetheretrievertolearntoretrievebeneficial\npassagesforthegenerator[33–35].HolisticapproachestreatRAGasanintegratedsystem,fine-tuning\nboth retriever and generator together to enhance overall performance [36–38], despite increased\ncomplexityandintegrationchallenges.\nSeveralsurveyshaveextensivelydiscussedcurrentRAGsystems,coveringaspectsliketextgenera-\ntion[7,8],integrationwithLLMs[6,39],multimodal[40],andAI-generatedcontent[41]. While", "metadata": {"id": "6ce3c38469f143378bbc1b31d26b77dcafda3f78", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 3, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "tion[7,8],integrationwithLLMs[6,39],multimodal[40],andAI-generatedcontent[41]. While\nthesesurveysprovidecomprehensiveoverviewsofexistingRAGmethodologies,selectingtheappro-\n3", "metadata": {"id": "f690b772e3de18873228ebd61178d860d369a712", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 3, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "Sufficient information\nNo Retrieval Needed Need to Retrieval\n\"To be, or not to be, that is the \"The Renaissance was a\nquestion.\" cultural transformation in\nPlease translate this sentence into European history, marking the\nFrench. < Translation > revival of arts, sciences, and Please give me a plan for holding a graduation party.\nhumanistic thought. The < Planning >\n\"Dave is attending his aunt's fervor of artists and scholars\nbrother funeral today.\" propelled prosperity and", "metadata": {"id": "1ae6db282d8ef26b5b305db22b81137f9cef8b00", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 4, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "brother funeral today.\" propelled prosperity and\nP ef a f r e a c p ti h v r e a ly s . e th e g i v e n < in R fo e r w m r a it t i io n n g > i a s n u n n m d o m s v c a a i t e r i n o y . c n e < i . n \" S G a u r i m v ts e m , l m a it r e e i r z a a a t t u i r o e n , > I w s f h a I o n w u t l a d to n I t c d t h o r o i t o v r s e a e v o t e r h l t e f a r c k o h e m e a a L p p o e la s s n t A e m n ? g o e d l e e s o f t < o t r D N an e e c s w i p s o i Y o r o t n r a k t m i o a a n n k , d i n I g >\nTom has three sisters, and each\nsister has a brother. How many", "metadata": {"id": "941395b51ca3152d0208b59f12fcbd54f2506ddd", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 4, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "Tom has three sisters, and each\nsister has a brother. How many\ns < i b R l e in a g so s n a n re in t g h e > re in total? \" O C p h e a n t A G I P .\" T is a product of I r e e a h l c a a h d ti o o a t n h q s e h u r i a . p r H r w e o l i w t w h s i m t h h o y u m b l y d o y p I f a p r r i e e e r n n s d u ts , a b b d e u e c t m a w u y s e e p g a t e h r n e e u n y i t n o s e p t l o p y o a l s c o e c v e m e p y t\nPlease provide the ownership our relationship? < Suggestion >\nIdentify who is football players: relationship.\nMessi, Jordan, Kobe. < Information extraction >", "metadata": {"id": "0b6360dad3d7b6125a02943cf694e041e15c9546", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 4, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "Identify who is football players: relationship.\nMessi, Jordan, Kobe. < Information extraction >\n< Closed QA > Which city will the next World Cup be held?\n< Search >\nInsufficient information\n\"French.Washington played a Please find a novel that is as If you're currently a computer science student and your\ncrucial role in the American famous as \"One Hundred Years computer system encounters a malfunction, what should\nRevolutionary War, leading the of Solitude\". < Search > you do? < Role-play >\nContinental Army against the\nBritish. \"", "metadata": {"id": "328717356b9a5d68124067a866ee00052c2885c1", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 4, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "Continental Army against the\nBritish. \"\nPlease continue writing the Q: 3,1 A: 3 Q: 2,5 A: 5 Write an article about the geography of Europe, focusing\nabove paragraph. Q: 5,7 A: ? on the changes in rainfall in the western part of the\n< Continuation writing > < In-context learning > country. < Writing >\nBackground Knowledge No Background Knowledge\nFigure2: Classificationofretrievalrequirementsfordifferenttasks. Incaseswhereinformationis\nnotprovided,wedifferentiatetasksbasedonthefunctionsofthemodel.\npriatealgorithmforpracticalimplementationremainschallenging. Inthispaper,wefocusonbest", "metadata": {"id": "c3ad2a2e41250c7751114ce70ed6720b5338c5f4", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 4, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "priatealgorithmforpracticalimplementationremainschallenging. Inthispaper,wefocusonbest\npracticesforapplyingRAGmethods,advancingtheunderstandingandapplicationofRAGinLLMs.\n3 RAGWorkflow\nIn this section, we detail the components of the RAG workflow. For each module, we review\ncommonly used approaches and select the default and alternative methods for our final pipeline.\nSection4willdiscussbestpractices. Figure1presentstheworkflowandmethodsforeachmodule.\nDetailed experimental setups, including datasets, hyperparameters, and results are provided in\nAppendixA.\n3.1 QueryClassification", "metadata": {"id": "6abcf0498e7e5135243c577e932fb7598e8c108b", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 4, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "AppendixA.\n3.1 QueryClassification\nNotallqueriesrequireretrieval-augmentedduetotheinherentcapabilitiesofLLMs. WhileRAGcan\nenhanceinformationaccuracyandreducehallucinations,frequentretrievalcanincreaseresponse\ntime. Therefore, webeginbyclassifyingqueriestodeterminethenecessityofretrieval. Queries\nrequiringretrievalproceedthroughtheRAGmodules;othersarehandleddirectlybyLLMs.\nRetrieval is generally recommended when knowledge beyond the model’s parameters is needed.\nHowever, the necessity of retrieval varies by task. For instance, an LLM trained up to 2023 can", "metadata": {"id": "9cf79358adc9a64dc7c06866900675ee4f167b73", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 4, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "However, the necessity of retrieval varies by task. For instance, an LLM trained up to 2023 can\nhandleatranslationrequestfor“SorawasdevelopedbyOpenAI”withoutretrieval. Conversely,an\nintroductionrequestforthesametopicwouldrequireretrievaltoproviderelevantinformation.\nTherefore,weproposeclassifyingtasksbytypetodetermineifaqueryneedsretrieval. Wecategorize\n15 tasks based on whether they provide suffi-\ncientinformation,withspecifictasksandexam-\nMetrics\nples illustrated in Figure 2. For tasks entirely Model\nbasedonuser-giveninformation,wedenoteas Acc Prec Rec F1", "metadata": {"id": "67a3aef8a11acd2b11f3669637669ec52c9312d8", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 4, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "basedonuser-giveninformation,wedenoteas Acc Prec Rec F1\n“sufficient”,whichneednotretrieval;otherwise,\nBERT-base-multilingual 0.95 0.96 0.94 0.95\nwedenoteas“insufficient”,andretrievalmay\nbenecessary. Wetrainaclassifiertoautomate Table1: ResultsoftheQueryClassifier.\nthisdecision-makingprocess. Experimentalde-\ntailsarepresentedinAppendixA.1. Section4\nexplorestheimpactofqueryclassificationontheworkflow,comparingscenarioswithandwithout\nclassification.\n4", "metadata": {"id": "4ce3676994d2dd7463926fc92101a059f52eabd6", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 4, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "namespace-Pt/msmarco\nEmbeddingModel\nMRR@1 MRR@10 MRR@100 R@1 R@10 R@100\nBAAI/LLM-Embedder[20] 24.79 37.58 38.62 24.07 66.45 90.75\nBAAI/bge-base-en-v1.5[12] 23.34 35.80 36.94 22.63 64.12 90.13\nBAAI/bge-small-en-v1.5[12] 23.27 35.78 36.89 22.65 63.92 89.80\nBAAI/bge-large-en-v1.5[12] 24.63 37.48 38.59 23.91 65.57 90.60\nBAAI/bge-large-en[12] 24.84 37.66 38.73 24.13 66.09 90.64\nBAAI/bge-small-en[12] 23.28 35.79 36.91 22.62 63.96 89.67\nBAAI/bge-base-en[12] 23.47 35.94 37.07 22.73 64.17 90.14\nAlibaba-NLP/gte-large-en-v1.5[21] 8.93 15.60 16.71 8.67 32.28 60.36", "metadata": {"id": "100e1f09365ee6f747f9f9bf5f1286aca3289ae7", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 5, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "Alibaba-NLP/gte-large-en-v1.5[21] 8.93 15.60 16.71 8.67 32.28 60.36\nthenlper/gte-base[21] 7.42 13.23 14.30 7.21 28.27 56.20\nthenlper/gte-small[21] 7.97 14.81 15.95 7.71 32.07 61.08\njinaai/jina-embeddings-v2-small-en[42] 8.07 15.02 16.12 7.87 32.55 60.36\nintfloat/e5-small-v2[11] 10.04 18.23 19.41 9.74 38.92 68.42\nintfloat/e5-large-v2[11] 9.58 17.94 19.03 9.35 39.00 66.11\nsentence-transformers/all-mpnet-base-v2 5.80 11.26 12.26 5.66 25.57 50.94\nTable2: Resultsfordifferentembeddingmodelsonnamespace-Pt/msmarco.\n3.2 Chunking", "metadata": {"id": "b8ac483ca692227806f4eeb45b11ac41f8ee5556", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 5, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "Table2: Resultsfordifferentembeddingmodelsonnamespace-Pt/msmarco.\n3.2 Chunking\nChunkingdocumentsintosmallersegmentsiscrucialforenhancingretrievalprecisionandavoiding\nlengthissuesinLLMs. Thisprocesscanbeappliedatvariouslevelsofgranularity,suchastoken,\nsentence,andsemanticlevels.\n• Token-levelChunkingisstraightforwardbutmaysplitsentences,affectingretrievalquality.\n• Semantic-level Chunking uses LLMs to determine breakpoints, context-preserving but time-\nconsuming.\n• Sentence-levelChunkingbalancespreservingtextsemanticswithsimplicityandefficiency.", "metadata": {"id": "1e41665d34e848932d40c8c41cd61a17da345c2f", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 5, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "consuming.\n• Sentence-levelChunkingbalancespreservingtextsemanticswithsimplicityandefficiency.\nInthisstudy,weusesentence-levelchunking,balancingsimplicityandsemanticpreservation. We\nexaminechunkingfromfourdimensions.\n3.2.1 ChunkSize\nChunk size significantly impacts performance. Larger chunks provide more context, enhancing\ncomprehensionbutincreasingprocesstime. Smallerchunksimproveretrievalrecallandreducetime\nbutmaylacksufficientcontext.\nFindingtheoptimalchunksizeinvolvesabalancebetweensomemetricssuchasfaithfulness,and", "metadata": {"id": "1c7bff0b932a6acbe429bfef4bcbb5d245b068d3", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 5, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "Findingtheoptimalchunksizeinvolvesabalancebetweensomemetricssuchasfaithfulness,and\nrelevancy. Faithfulnessmeasureswhethertheresponseishallucinatedormatchestheretrievedtexts.\nRelevancymeasureswhethertheretrievedtexts\nand responses match queries. We use the\nlyft_2021\nevaluation module of LlamaIndex [43] to cal-\nculate the metrics above. For embedding, ChunkSize Average Average\nweusethetext-embedding-ada-0022 model, Faithfulness Relevancy\n2048 80.37 91.11\nwhich supports long input length. We choose\n1024 94.26 95.56\nzephyr-7b-alpha3 and gpt-3.5-turbo4 as\n512 97.59 97.41", "metadata": {"id": "59fa8d9953a6c11c77095a495386438b0c67d243", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 5, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "1024 94.26 95.56\nzephyr-7b-alpha3 and gpt-3.5-turbo4 as\n512 97.59 97.41\ngenerationmodelandevaluationmodelrespec-\n256 97.22 97.78\ntively.Thesizeofthechunkoverlapis20tokens.\n128 95.74 97.22\nFirstsixtypagesofthedocumentlyft_20215\nare used as corpus, then prompting LLMs to Table3: Comparisonofdifferentchunksizes.\ngenerateaboutonehundredandseventyqueries\naccordingtochosencorpus. TheimpactofdifferentchunksizesisshowninTable3.\n2https://platform.openai.com/docs/guides/embeddings/embedding-models\n3https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha\n4https://www.openai.com/", "metadata": {"id": "0b8df236235962ac3cb9ad351adbbd27d28df6fc", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 5, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "3https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha\n4https://www.openai.com/\n5https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/\ndata/10k/lyft_2021.pdf\n5", "metadata": {"id": "d039febf62b41b0e90e8ddb6cfd9c0a3b58113b5", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 5, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "3.2.2 ChunkingTechniques\nAdvancedtechniquessuchassmall-to-bigandslidingwindowimproveretrievalqualitybyorganizing\nchunk block relationships. Small-sized blocks are used to match queries, and larger blocks that\nincludethesmallonesalongwithcontextualinformationarereturned.\nTodemonstratetheeffectivenessofadvancedchunkingtechniques,weusetheLLM-Embedder[20]\nmodelasanembeddingmodel. Thesmallerchunksizeis175tokens,thelargerchunksizeis512\ntokensandthechunkoverlapis20tokens. Techniqueslikesmall-to-bigandslidingwindowimprove", "metadata": {"id": "6e2bfce94bea8a263ce974355218921d5f2dc513", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 6, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "tokensandthechunkoverlapis20tokens. Techniqueslikesmall-to-bigandslidingwindowimprove\nretrieval quality by maintaining context and ensuring relevant information is retrieved. Detailed\nresultsareshowninTable4.\n3.2.3 EmbeddingModelSelection\nChoosing the right embedding model is crucial for effective semantic matching of queries\nand chunk blocks. We use the evaluation module of FlagEmbedding6 which uses the dataset\nnamespace-Pt/msmarco7 as queries and\ndataset namespace-Pt/msmarco-corpus8 as\nlyft_2021\ncorpus to choose the appropriate open source", "metadata": {"id": "4e86c11d6b9c7f0a8de11e1390b8e1758c23124f", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 6, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "dataset namespace-Pt/msmarco-corpus8 as\nlyft_2021\ncorpus to choose the appropriate open source\nembedding model. As shown in Table 2, ChunkSkill Average Average\nFaithfulness Relevancy\nLLM-Embedder [20] achieves comparable\nOriginal 95.74 95.37\nresultswithBAAI/bge-large-en[12],however,\nsmall2big 96.67 95.37\nthe size of the former is three times smaller\nslidingwindow 97.41 96.85\nthan that of the latter. Thus, we select the\nLLM-Embedder [20] for its balance of Table4: Comparisonofdifferentchunkskills.\nperformanceandsize.\n3.2.4 MetadataAddition", "metadata": {"id": "a96bd424199471339c53c1ddf4d18784b89d4eb2", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 6, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "performanceandsize.\n3.2.4 MetadataAddition\nEnhancingchunkblockswithmetadataliketitles,keywords,andhypotheticalquestionscanimprove\nretrieval, provide more ways to post-process retrieved texts, and help LLMs better understand\nretrievedinformation. Adetailedstudyonmetadatainclusionwillbeaddressedinfuturework.\n3.3 VectorDatabases\nVectordatabasesstoreembeddingvectorswiththeirmetadata,enablingefficientretrievalofdoc-\numents relevant to queries through various indexing and approximate nearest neighbor (ANN)\nmethods.", "metadata": {"id": "0df9dbdbaf3036818a4ff4df83b045015748b4a0", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 6, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "uments relevant to queries through various indexing and approximate nearest neighbor (ANN)\nmethods.\nTo select an appropriate vector database for our research, we evaluated several options based on\nfourkeycriteria: multipleindextypes,billion-scalevectorsupport,hybridsearch,andcloud-native\ncapabilities. Thesecriteriawerechosenfortheir\nimpact on flexibility, scalability, and ease of Multiple Billion- Hybrid Cloud-\ndeploymentinmodern,cloud-basedinfrastruc- Database IndexType Scale Search Native\ntures. Multipleindextypesprovidetheflexibil- Weaviate ✗ ✗ ✓ ✓", "metadata": {"id": "69f7784934867686102902b9374dc2fb30f799dd", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 6, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "tures. Multipleindextypesprovidetheflexibil- Weaviate ✗ ✗ ✓ ✓\nitytooptimizesearchesbasedondifferentdata Faiss ✓ ✗ ✗ ✗\ncharacteristicsandusecases. Billion-scalevec- Chroma ✗ ✗ ✓ ✓\ntorsupportiscrucialforhandlinglargedatasets Qdrant ✗ ✓ ✓ ✓\ninLLMapplications. Hybridsearchcombines Milvus ✓ ✓ ✓ ✓\nvectorsearchwithtraditionalkeywordsearch,\nTable5: ComparisonofVariousVectorDatabases\nenhancing retrieval accuracy. Finally, cloud-\nnativecapabilitiesensureseamlessintegration,scalability,andmanagementincloudenvironments.", "metadata": {"id": "8912fb3e711bb647fad92105c1c1c71506870b2e", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 6, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "nativecapabilitiesensureseamlessintegration,scalability,andmanagementincloudenvironments.\nTable 5 presents a detailed comparison of five open-source vector databases: Weaviate, Faiss,\nChroma,Qdrant,andMilvus.\nOur evaluation indicates that Milvus stands out as the most comprehensive solution among the\ndatabasesevaluated,meetingalltheessentialcriteriaandoutperformingotheropen-sourceoptions.\n6https://github.com/FlagOpen/FlagEmbedding\n7https://huggingface.co/datasets/namespace-Pt/msmarco\n8https://huggingface.co/datasets/namespace-Pt/msmarco-corpus\n6", "metadata": {"id": "9f0f0562cad07af9e87f10db5af771f232979b43", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 6, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "TRECDL19 TRECDL20\nMethod\nmAP nDCG@10 R@50 R@1k Latency mAP nDCG@10 R@50 R@1k Latency\nunsupervised\nBM25 30.13 50.58 38.32 75.01 0.07 28.56 47.96 46.18 78.63 0.29\nContriever 23.99 44.54 37.54 74.59 3.06 23.98 42.13 43.81 75.39 0.98\nsupervised\nLLM-Embedder 44.66 70.20 49.06 84.48 2.61 45.60 68.76 61.36 84.41 0.71\n+QueryRewriting 44.56 67.89 51.45 85.35 7.80 45.16 65.62 59.63 83.45 2.06\n+QueryDecomposition 41.93 66.10 48.66 82.62 14.98 43.30 64.95 57.74 84.18 2.01\n+HyDE 50.87 75.44 54.93 88.76 7.21 50.94 73.94 63.80 88.03 2.14\n+HybridSearch 47.14 72.50 51.13 89.08 3.20 47.72 69.80 64.32 88.04 0.77", "metadata": {"id": "b93f7162281c60fba551e77a7b60fc0e064cc568", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 7, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "+HybridSearch 47.14 72.50 51.13 89.08 3.20 47.72 69.80 64.32 88.04 0.77\n+HyDE+HybridSearch 52.13 73.34 55.38 90.42 11.16 53.13 72.72 66.14 90.67 2.95\nTable6: ResultsfordifferentretrievalmethodsonTRECDL19/20. Thebestresultforeachmethod\nismadeboldandthesecondisunderlined.\nTRECDL19 TRECDL20\nConfiguration\nmAP nDCG@10 R@50 R@1k latency mAP nDCG@10 R@50 R@1k Latency\nHyDE\nw/1pseudo-doc 48.77 72.49 53.20 87.73 8.08 51.31 70.37 63.28 87.81 2.09\nw/1pseudo-doc+query 50.87 75.44 54.93 88.76 7.21 50.94 73.94 63.80 88.03 2.14\nw/8pseudo-doc+query 51.64 75.12 54.51 89.17 14.15 53.14 73.65 65.79 88.67 3.44", "metadata": {"id": "c89917148997cc526cbd69551ca2ea1bec11c5d3", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 7, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "w/8pseudo-doc+query 51.64 75.12 54.51 89.17 14.15 53.14 73.65 65.79 88.67 3.44\nTable7: HyDEwithdifferentconcatenationofhypotheticaldocumentsandqueries.\n3.4 RetrievalMethods\nGivenauserquery,theretrievalmoduleselectsthetop-krelevantdocumentsfromapre-builtcorpus\nbased on the similarity between the query and the documents. The generation model then uses\nthesedocumentstoformulateanappropriateresponsetothequery. However,originalqueriesoften\nunderperformduetopoorexpressionandlackofsemanticinformation[6],negativelyimpactingthe", "metadata": {"id": "d2fa65dcdd6efa7ef2f313295ac5b06a52cd7721", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 7, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "underperformduetopoorexpressionandlackofsemanticinformation[6],negativelyimpactingthe\nretrievalprocess. Toaddresstheseissues,weevaluatedthreequerytransformationmethodsusingthe\nLLM-EmbedderrecommendedinSection3.2asthequeryanddocumentencoder:\n• QueryRewriting: Queryrewritingrefinesqueriestobettermatchrelevantdocuments. Inspired\nbytheRewrite-Retrieve-Readframework[9],wepromptanLLMtorewritequeriestoenhance\nperformance.\n• Query Decomposition: This approach involves retrieving documents based on sub-questions\nderivedfromtheoriginalquery,whichismorecomplextocomprehendandhandle.", "metadata": {"id": "dbdf8ecef60e71ee64b580eed9592cc4aa339930", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 7, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "derivedfromtheoriginalquery,whichismorecomplextocomprehendandhandle.\n• Pseudo-documentsGeneration: Thisapproachgeneratesahypotheticaldocumentbasedonthe\nuserqueryandusestheembeddingofhypotheticalanswerstoretrievesimilardocuments. One\nnotableimplementisHyDE[10],\nRecentstudies,suchas[44],indicatethatcombininglexical-basedsearchwithvectorsearchsignifi-\ncantlyenhancesperformance. Inthisstudy,weuseBM25forsparseretrievalandContriever[45],an\nunsupervisedcontrastiveencoder,fordenseretrieval,servingastworobustbaselinesbasedonThakur\netal.[46].\n3.4.1 Resultsfordifferentretrievalmethods", "metadata": {"id": "eb75f99c211092c95498a792014ed0f5864a759d", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 7, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "etal.[46].\n3.4.1 Resultsfordifferentretrievalmethods\nWeevaluatedtheperformanceofdifferentsearchmethodsontheTRECDL2019and2020passage\nranking datasets. The results presented in Table 6 show that supervised methods significantly\noutperformedunsupervisedmethods. CombiningwithHyDEandhybridsearch,LLM-Embedder\nachievesthehighestscores. However,queryrewritingandquerydecompositiondidnotenhance\nretrieval performance as effectively. Considering the best performance and tolerated latency, we\nrecommend HybridSearchwithHyDE as the default retrieval method. Taking efficiency into", "metadata": {"id": "5ae8fc2b10fe14f7275926601e5bf427c3db7761", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 7, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "recommend HybridSearchwithHyDE as the default retrieval method. Taking efficiency into\nconsideration, Hybrid Search combines sparse retrieval (BM25) and dense retrieval (Original\nembedding)andachievesnotableperformancewithrelativelylowlatency.\n3.4.2 HyDEwithDifferentConcatenationofDocumentsandQuery\nTable7showstheimpactofdifferentconcatenationstrategiesforhypotheticaldocumentsandqueries\nusingHyDE.Concatenatingmultiplepseudo-documentswiththeoriginalquerycansignificantly\n7", "metadata": {"id": "89a7f18452a95f19ead0d17c815c007efa202331", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 7, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "TRECDL19 TRECDL20\nHyperparameter\nmAP nDCG@10 R@50 R@1k latency mAP nDCG@10 R@50 R@1k Latency\nHybridSearch\nα=0.1 46.00 70.87 49.24 88.89 2.98 46.54 69.05 63.36 87.32 0.90\nα=0.3 47.14 72.50 51.13 89.08 3.20 47.72 69.80 64.32 88.04 0.77\nα=0.5 47.36 72.24 52.71 88.09 3.02 47.19 68.12 64.90 87.86 0.87\nα=0.7 47.21 71.89 52.40 88.01 3.15 45.82 67.30 64.23 87.92 1.02\nα=0.9 46.35 70.67 52.64 88.22 2.74 44.02 65.55 63.22 87.76 1.20\nTable8: Resultsofhybridsearchwithdifferentalphavalues.\nMSMARCOPassageranking\nMethod\nBaseModel #Params MRR@1 MRR@10 MRR@1k HitRate@10 Latency\nw/oReranking", "metadata": {"id": "743dcd6589daa9e5d95515fa4ea7bf788ad95564", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 8, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "MSMARCOPassageranking\nMethod\nBaseModel #Params MRR@1 MRR@10 MRR@1k HitRate@10 Latency\nw/oReranking\nRandomOrdering - - 0.011 0.027 0.068 0.092 -\nBM25 - - 6.52 11.65 12.59 24.63 -\nDLMReranking\nmonoT5 T5-base 220M 21.62 31.78 32.40 54.07 4.5\nmonoBERT BERT-large 340M 21.65 31.69 32.35 53.38 15.8\nRankLLaMA Llama-2-7b 7B 22.08 32.35 32.97 54.53 82.4\nTILDEReranking\nTILDEv2 BERT-base 110M 18.57 27.83 28.60 49.07 0.02\nTable9: ResultsofdifferentrerankingmethodsonthedevsetoftheMSMARCOPassageranking\ndataset. Foreachquery,thetop-1000candidatepassagesretrievedbyBM25arereranked. Latencyis", "metadata": {"id": "00f58a5e8734bd1511b461d9a2f029e01541ece4", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 8, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "dataset. Foreachquery,thetop-1000candidatepassagesretrievedbyBM25arereranked. Latencyis\nmeasuredinsecondsperquery.\nenhanceretrievalperformance,thoughatthecostofincreasedlatency,suggestingatrade-offbetween\nretrievaleffectivenessandefficiency.However,indiscriminatelyincreasingthenumberofhypothetical\ndocumentsdoesnotyieldsignificantbenefitsandsubstantiallyraiseslatency,indicatingthatusinga\nsinglehypotheticaldocumentissufficient.\n3.4.3 HybridSearchwithDifferentWeightonSparseRetrieval\nTable8presentstheimpactofdifferentαvaluesinhybridsearch,whereαcontrolstheweighting", "metadata": {"id": "281e726897ee684cdff4d7f4ed9bd25f0e51d0a3", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 8, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "Table8presentstheimpactofdifferentαvaluesinhybridsearch,whereαcontrolstheweighting\nbetweensparseretrievalanddenseretrievalcomponents. Therelevancescoreiscalculatedasfollows:\nS =α·S +S (1)\nh s d\nwhereS ,S arethenormalizedrelevancescoresfromsparseretrievalanddenseretrievalrespectively,\ns d\nandS isthetotalretrievalscore.\nh\nWeevaluatedfivedifferentαvaluestodeterminetheirinfluenceonperformance. Theresultsindicate\nthatan α value of0.3yields thebestperformance, demonstrating that appropriateadjustmentof\nαcanenhanceretrievaleffectivenesstoacertainextent. Therefore, weselectedα = 0.3forour", "metadata": {"id": "651779824b5eaa98424b37e6f77528d22745f7a3", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 8, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "αcanenhanceretrievaleffectivenesstoacertainextent. Therefore, weselectedα = 0.3forour\nretrievalandmainexperiments. AdditionalimplementationdetailsarepresentedinAppendixA.2.\n3.5 RerankingMethods\nAftertheinitialretrieval,arerankingphaseisemployedtoenhancetherelevanceoftheretrieved\ndocuments,ensuringthatthemostpertinentinformationappearsatthetopofthelist. Thisphaseuses\nmorepreciseandtime-intensivemethodstoreorderdocumentseffectively,increasingthesimilarity\nbetweenthequeryandthetop-rankeddocuments.\nWe consider two approaches in our reranking module: DLM Reranking, which utilizes classifi-", "metadata": {"id": "0df63b94ad6a40b98cc17bbd624319fb167cc1b8", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 8, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "We consider two approaches in our reranking module: DLM Reranking, which utilizes classifi-\ncation,andTILDEReranking,whichfocusesonquerylikelihoods. Theseapproachesprioritize\nperformanceandefficiency,respectively.\n• DLMReranking: Thismethodleveragesdeeplanguagemodels(DLMs)[25–27]forreranking.\nThesemodelsarefine-tunedtoclassifydocumentrelevancytoaqueryas“true”or“false”. During\nfine-tuning,themodelistrainedwithconcatenatedqueryanddocumentinputs,labeledbyrelevancy.\nAtinference,documentsarerankedbasedontheprobabilityofthe“true”token.", "metadata": {"id": "d02e653e155086855f97012665abaa0a75806b99", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 8, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "Atinference,documentsarerankedbasedontheprobabilityofthe“true”token.\n• TILDEReranking: TILDE[28,29]calculatesthelikelihoodofeachquerytermindependently\nbypredictingtokenprobabilitiesacrossthemodel’svocabulary. Documentsarescoredbysumming\n8", "metadata": {"id": "524fb200d4c8eeff856adad014784583a91d82aa", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 8, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "NQ TQA HotPotQA\nMethod Avg. Avg.Token\nF1 #token F1 #token F1 #token\nw/oSummarization\nOriginPrompt 27.07 124 33.61 152 33.92 141 31.53 139\nExtractiveMethod\nBM25 27.97 40 32.44 59 28.00 63 29.47 54\nContriever 23.62 42 33.79 65 23.64 60 27.02 56\nRecomp(extractive) 27.84 34 35.32 60 29.46 58 30.87 51\nAbstractiveMethod\nSelectiveContext 25.05 65 34.25 70 34.43 66 31.24 67\nLongLLMlingua 21.32 51 32.81 56 30.79 57 28.29 55\nRecomp(abstractive) 33.68 59 35.87 61 29.01 57 32.85 59\nTable10: Comparisonbetweendifferentsummarizationmethods.", "metadata": {"id": "65e765fbafb9f68ddc58a55d676d665ccb0da3ff", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 9, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "Table10: Comparisonbetweendifferentsummarizationmethods.\nthe pre-calculated log probabilities of query tokens, allowing for rapid reranking at inference.\nTILDEv2improvesthisbyindexingonlydocument-presenttokens,usingNCEloss,andexpanding\ndocuments,thusenhancingefficiencyandreducingindexsize.\nOurexperimentswereconductedontheMSMARCOPassagerankingdataset[47],alarge-scale\ndatasetformachinereadingcomprehension.Wefollowandmakemodificationstotheimplementation\nprovidedbyPyGaggle[26]andTILDE[28],usingthemodelsmonoT5,monoBERT,RankLLaMA", "metadata": {"id": "6b0fc7fa2c8c788aef24b7dcd9526d24bc36cdf4", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 9, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "providedbyPyGaggle[26]andTILDE[28],usingthemodelsmonoT5,monoBERT,RankLLaMA\nandTILDEv2. RerankingresultsareshowninTable9. WerecommendmonoT5asacomprehensive\nmethod balancing performance and efficiency. RankLLaMA is suitable for achieving the best\nperformance,whileTILDEv2isidealforthequickestexperienceonafixedcollection. Detailson\ntheexperimentalsetupandresultsarepresentedinAppendixA.3.\n3.6 DocumentRepacking\nTheperformanceofsubsequentprocesses,suchasLLMresponsegeneration,maybeaffectedbythe\norderdocumentsareprovided. Toaddressthisissue,weincorporateacompactrepackingmoduleinto", "metadata": {"id": "c7de1a44ffe9f3d65e376ae1c4ad2757650ff91e", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 9, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "orderdocumentsareprovided. Toaddressthisissue,weincorporateacompactrepackingmoduleinto\ntheworkflowafterreranking,featuringthreerepackingmethods: “forward”,“reverse”and“sides”.\nThe“forward”methodrepacksdocumentsbydescendingrelevancyscoresfromthererankingphase,\nwhereasthe“reverse”arrangestheminascendingorder. InspiredbyLiuetal.[48],concludingthat\noptimalperformanceisachievedwhenrelevantinformationisplacedattheheadortailoftheinput,\nwealsoincludea“sides”option.\nSince the repacking method primarily affects subsequent modules, we select the best repacking", "metadata": {"id": "21a10f8146b878382d2dc29b0bbdeb980dcd7e7c", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 9, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "Since the repacking method primarily affects subsequent modules, we select the best repacking\nmethodinSection4bytestingitincombinationwithothermodules. Inthissection,wechoosethe\n“sides”methodasthedefaultrepackingmethod.\n3.7 Summarization\nRetrievalresultsmaycontainredundantorunnecessaryinformation,potentiallypreventingLLMs\nfromgeneratingaccurateresponses. Additionally,longpromptscanslowdowntheinferenceprocess.\nTherefore,efficientmethodstosummarizeretrieveddocumentsarecrucialintheRAGpipeline.\nSummarizationtaskscanbeextractiveorabstractive. Extractivemethodssegmenttextintosen-", "metadata": {"id": "7d13d49c7f4df7852725bd1f923cd9e64b281712", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 9, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "Summarizationtaskscanbeextractiveorabstractive. Extractivemethodssegmenttextintosen-\ntences,thenscoreandrankthembasedonimportance. Abstractivecompressorssynthesizeinfor-\nmationfrommultipledocumentstorephraseandgenerateacohesivesummary. Thesetaskscanbe\nquery-basedornon-query-based. Inthispaper,asRAGretrievesinformationrelevanttoqueries,we\nfocusexclusivelyonquery-basedmethods.\n• Recomp: Recomp[23]hasextractiveandabstractivecompressors. Theextractivecompressor\nselectsusefulsentences,whiletheabstractivecompressorsynthesizesinformationfrommultiple\ndocuments.", "metadata": {"id": "43a2e92a559096d6fb123797c7bb2c04dc51e6f2", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 9, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "selectsusefulsentences,whiletheabstractivecompressorsynthesizesinformationfrommultiple\ndocuments.\n• LongLLMLingua: LongLLMLingua[49]improvesLLMLinguabyfocusingonkeyinforma-\ntionrelatedtothequery.\n• Selective Context SelectiveContextenhances LLM efficiency byidentifying andremoving\nredundantinformationintheinputcontext. Itevaluatestheinformativenessoflexicalunitsusing\n9", "metadata": {"id": "1c7f515b3b4c6b27aea52b82e980e9ab730c0cab", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 9, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "self-informationcomputedbyabasecausallanguagemodel. Thismethodisnon-query-based,\nallowingacomparisonbetweenquery-basedandnon-query-basedapproaches.\nWeevaluatethesemethodsonthreebenchmarkdatasets:NQ,TriviaQA,andHotpotQA.Comparative\nresults of different summarization methods are shown in Table 10. We recommend Recomp for\nits outstanding performance. LongLLMLingua does not perform well but demonstrates better\ngeneralizationcapabilitiesasitwasnottrainedontheseexperimentaldatasets.Therefore,weconsider\nitasanalternativemethod. Additionalimplementationdetailsanddiscussionsonnon-query-based", "metadata": {"id": "f687807105aa208d543c637aa940702f2443d9c9", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 10, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "itasanalternativemethod. Additionalimplementationdetailsanddiscussionsonnon-query-based\nmethodsareprovidedinAppendixA.4.\n3.8 GeneratorFine-tuning\nInthissection,wefocusonfine-tuningthegeneratorwhileleavingretrieverfine-tuningforfuture\nexploration. Weaimtoinvestigatetheimpactoffine-tuning,particularlytheinfluenceofrelevantor\nirrelevantcontextsonthegenerator’sperformance.\nFormally,wedenotexasthequeryfedintotheRAGsystem,andDasthecontextsforthisinput.\nThefine-tuninglossofthegeneratoristhenegativelog-likelihoodoftheground-truthoutputy.", "metadata": {"id": "2f132f9e840d35f72f5b65548cd3575d72c22612", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 10, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "Thefine-tuninglossofthegeneratoristhenegativelog-likelihoodoftheground-truthoutputy.\nToexploretheimpactoffine-tuning,especiallyrelevantandirrelevantcontexts,wedefined asa\ngold\ncontextrelevanttothequery,andd asarandomlyretrievedcontext. Wetrainthemodelby\nrandom\nvaryingthecompositionofDasfollows:\n• D : Theaugmentedcontextconsistsofquery-relevantdocuments,denotedasD ={d }.\ng g gold\n• D : Thecontextcontainsonerandomlysampleddocument,denotedasD ={d }.\nr r random\n• D : Theaugmentedcontextcomprisesarelevantdocumentandarandomly-selectedone,denoted\ngr\nasD ={d ,d }.\ngr gold random", "metadata": {"id": "efd6fd6a4a00cc21618b75f72fa00fa249872bba", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 10, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "gr\nasD ={d ,d }.\ngr gold random\n• D : The augmented context consists of two copies of a query-relevant document, denoted as\ngg\nD ={d ,d }.\ngg gold gold\nWe denote the base LM generator not fine-tuned as M , and the model fine-tuned under the\nb\ncorresponding D as M , M , M , M . We fine-tuned our model on several QA and reading\ng r gr gg\n100\n80\n60\n40\n20\n0\n\u0000\u0006 \u0000\u0006\u0000 \u0000\u0006\u0000 \u0000\u0006\u0000\u0000\n\u0000\u0000\u0000\u0000\u0000\u0015\u0000ȱ\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0005\ncomprehensiondatasets. Ground-truthcoverage\nisusedasourevaluationmetricsinceQAtask\n\u0000\u000f\u0000\u0000\u0000\u0000\u0000\u0000ȱ\u0000\u0016\u0000\u0000\u0000\u0000\u0000\u0000\u0000ȱ\u0000 \u0000\u0000\u0000\u0000ȱ\u0000\u0006\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000ȱ\u0000\u000f\u0000\u0000\u0000\u0000\u0000", "metadata": {"id": "a1f1d698343871e1ce027dc377772f176f1476f4", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 10, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "\u0000\u000f\u0000\u0000\u0000\u0000\u0000\u0000ȱ\u0000\u0016\u0000\u0000\u0000\u0000\u0000\u0000\u0000ȱ\u0000 \u0000\u0000\u0000\u0000ȱ\u0000\u0006\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000ȱ\u0000\u000f\u0000\u0000\u0000\u0000\u0000\nanswersarerelativelyshort. WeselectLlama-2- \u0000\u000f\u0000 \u0000\u000f\u0000 \u0000\u000f\u0000\u0000\n7B[50]asthebasemodel. Similartotraining, \u0000\u000f\u0000 \u0000\u000f\u0000\u0000\nweevaluatealltrainedmodelsonvalidationsets\nwithD\ng\n,D\nr\n,D\ngr\n,andD∅,whereD∅indicates\ninference without retrieval. Figure 3 presents\nourmainresults. Modelstrainedwithamixof\nrelevantandrandomdocuments(M )perform\ngr\nbestwhenprovidedwitheithergoldormixed\ncontexts. Thissuggeststhatmixingrelevantand\nrandomcontextsduringtrainingcanenhancethe\ngenerator’srobustnesstoirrelevantinformation", "metadata": {"id": "ba5cff007283ea14b56a9d918beec7be24eed17e", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 10, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "randomcontextsduringtrainingcanenhancethe\ngenerator’srobustnesstoirrelevantinformation\nwhileensuringeffectiveutilizationofrelevant Figure3: Resultsofgeneratorfine-tuning.\ncontexts. Therefore,weidentifythepracticeof\naugmenting with a few relevantandrandomly-selected documentsduringtraining as the best\napproach. Detaileddatasetinformation,hyperparametersandexperimentalresultscanbefoundin\nAppendixA.5.\n4 SearchingforBestRAGPractices\nIn the following section, we investigate the optimal practices for implementing RAG. To begin", "metadata": {"id": "743ecb3a9dfb84a63a3ea693f7977ee0727195aa", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 10, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "In the following section, we investigate the optimal practices for implementing RAG. To begin\nwith,weusedthedefaultpracticeidentifiedinSection3foreachmodule. Followingtheworkflow\ndepictedinFigure1,wesequentiallyoptimizedindividualmodulesandselectedthemosteffective\noptionamongalternatives. Thisiterativeprocesscontinueduntilwedeterminedthebestmethodfor\nimplementingthefinalsummarizationmodule. BasedonSection3.8,weusedtheLlama2-7B-Chat\nmodelfine-tunedwhereeachquerywasaugmentedbyafewrandom-selectedandrelevantdocuments\n10", "metadata": {"id": "ca64c8aefb60f2970d7e3c533e21e72503fda83e", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 10, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "Commonsense FactCheck ODQA Multihop Medical RAG Avg.\nMethod\nAcc Acc EM F1 EM F1 Acc Score Score F1 Latency\nclassificationmodule ,HybridwithHyDE,monoT5,sides,Recomp\nw/oclassification 0.719 0.505 0.391 0.450 0.212 0.255 0.528 0.540 0.465 0.353 16.58\n+classification 0.727 0.595 0.393 0.450 0.207 0.257 0.460 0.580 0.478 0.353 11.71\nwithclassification, retrievalmodule ,monoT5,sides,Recomp\n+HyDE 0.718 0.595 0.320 0.373 0.170 0.213 0.400 0.545 0.443 0.293 11.58\n+Original 0.721 0.585 0.300 0.350 0.153 0.197 0.390 0.486 0.428 0.273 1.44", "metadata": {"id": "76a5dac34b1309838faa8879570ca647dc968592", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 11, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "+Original 0.721 0.585 0.300 0.350 0.153 0.197 0.390 0.486 0.428 0.273 1.44\n+Hybrid 0.718 0.595 0.347 0.397 0.190 0.240 0.750 0.498 0.477 0.318 1.45\n+HybridwithHyDE 0.727 0.595 0.393 0.450 0.207 0.257 0.460 0.580 0.478 0.353 11.71\nwithclassification,HybridwithHyDE, rerankingmodule ,sides,Recomp\nw/oreranking 0.720 0.591 0.365 0.429 0.211 0.260 0.512 0.530 0.470 0.334 10.31\n+monoT5 0.727 0.595 0.393 0.450 0.207 0.257 0.460 0.580 0.478 0.353 11.71\n+monoBERT 0.723 0.593 0.383 0.443 0.217 0.259 0.482 0.551 0.475 0.351 11.65", "metadata": {"id": "90a1f757f873cd793d82a37af5d4f247d2ac48b2", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 11, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "+monoBERT 0.723 0.593 0.383 0.443 0.217 0.259 0.482 0.551 0.475 0.351 11.65\n+RankLLaMA 0.723 0.597 0.382 0.443 0.197 0.240 0.454 0.558 0.470 0.342 13.51\n+TILDEv2 0.725 0.588 0.394 0.456 0.209 0.255 0.486 0.536 0.476 0.355 11.26\nwithclassification,HybridwithHyDE,monoT5, repackingmodule ,Recomp\n+sides 0.727 0.595 0.393 0.450 0.207 0.257 0.460 0.580 0.478 0.353 11.71\n+forward 0.722 0.599 0.379 0.437 0.215 0.260 0.472 0.542 0.474 0.349 11.68\n+reverse 0.728 0.592 0.387 0.445 0.219 0.263 0.532 0.560 0.483 0.354 11.70\nwithclassification,HybridwithHyDE,monoT5,reverse, summarizationmodule", "metadata": {"id": "db7ac057aebee765a07e4dab9184356afe04034d", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 11, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "withclassification,HybridwithHyDE,monoT5,reverse, summarizationmodule\nw/osummarization 0.729 0.591 0.402 0.457 0.205 0.252 0.528 0.533 0.480 0.355 10.97\n+Recomp 0.728 0.592 0.387 0.445 0.219 0.263 0.532 0.560 0.483 0.354 11.70\n+LongLLMLingua 0.713 0.581 0.362 0.423 0.199 0.245 0.530 0.539 0.466 0.334 16.17\nTable11: ResultsofthesearchforoptimalRAGpractices. Modulesenclosedina boxedmodule\nareunderinvestigationtodeterminethebestmethod.Theunderlinedmethodrepresentstheselected\nimplementation. The“Avg”(averagescore)iscalculatedbasedontheAcc,EM,andRAGscoresfor", "metadata": {"id": "c09af7f7f7ceefc29feae81af4e9798967aace35", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 11, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "implementation. The“Avg”(averagescore)iscalculatedbasedontheAcc,EM,andRAGscoresfor\nalltasks,whiletheaveragelatencyismeasuredinsecondsperquery. Thebestscoresarehighlighted\ninbold.\nasthegenerator. WeusedMilvustobuildavectordatabasethatincludes10milliontextofEnglish\nWikipediaand4milliontextofmedicaldata. WealsoinvestigatedtheimpactofremovingtheQuery\nClassification,Reranking,andSummarizationmodulestoassesstheircontributions.\n4.1 ComprehensiveEvaluation\nWe conducted extensive experiments across various NLP tasks and datasets to assess the perfor-", "metadata": {"id": "54341d4dfd23ece59bdc469aefbdb3e5ca77ee0f", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 11, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "We conducted extensive experiments across various NLP tasks and datasets to assess the perfor-\nmance of RAG systems. Specifically: (I) Commonsense Reasoning; (II) Fact Checking; (III)\nOpen-Domain QA; (IV) MultiHop QA; (V) Medical QA. For further details on the tasks and\ntheircorrespondingdatasets, pleaserefertoAppendixA.6. Furthermore, weevaluatedtheRAG\ncapabilitiesonsubsetsextractedfromthesedatasets,employingthemetricsrecommendedinRA-\nGAs[51],includingFaithfulness,ContextRelevancy,AnswerRelevancy,andAnswerCorrectness.", "metadata": {"id": "8e983ff76dd3658002cdcfb6b3f707f481610778", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 11, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "GAs[51],includingFaithfulness,ContextRelevancy,AnswerRelevancy,andAnswerCorrectness.\nAdditionally,wemeasuredRetrievalSimilaritybycomputingthecosinesimilaritybetweenretrieved\ndocumentsandgolddocuments.\nWeusedaccuracyastheevaluationmetricforthetasksofCommonsenseReasoning,FactChecking,\nandMedicalQA.ForOpen-DomainQAandMultihopQA,weemployedtoken-levelF1scoreand\nExactMatch(EM)score. ThefinalRAGscorewascalculatedbyaveragingtheaforementionedfive\nRAGcapabilities. WefollowedTrivedietal.[52]andsub-sampledupto500examplesfromeach\ndataset.\n4.2 ResultsandAnalysis", "metadata": {"id": "e0eaac7f2b457f273ab4678d3d700373671ba38e", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 11, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "dataset.\n4.2 ResultsandAnalysis\nBasedontheexperimentalresultspresentedinTable11,thefollowingkeyinsightsemerge:\n• QueryClassificationModule: Thismoduleisreferencedandcontributestobotheffectiveness\nandefficiency,leadingtoanaverageimprovementintheoverallscorefrom0.428to0.443anda\nreductioninlatencytimefrom16.41to11.58secondsperquery.\n11", "metadata": {"id": "f17c94abc5916416f38ae76c4fb35ba6e2992a22", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 11, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "• RetrievalModule: Whilethe“HybridwithHyDE”methodattainedthehighestRAGscoreof\n0.58,itdoessoataconsiderablecomputationalcostwith11.71secondperquery. Consequently,\nthe“Hybrid”or“Original”methodsarerecommended,astheyreducelatencywhilemaintaining\ncomparableperformance.\n• RerankingModule: Theabsenceofarerankingmoduleledtoanoticeabledropinperformance,\nhighlightingitsnecessity. MonoT5achievedthehighestaveragescore,affirmingitsefficacyin\naugmentingtherelevanceofretrieveddocuments. Thisindicatesthecriticalroleofrerankingin\nenhancingthequalityofgeneratedresponses.", "metadata": {"id": "dac669cf62a6b778eed08ab6420a24a4cabeecfe", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 12, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "enhancingthequalityofgeneratedresponses.\n• Repacking Module: The Reverse configuration exhibited superior performance, achieving an\nRAGscoreof0.560. Thisindicatesthatpositioningmorerelevantcontextclosertothequeryleads\ntooptimaloutcomes.\n• SummarizationModule: Recompdemonstratedsuperiorperformance,althoughachievingcompa-\nrableresultswithlowerlatencywaspossiblebyremovingthesummarizationmodule. Nevertheless,\nRecompremainsthepreferredchoiceduetoitscapabilitytoaddressthegenerator’smaximum\nlengthconstraints. Intime-sensitiveapplications,removingsummarizationcouldeffectivelyreduce\nresponsetime.", "metadata": {"id": "a60428f0ddc32defe70c078a348b47a87cc1da6f", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 12, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "responsetime.\nTheexperimentalresultsdemonstratethateachmodulecontributesuniquelytotheoverallperfor-\nmanceoftheRAGsystem. Thequeryclassificationmoduleenhancesaccuracyandreduceslatency,\nwhiletheretrievalandrerankingmodulessignificantlyimprovethesystem’sabilitytohandlediverse\nqueries. The repacking and summarization modules further refine the system’s output, ensuring\nhigh-qualityresponsesacrossdifferenttasks.\n5 Discussion\n5.1 BestPracticesforImplementingRAG\nAccordingtoourexperimentalfindings,wesuggesttwodistinctrecipesorpracticesforimplementing", "metadata": {"id": "6fc01b73d9712c0b3769cb48709be08ef4a4071c", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 12, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "Accordingtoourexperimentalfindings,wesuggesttwodistinctrecipesorpracticesforimplementing\nRAG systems, each customized to address specific requirements: one focusing on maximizing\nperformance,andtheotheronstrikingabalancebetweenefficiencyandefficacy.\nBestPerformancePractice: Toachievethehighestperformance,itisrecommendedtoincorporate\nqueryclassificationmodule,usethe“HybridwithHyDE”methodforretrieval,employmonoT5for\nreranking,optforReverseforrepacking,andleverageRecompforsummarization. Thisconfiguration\nyieldedthehighestaveragescoreof0.483,albeitwithacomputationally-intensiveprocess.", "metadata": {"id": "2a13c81f0de894b41db88fd480c09312f5f367e0", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 12, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "yieldedthehighestaveragescoreof0.483,albeitwithacomputationally-intensiveprocess.\nBalancedEfficiencyPractice: Inordertoachieveabalancebetweenperformanceandefficiency,\nitisrecommendedtoincorporatethequeryclassificationmodule, implementtheHybridmethod\nforretrieval,useTILDEv2forreranking,optforReverseforrepacking,andemployRecompfor\nsummarization. Giventhattheretrievalmoduleaccountsforthemajorityofprocessingtimeinthe\nsystem,transitioningtotheHybridmethodwhilekeepingothermodulesunchangedcansubstantially\nreducelatencywhilepreservingacomparableperformance.\n5.2 MultimodalExtension", "metadata": {"id": "a971312bd0ad2c66a2bcc6f67336809eefeb0c89", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 12, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "reducelatencywhilepreservingacomparableperformance.\n5.2 MultimodalExtension\nWehaveextendedRAGtomultimodalapplications. Specifically,wehaveincorporatedtext2image\nandimage2textretrievalcapabilitiesintothesystemwithasubstantialcollectionofpairedimageand\ntextualdescriptionsasaretrievalsource. AsdepictedinFigure4,thetext2imagecapabilityspeeds\nuptheimagegenerationprocesswhenauserqueryalignswellwiththetextualdescriptionsofstored\nimages(i.e.,“retrievalasgeneration”strategy),whiletheimage2textfunctionalitycomesintoplay\nwhenauserprovidesanimageandengagesinconversationabouttheinputimage. Thesemultimodal", "metadata": {"id": "5ad45ad1811d50bbdfdbd0dc5d3219a65ab3c7f1", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 12, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "whenauserprovidesanimageandengagesinconversationabouttheinputimage. Thesemultimodal\nRAGcapabilitiesofferthefollowingadvantages:\n• Groundedness:Retrievalmethodsprovideinformationfromverifiedmultimodalmaterials,thereby\nensuringauthenticityandspecificity. Incontrast,on-the-flygenerationreliesonmodelstogenerate\nnewcontent,whichcanoccasionallyresultinfactualerrorsorinaccuracies.\n• Efficiency: Retrievalmethodsaretypicallymoreefficient,especiallywhentheansweralready\nexists in stored materials. Conversely, generation methods may require more computational", "metadata": {"id": "693113e571b9ca55f9ff5fa1ed33ed4ad6a1e9b0", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 12, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "exists in stored materials. Conversely, generation methods may require more computational\nresourcestoproducenewcontent,particularlyforimagesorlengthytexts.\n12", "metadata": {"id": "fe948db7542ffd46f827eb8fa36834eac4922cb9", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 12, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "Text2image Retrieval\nRetrieval\nA dog is sleeping\nRetrieval\nA dog is sleeping\nImage2text Retrieval\nRetrieval\nA dog is sleeping.\nA dog is drinking water\nRetrieval User Query\nLow Similarity\nHigh Similarity\nImage Generation Model\nA dog is sleeping\nImage Caption Model\nFigure4: Workflowofmultimodalretrieval. Theuppersectionillustratesthetext-to-imageretrieval\nprocess. Initially,atextqueryisusedtofindimagesinthedatabasewiththehighestsimilarity. Ifa\nhighsimilarityisfound,theimageisreturneddirectly. Ifnot,animagegenerationmodelisemployed", "metadata": {"id": "d811ccf81b14b465abf52698fa74c4addfd4ccf0", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 13, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "highsimilarityisfound,theimageisreturneddirectly. Ifnot,animagegenerationmodelisemployed\ntocreateandreturnanappropriateimage. Thelowersectiondemonstratestheimage-to-textretrieval\nprocess. Here, auser-providedimageismatchedwithimagesinthedatabasetofindthehighest\nsimilarity. Ifahighsimilarityisidentified,thepre-storedcaptionofthematchingimageisreturned.\nOtherwise,animagecaptioningmodelgeneratesandreturnsanewcaption.\n• Maintainability: Generationmodelsoftennecessitatecarefulfine-tuningtotailorthemfornew\napplications. Incontrast,retrieval-basedmethodscanbeimprovedtoaddressnewdemandsby", "metadata": {"id": "3020a42538a30a6ad57078c669cb8cc3cddc3b99", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 13, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "applications. Incontrast,retrieval-basedmethodscanbeimprovedtoaddressnewdemandsby\nsimplyenlargingthesizeandenhancingthequalityofretrievalsources.\nWeplantobroadentheapplicationofthisstrategytoincludeothermodalities,suchasvideoand\nspeech,whilealsoexploringefficientandeffectivecross-modalretrievaltechniques.\n6 Conclusion\nInthisstudy,weaimtoidentifyoptimalpracticesforimplementingretrieval-augmentedgeneration\ninordertoimprovethequalityandreliabilityofcontentproducedbylargelanguagemodels. We\nsystematicallyassessedarangeofpotentialsolutionsforeachmodulewithintheRAGframework", "metadata": {"id": "0cd3540d84728c045aba1325bea2c1b83959271a", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 13, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "systematicallyassessedarangeofpotentialsolutionsforeachmodulewithintheRAGframework\nand recommended the most effective approach for each module. Furthermore, we introduced a\ncomprehensive evaluation benchmark for RAG systems and conducted extensive experiments to\ndeterminethebestpracticesamongvariousalternatives. Ourfindingsnotonlycontributetoadeeper\nunderstandingofretrieval-augmentedgenerationsystemsbutalsoestablishafoundationforfuture\nresearch.\nLimitations\nWehaveevaluatedtheimpactofvariousmethodsforfine-tuningLLMgenerators. Previousstudies", "metadata": {"id": "0f52c90e250e522e582a80bd02e60953f162b5e9", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 13, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "Limitations\nWehaveevaluatedtheimpactofvariousmethodsforfine-tuningLLMgenerators. Previousstudies\nhave demonstrated the feasibility of training both the retriever and generator jointly. We would\nliketoexplorethispossibilityinthefuture. Inthisstudy, weembracedtheprincipleofmodular\n13", "metadata": {"id": "4399c0d17f094fbbf644b120048c22a21913448c", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 13, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "designtosimplifythesearchforoptimalRAGimplementations,therebyreducingcomplexity. Due\ntothedauntingcostsassociatedwithconstructingvectordatabasesandconductingexperiments,our\nevaluationwaslimitedtoinvestigatingtheeffectivenessandinfluenceofrepresentativechunking\ntechniques within the chunking module. It would be intriguing to further explore the impact of\ndifferentchunkingtechniquesontheentireRAGsystems. Whilewehavediscussedtheapplicationof\nRAGinthedomainofNLPandextendeditsscopetoimagegeneration,anenticingavenueforfuture", "metadata": {"id": "3a6b408f0338580d2d64c5bc3b90485b05be9f88", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 14, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "RAGinthedomainofNLPandextendeditsscopetoimagegeneration,anenticingavenueforfuture\nexplorationwouldinvolveexpandingthisresearchtoothermodalitiessuchasspeechandvideo.\nAcknowledgments\nTheauthorswouldliketothanktheanonymousreviewersfortheirvaluablecomments. Thiswork\nwassupportedbyNationalNaturalScienceFoundationofChina(No. 62076068).\nReferences\n[1] LongOuyang,JeffWu,XuJiang,DiogoAlmeida,CarrollL.Wainwright,PamelaMishkin,\nChongZhang,SandhiniAgarwal,KatarinaSlama,AlexRay,JohnSchulman,JacobHilton,\nFraserKelton,LukeMiller,MaddieSimens,AmandaAskell,PeterWelinder,PaulChristiano,", "metadata": {"id": "0984c6637956a46d8f83b0050cea332cee8d6413", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 14, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "FraserKelton,LukeMiller,MaddieSimens,AmandaAskell,PeterWelinder,PaulChristiano,\nJan Leike, and Ryan Lowe. Training language models to follow instructions with human\nfeedback.InProceedingsoftheConferenceonNeuralInformationProcessingSystems(NeurIPS\n2022),2022.\n[2] RafaelRafailov,ArchitSharma,EricMitchell,StefanoErmon,ChristopherDManning,and\nChelseaFinn. Directpreferenceoptimization: Yourlanguagemodelissecretlyarewardmodel.\narXivpreprintarXiv:2305.18290,2023.\n[3] YaoZhao,RishabhJoshi,TianqiLiu,MishaKhalman,MohammadSaleh,andPeterJLiu.SLIC-", "metadata": {"id": "d39e160487ebaf34b412da46fc855c9fee8c217d", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 14, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "[3] YaoZhao,RishabhJoshi,TianqiLiu,MishaKhalman,MohammadSaleh,andPeterJLiu.SLIC-\nHF:Sequencelikelihoodcalibrationwithhumanfeedback. arXivpreprintarXiv:2305.10425,\n2023.\n[4] ZhengYuan,HongyiYuan,ChuanqiTan,WeiWang,SongfangHuang,andFeiHuang. RRHF:\nRankresponsestoalignlanguagemodelswithhumanfeedbackwithouttears. arXivpreprint\narXiv:2304.05302,2023.\n[5] WenhaoLiu,XiaohuaWang,MulingWu,TianlongLi,ChangzeLv,ZixuanLing,JianhaoZhu,\nCenyuanZhang,XiaoqingZheng,andXuanjingHuang. Aligninglargelanguagemodelswith\nhumanpreferencesthroughrepresentationengineering. arXivpreprintarXiv:2312.15997,2023.", "metadata": {"id": "fe159db105c3f9105d09ad82b982654a59bfcfc0", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 14, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "humanpreferencesthroughrepresentationengineering. arXivpreprintarXiv:2312.15997,2023.\n[6] YunfanGao,YunXiong,XinyuGao,KangxiangJia,JinliuPan,YuxiBi,YiDai,JiaweiSun,\nandHaofenWang. Retrieval-augmentedgenerationforlargelanguagemodels: Asurvey. arXiv\npreprintarXiv:2312.10997,2023.\n[7] HuayangLi,YixuanSu,DengCai,YanWang,andLemaoLiu.Asurveyonretrieval-augmented\ntextgeneration. arXivpreprintarXiv:2202.01110,2022.\n[8] DengCai,YanWang,LemaoLiu,andShumingShi. Recentadvancesinretrieval-augmented\ntextgeneration. InProceedingsofthe45thinternationalACMSIGIRconferenceonresearch", "metadata": {"id": "5f2d6404cba55ace2ca37cd69081b44b919397de", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 14, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "textgeneration. InProceedingsofthe45thinternationalACMSIGIRconferenceonresearch\nanddevelopmentininformationretrieval,pages3417–3419,2022.\n[9] Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. Query rewriting for\nretrieval-augmentedlargelanguagemodels. arXivpreprintarXiv:2305.14283,2023.\n[10] Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. Precise zero-shot dense retrieval\nwithoutrelevancelabels. arXivpreprintarXiv:2212.10496,2022.\n[11] LiangWang,NanYang,XiaolongHuang,BinxingJiao,LinjunYang,DaxinJiang,Rangan", "metadata": {"id": "f92ecf2af424b7255701412506c5132b5f397100", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 14, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "[11] LiangWang,NanYang,XiaolongHuang,BinxingJiao,LinjunYang,DaxinJiang,Rangan\nMajumder, and Furu Wei. Text embeddings by weakly-supervised contrastive pre-training.\narXivpreprintarXiv:2212.03533,2022.\n[12] ShitaoXiao,ZhengLiu,PeitianZhang,andNiklasMuennighoff. C-pack: Packagedresources\ntoadvancegeneralchineseembedding,2023.\n14", "metadata": {"id": "fe17bcc94704cc7079bd45f44da8da9c7816022a", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 14, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "[13] OpenAI. GPT-4technicalreport. CoRR,abs/2303.08774,2023. doi: 10.48550/ARXIV.2303.\n08774. URLhttps://doi.org/10.48550/arXiv.2303.08774.\n[14] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,Timo-\nthéeLacroix,BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,etal. LLaMA:Open\nandefficientfoundationlanguagemodels. arXivpreprintarXiv:2302.13971,2023.\n[15] YueZhang,YafuLi,LeyangCui,DengCai,LemaoLiu,TingchenFu,XintingHuang,Enbo\nZhao,YuZhang,YulongChen,etal. Siren’ssongintheaiocean: asurveyonhallucinationin\nlargelanguagemodels. arXivpreprintarXiv:2309.01219,2023.", "metadata": {"id": "e9c8a3829d610051ce1935d46165aa1f8a07fee9", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 15, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "largelanguagemodels. arXivpreprintarXiv:2309.01219,2023.\n[16] XiaohuaWang,YuliangYan,LongtaoHuang,XiaoqingZheng,andXuan-JingHuang. Halluci-\nnationdetectionforgenerativelargelanguagemodelsbybayesiansequentialestimation. In\nProceedingsofthe2023ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,\npages15361–15371,2023.\n[17] Liang Wang, Nan Yang, and Furu Wei. Query2doc: Query expansion with large language\nmodels. arXivpreprintarXiv:2303.07678,2023.\n[18] GangwooKim,SungdongKim,ByeonggukJeon,JoonsukPark,andJaewooKang. Treeof", "metadata": {"id": "054f3991f6e1c6007db4ddc4cf9fbed106f91eb2", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 15, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "[18] GangwooKim,SungdongKim,ByeonggukJeon,JoonsukPark,andJaewooKang. Treeof\nclarifications: Answeringambiguousquestionswithretrieval-augmentedlargelanguagemodels.\narXivpreprintarXiv:2310.14696,2023.\n[19] JerryLiu. LlamaIndex,112022. URLhttps://github.com/jerryjliu/llama_index.\n[20] PeitianZhang,ShitaoXiao,ZhengLiu,ZhichengDou,andJian-YunNie. Retrieveanythingto\naugmentlargelanguagemodels. arXivpreprintarXiv:2310.07554,2023.\n[21] Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang.\nTowards general text embeddings with multi-stage contrastive learning. arXiv preprint", "metadata": {"id": "7cb4367ff1d558290eb028c9689890a033d48719", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 15, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "Towards general text embeddings with multi-stage contrastive learning. arXiv preprint\narXiv:2308.03281,2023.\n[22] Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Llmlingua:\nCompressing prompts for accelerated inference of large language models. arXiv preprint\narXiv:2310.05736,2023.\n[23] FangyuanXu,WeijiaShi,andEunsolChoi. Recomp: Improvingretrieval-augmentedlmswith\ncompressionandselectiveaugmentation. arXivpreprintarXiv:2310.04408,2023.\n[24] ZhiruoWang,JunAraki,ZhengbaoJiang,MdRizwanParvez,andGrahamNeubig. Learning", "metadata": {"id": "d811061ecf3d87cc2004cc8fe615f92e4fd37d68", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 15, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "[24] ZhiruoWang,JunAraki,ZhengbaoJiang,MdRizwanParvez,andGrahamNeubig. Learning\ntofiltercontextforretrieval-augmentedgeneration. arXivpreprintarXiv:2311.08377,2023.\n[25] RodrigoNogueira,WeiYang,KyunghyunCho,andJimmyLin. Multi-stagedocumentranking\nwithbert. arXivpreprintarXiv:1910.14424,2019.\n[26] Rodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. Document ranking with a pretrained\nsequence-to-sequencemodel. arXivpreprintarXiv:2003.06713,2020.\n[27] Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. Fine-tuning llama for\nmulti-stagetextretrieval. arXivpreprintarXiv:2310.08319,2023.", "metadata": {"id": "9e1b4e506c88375b6ac81d24b38f950329f5ab97", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 15, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "multi-stagetextretrieval. arXivpreprintarXiv:2310.08319,2023.\n[28] ShengyaoZhuangandGuidoZuccon. Tilde: Termindependentlikelihoodmodelforpassage\nre-ranking. InProceedingsofthe44thInternationalACMSIGIRConferenceonResearchand\nDevelopmentinInformationRetrieval,pages1483–1492,2021.\n[29] ShengyaoZhuangandGuidoZuccon. Fastpassagere-rankingwithcontextualizedexactterm\nmatchingandefficientpassageexpansion. arXivpreprintarXiv:2108.08513,2021.\n[30] HongyinLuo,Yung-SungChuang,YuanGong,TianhuaZhang,YoonKim,XixinWu,Danny\nFox, Helen M. Meng, and James R. Glass. Sail: Search-augmented instruction learning.", "metadata": {"id": "e1dfcae0aeb844e522889253cc6f0605fff7be32", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 15, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "Fox, Helen M. Meng, and James R. Glass. Sail: Search-augmented instruction learning.\nIn Conference on Empirical Methods in Natural Language Processing, 2023. URL https:\n//api.semanticscholar.org/CorpusID:258865283.\n15", "metadata": {"id": "bd7bc01f48bafbedbe5036af67434efdc460d5a7", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 15, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "[31] Tianjun Zhang, Shishir G. Patil, Naman Jain, Sheng Shen, Matei A. Zaharia, Ion Stoica,\nand Joseph E. Gonzalez. Raft: Adapting language model to domain specific rag. ArXiv,\nabs/2403.10131,2024.\n[32] ZihanLiu,WeiPing,RajarshiRoy,PengXu,ChankyuLee,MohammadShoeybi,andBryan\nCatanzaro. Chatqa: Surpassing gpt-4 on conversational qa and rag. 2024. URL https:\n//api.semanticscholar.org/CorpusID:267035133.\n[33] GautierIzacard,PatrickLewis,MariaLomeli,LucasHosseini,FabioPetroni,TimoSchick,\nJane A. Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with", "metadata": {"id": "66f89b226f7a6970b15f3479bb11aa3785b4457d", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 16, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "Jane A. Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with\nretrievalaugmentedlanguagemodels. ArXiv,abs/2208.03299,2022.\n[34] LingxiZhang,YueYu,KuanWang,andChaoZhang. Arl2: Aligningretrieversforblack-box\nlarge language models via self-guided adaptive relevance labeling. ArXiv, abs/2402.13542,\n2024.\n[35] WeijiaShi,SewonMin,MichihiroYasunaga,MinjoonSeo,RichJames,MikeLewis,Luke\nZettlemoyer,andWen-tauYih. Replug:Retrieval-augmentedblack-boxlanguagemodels. arXiv\npreprintarXiv:2301.12652,2023.", "metadata": {"id": "d7523404a716ce086a272a16ad4904db8514a13a", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 16, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "preprintarXiv:2301.12652,2023.\n[36] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Realm:\nRetrieval-augmentedlanguagemodelpre-training. ArXiv,abs/2002.08909,2020.\n[37] Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Rich James, Pedro\nRodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, Luke Zettlemoyer, and Scott Yih.\nRa-dit: Retrieval-augmenteddualinstructiontuning. ArXiv,abs/2310.01352,2023.\n[38] HamedZamaniandMichaelBendersky. Stochasticrag: End-to-endretrieval-augmentedgen-", "metadata": {"id": "32f52c030d0995f7b8555107a658c88e2fe640d3", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 16, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "[38] HamedZamaniandMichaelBendersky. Stochasticrag: End-to-endretrieval-augmentedgen-\nerationthroughexpectedutilitymaximization. 2024. URLhttps://api.semanticscholar.\norg/CorpusID:269605438.\n[39] YizhengHuangandJimmyHuang. Asurveyonretrieval-augmentedtextgenerationforlarge\nlanguagemodels. arXivpreprintarXiv:2404.10981,2024.\n[40] Ruochen Zhao, Hailin Chen, Weishi Wang, Fangkai Jiao, Xuan Long Do, Chengwei Qin,\nBoshengDing,XiaobaoGuo,MinzhiLi,XingxuanLi,etal. Retrievingmultimodalinformation\nforaugmentedgeneration: Asurvey. arXivpreprintarXiv:2303.10868,2023.", "metadata": {"id": "c5e208a15a0308cb91301b375bcab4733e1badec", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 16, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "foraugmentedgeneration: Asurvey. arXivpreprintarXiv:2303.10868,2023.\n[41] PenghaoZhao,HailinZhang,QinhanYu,ZhengrenWang,YuntengGeng,FangchengFu,Ling\nYang,WentaoZhang,andBinCui. Retrieval-augmentedgenerationforai-generatedcontent: A\nsurvey. arXivpreprintarXiv:2402.19473,2024.\n[42] MichaelGünther,JackminOng,IsabelleMohr,AlaeddineAbdessalem,TanguyAbel,Moham-\nmadKalimAkram,SusanaGuzman,GeorgiosMastrapas,SabaSturua,BoWang,etal. Jina\nembeddings2:8192-tokengeneral-purposetextembeddingsforlongdocuments. arXivpreprint\narXiv:2310.19923,2023.", "metadata": {"id": "1b3397288ba9a631f4039016f4596a449c5470a2", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 16, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "arXiv:2310.19923,2023.\n[43] LlamaIndex. Llamaindexwebsite. https://www.llamaindex.com. Accessed: 2024-06-08.\n[44] KunalSawarkar, AbhilashaMangal, andShivamRajSolanki. Blendedrag: Improvingrag\n(retriever-augmentedgeneration)accuracywithsemanticsearchandhybridquery-basedretriev-\ners. arXivpreprintarXiv:2404.07220,2024.\n[45] GautierIzacard,MathildeCaron,LucasHosseini,SebastianRiedel,PiotrBojanowski,Armand\nJoulin,andEdouardGrave. Unsuperviseddenseinformationretrievalwithcontrastivelearning.\narXivpreprintarXiv:2112.09118,2021.", "metadata": {"id": "70dfc807974779fde83ef5031e545f1d286658c6", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 16, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "arXivpreprintarXiv:2112.09118,2021.\n[46] NandanThakur,NilsReimers,AndreasRücklé,AbhishekSrivastava,andIrynaGurevych. Beir:\nA heterogenous benchmark for zero-shot evaluation of information retrieval models. arXiv\npreprintarXiv:2104.08663,2021.\n[47] PayalBajaj,DanielCampos,NickCraswell,LiDeng,JianfengGao,XiaodongLiu,Rangan\nMajumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, et al. Ms marco: A human\ngeneratedmachinereadingcomprehensiondataset. arXivpreprintarXiv:1611.09268,2016.\n16", "metadata": {"id": "db48aaa22037f380c9dae7d9ec99c5c55fc4a883", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 16, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "[48] NelsonFLiu,KevinLin,JohnHewitt,AshwinParanjape,MicheleBevilacqua,FabioPetroni,\nandPercyLiang. Lostinthemiddle: Howlanguagemodelsuselongcontexts. Transactionsof\ntheAssociationforComputationalLinguistics,12:157–173,2024.\n[49] HuiqiangJiang,QianhuiWu,XufangLuo,DongshengLi,Chin-YewLin,YuqingYang,and\nLiliQiu. Longllmlingua: Acceleratingandenhancingllmsinlongcontextscenariosviaprompt\ncompression. arXivpreprintarXiv:2310.06839,2023.\n[50] HugoTouvron,LouisMartin,KevinR.Stone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,\nNikolayBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,DanielM.Bikel,Lukas", "metadata": {"id": "0db88b22842edc781de4fb3ae7cecf74841ce0f4", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 17, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "NikolayBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,DanielM.Bikel,Lukas\nBlecher,CristianCantónFerrer,MoyaChen,GuillemCucurull,DavidEsiobu,JudeFernandes,\nJeremyFu,WenyinFu,BrianFuller,CynthiaGao,VedanujGoswami,NamanGoyal,AnthonyS.\nHartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian\nKhabsa,IsabelM.Kloumann,A.V.Korenev,PunitSinghKoura,Marie-AnneLachaux,Thibaut\nLavril,JenyaLee,DianaLiskovich,YinghaiLu,YuningMao,XavierMartinet,TodorMihaylov,\nPushkarMishra,IgorMolybog,YixinNie,AndrewPoulton,JeremyReizenstein,RashiRungta,", "metadata": {"id": "e2af3916446aeae489b3d34b028ae57771ac43ab", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 17, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "PushkarMishra,IgorMolybog,YixinNie,AndrewPoulton,JeremyReizenstein,RashiRungta,\nKalyanSaladi,AlanSchelten,RuanSilva,EricMichaelSmith,R.Subramanian,XiaTan,Binh\nTang,RossTaylor,AdinaWilliams,JianXiangKuan,PuxinXu,ZhengxuYan,IliyanZarov,\nYuchenZhang,AngelaFan,MelanieKambadur,SharanNarang,AurelienRodriguez,Robert\nStojnic,SergeyEdunov,andThomasScialom. Llama2: Openfoundationandfine-tunedchat\nmodels. ArXiv,abs/2307.09288,2023.\n[51] ES Shahul, Jithin James, Luis Espinosa Anke, and Steven Schockaert. Ragas: Automated\nevaluationofretrievalaugmentedgeneration. InConferenceoftheEuropeanChapterofthe", "metadata": {"id": "e698ff563ade635ba3f13d86b4648a9dea82cad0", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 17, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "evaluationofretrievalaugmentedgeneration. InConferenceoftheEuropeanChapterofthe\nAssociationforComputationalLinguistics,2023. URLhttps://api.semanticscholar.org/\nCorpusID:263152733.\n[52] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Musique:\nMultihop questions via single-hop question composition. Transactions of the Association\nforComputationalLinguistics,page539–554,May2022. doi: 10.1162/tacl_a_00475. URL\nhttp://dx.doi.org/10.1162/tacl_a_00475.\n[53] Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi,", "metadata": {"id": "e8ec5c098b61a618f5c73523f6824e928761750c", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 17, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "[53] Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi,\nPatrickWendell,MateiZaharia,andReynoldXin. Freedolly: Introducingtheworld’sfirsttruly\nopeninstruction-tunedllm,2023. URLhttps://www.databricks.com/blog/2023/04/12/\ndolly-first-open-commercially-viable-instruction-tuned-llm.\n[54] NickCraswell,BhaskarMitra,EmineYilmaz,DanielFernandoCampos,andEllenM.Voorhees.\nOverviewofthetrec2019deeplearningtrack. ArXiv,abs/2003.07820,2020. URLhttps:\n//api.semanticscholar.org/CorpusID:253234683.", "metadata": {"id": "2343bd6923fb0afc3c96193bfedce25fe86a25af", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 17, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "//api.semanticscholar.org/CorpusID:253234683.\n[55] NickCraswell,BhaskarMitra,EmineYilmaz,DanielFernandoCampos,andEllenM.Voorhees.\nOverviewofthetrec2020deeplearningtrack. ArXiv,abs/2102.07662,2021. URLhttps:\n//api.semanticscholar.org/CorpusID:212737158.\n[56] JimmyLin,XueguangMa,Sheng-ChiehLin,Jheng-HongYang,RonakPradeep,andRodrigo\nNogueira. Pyserini: Apythontoolkitforreproducibleinformationretrievalresearchwithsparse\nanddenserepresentations. InProceedingsofthe44thInternationalACMSIGIRConferenceon\nResearchandDevelopmentinInformationRetrieval,pages2356–2362,2021.", "metadata": {"id": "f0d4aedd83b35bfa9b98ab7631881ceba3a78621", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 17, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "ResearchandDevelopmentinInformationRetrieval,pages2356–2362,2021.\n[57] TomKwiatkowski,JennimariaPalomaki,OliviaRedfield,MichaelCollins,AnkurP.Parikh,\nChrisAlberti,DanielleEpstein,IlliaPolosukhin,JacobDevlin,KentonLee,KristinaToutanova,\nLlionJones,MatthewKelcey,Ming-WeiChang,AndrewM.Dai,JakobUszkoreit,QuocV.Le,\nandSlavPetrov. Naturalquestions:Abenchmarkforquestionansweringresearch. Transactions\noftheAssociationforComputationalLinguistics,7:453–466,2019.\n[58] MandarJoshi,EunsolChoi,DanielS.Weld,andLukeZettlemoyer. Triviaqa: Alargescale", "metadata": {"id": "ad84bf2b52f38b00fa2a55c4e35e5af6dd42e827", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 17, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "[58] MandarJoshi,EunsolChoi,DanielS.Weld,andLukeZettlemoyer. Triviaqa: Alargescale\ndistantly supervised challenge dataset for reading comprehension. ArXiv, abs/1705.03551,\n2017.\n[59] ZhilinYang,PengQi,SaizhengZhang,YoshuaBengio,WilliamWCohen,RuslanSalakhut-\ndinov,andChristopherDManning. Hotpotqa: Adatasetfordiverse,explainablemulti-hop\nquestionanswering. arXivpreprintarXiv:1809.09600,2018.\n17", "metadata": {"id": "e9628427663e7a0d3075ce50e44160d3ecc006e5", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 17, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "[60] IvanStelmakh,YiLuan,BhuwanDhingra,andMing-WeiChang. Asqa: Factoidquestionsmeet\nlong-formanswers. ArXiv,abs/2204.06092,2022.\n[61] TomášKocˇisky`,JonathanSchwarz,PhilBlunsom,ChrisDyer,KarlMoritzHermann,Gábor\nMelis,andEdwardGrefenstette. Thenarrativeqareadingcomprehensionchallenge. Transac-\ntionsoftheAssociationforComputationalLinguistics,6:317–328,2018.\n[62] PranavRajpurkar,JianZhang,KonstantinLopyrev,andPercyLiang.Squad:100,000+questions\nformachinecomprehensionoftext. arXivpreprintarXiv:1606.05250,2016.\n[63] StephanieLin, JacobHilton, andOwainEvans. Truthfulqa: Measuringhowmodelsmimic", "metadata": {"id": "1d65e91a1afaa5e3024e3ec1c4c96af06e2137dc", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 18, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "[63] StephanieLin, JacobHilton, andOwainEvans. Truthfulqa: Measuringhowmodelsmimic\nhumanfalsehoods. arXivpreprintarXiv:2109.07958,2021.\n[64] J.EdwardHu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,and\nWeizhuChen. Lora: Low-rankadaptationoflargelanguagemodels. ArXiv,abs/2106.09685,\n2021.\n[65] DanHendrycks,CollinBurns,StevenBasart,AndyZou,MantasMazeika,DawnSong,and\nJacobSteinhardt. Measuringmassivemultitasklanguageunderstanding. CornellUniversity-\narXiv,CornellUniversity-arXiv,Sep2020.\n[66] PeterClark,IsaacCowhey,OrenEtzioni,TusharKhot,AshishSabharwal,CarissaSchoenick,", "metadata": {"id": "c02f07b695acea6165ae7d264df1542204edf8c4", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 18, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "[66] PeterClark,IsaacCowhey,OrenEtzioni,TusharKhot,AshishSabharwal,CarissaSchoenick,\nandOyvindTafjord. Thinkyouhavesolvedquestionanswering? tryarc,theai2reasoningchal-\nlenge. ArXiv,abs/1803.05457,2018. URLhttps://api.semanticscholar.org/CorpusID:\n3922816.\n[67] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor\nconduct electricity? a new dataset for open book question answering. In Proceedings of\nthe2018ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,Jan2018. doi:\n10.18653/v1/d18-1260. URLhttp://dx.doi.org/10.18653/v1/d18-1260.", "metadata": {"id": "dba93c0b4c596c2baa17c4d539aefd85f7e36fc3", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 18, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "10.18653/v1/d18-1260. URLhttp://dx.doi.org/10.18653/v1/d18-1260.\n[68] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. Fever: a\nlarge-scale dataset for fact extraction and verification. ArXiv, abs/1803.05355, 2018. URL\nhttps://api.semanticscholar.org/CorpusID:4711425.\n[69] Tianhua Zhang, Hongyin Luo, Yung-Sung Chuang, Wei Fang, Luc Gaitskell, Thomas\nHartvigsen,XixinWu,DannyFox,HelenM.Meng,andJamesR.Glass. Interpretableunified\nlanguage checking. ArXiv, abs/2304.03728, 2023. URL https://api.semanticscholar.\norg/CorpusID:258041307.", "metadata": {"id": "7e0974b4c807e48dcb2ac106c6c5d8370f2d6030", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 18, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "org/CorpusID:258041307.\n[70] JonathanBerant,AndrewChou,RoyFrostig,andPercyLiang. Semanticparsingonfreebase\nfromquestion-answerpairs. EmpiricalMethodsinNaturalLanguageProcessing,Empirical\nMethodsinNaturalLanguageProcessing,Oct2013.\n[71] Xanh Ho, A. Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing a multi-hop qa\ndatasetforcomprehensiveevaluationofreasoningsteps. ArXiv,abs/2011.01060,2020. URL\nhttps://api.semanticscholar.org/CorpusID:226236740.\n[72] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, NoahA. Smith, and Mike Lewis.", "metadata": {"id": "8964ec0f83e7305ba4110e04e01b3c51b1df4c5d", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 18, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "[72] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, NoahA. Smith, and Mike Lewis.\nMeasuringandnarrowingthecompositionalitygapinlanguagemodels. Oct2022.\n[73] QiaoJin,BhuwanDhingra,ZhengpingLiu,WilliamW.Cohen,andXinghuaLu. Pubmedqa: A\ndatasetforbiomedicalresearchquestionanswering. InConferenceonEmpiricalMethodsin\nNaturalLanguageProcessing,2019. URLhttps://api.semanticscholar.org/CorpusID:\n202572622.\n[74] AkariAsai,ZeqiuWu,YizhongWang,AvirupSil,andHannanehHajishirzi. Self-rag: Learning\ntoretrieve, generate, andcritiquethroughself-reflection. arXivpreprintarXiv:2310.11511,\n2023.\n18", "metadata": {"id": "cd97568a6e3f2d721435034d57d61d70921708fd", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 18, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "A ExperimentalDetails\nInthissection,weprovidedetailedexperimentalsettingsforeachmodule,coveringdatasetspecifics,\ntrainingparameters,andanyadditionalexperimentalresults.\nA.1 QueryClassification\nDatasets WeutilizedasubsetoftheDatabricks-Dolly-15K[53]andgeneratedadditionaldata\nusingGPT-4.TheprompttemplateforgeneratingquestionsisshowninTable14.\nImplementationDetails WechooseBERT-base-multilingual-casedasourclassifier,withabatch\nsizeof16andalearningrateof1e-5. TheevaluationofresultsisshowcasedinTable1.\nA.2 ExperimentalDetailsofRetrievalMethods", "metadata": {"id": "37a4b4f44f12626f5fb0d4927ca7a423d4c79aa9", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 19, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "A.2 ExperimentalDetailsofRetrievalMethods\nImplementationdetailsofthecomparativeexperimentsofdifferentretrievalmethodsareasbelow:\nDatasets WeusetheTRECDL2019[54]and2020[55]passagerankingdatasetstoevaluatethe\nperformanceofdifferentretrievalmethods.\nMetrics Widely-usedevaluationmetricsforretrievalincludemAP,nDCG@10,R@50andR@1k.\nBothmAPandnDCG@10areorder-awaremetricsthattaketherankingofsearchresultsintoaccount.\nIncontrast,R@kisanorder-unawaremetric. Wealsoreporttheaveragelatencyincurredbyeach\nmethodperquery.\nImplementationDetails Forsparseretrieval,weusetheBM25algorithm,whichreliesontheTF-", "metadata": {"id": "3fa7f7aa759616d9064331d6317812dd816424b4", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 19, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "methodperquery.\nImplementationDetails Forsparseretrieval,weusetheBM25algorithm,whichreliesontheTF-\nIDFalgorithm.Fordenseretrieval,weemployContrieverasourunsupervisedcontrastivetextencoder.\nBasedonourevaluationofembeddingmodels,weimplementoursuperviseddenseretrievalusing\nLLM-Embedder. WeusethedefaultimplementationofBM25andContrieverfromPyserini[56].\nTheBM25indexisconstructedusingLuceneonMSMARCOcollections,whilethedensevector\nindexisgeneratedwithFaissemployingFlatconfigurationonthesamedataset. Forqueryrewriting,\nwepromptZephyr-7b-alpha9,amodeltrainedtoactasahelpfulassistant,torewritetheoriginal", "metadata": {"id": "f35926214b977f62c98b78740aac01a0dee16707", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 19, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "wepromptZephyr-7b-alpha9,amodeltrainedtoactasahelpfulassistant,torewritetheoriginal\nquery. Forquerydecomposition,weemployGPT-3.5-turbo-0125tobreakdowntheoriginalquery\nintomultiplesub-queries. WecloselyfollowtheimplementationfromHyDE[10],utilizingthemore\nadvancedinstruction-followinglanguagemodel,GPT-3.5-turbo-instruct,togeneratehypothetical\nanswers. Themodelinferswithadefaulttemperatureof0.7, samplinguptoamaximumof512\ntokens. RetrievalexperimentsandevaluationareconductedusingthePyserinitoolkit.\nA.3 ExperimentalDetailsofRerankingMethods", "metadata": {"id": "e02970697891ad51969163af26b3df4dc9a32e15", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 19, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "A.3 ExperimentalDetailsofRerankingMethods\nDatasets OurexperimentsutilizetheMSMARCOPassagerankingdataset,asubstantialcorpus\ndesignedformachinereadingcomprehensiontasks. Thisdatasetcomprisesover8.8millionpassages\nand1millionqueries. Thetrainingsetcontainsapproximately398Mtuplesofqueriespairedwith\ncorrespondingpositiveandnegativepassages,whilethedevelopmentsetcomprises6,980queries,\npairedwiththeirBM25retrievalresults,andpreservesthetop-1000rankedcandidatepassagesfor\neachquery. Weevaluatetheeffectivenessofthemethodsonthedevelopmentset,asthetestsetisnot\npubliclyavailable.", "metadata": {"id": "661f37115a7e097e770cc1141f8ab26e78a08d19", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 19, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "publiclyavailable.\nMetrics The evaluation metrics MRR@1, MRR@10, MRR@1k and Hit Rate@10 are used.\nMRR@10istheofficialmetricproposedbyMSMARCO.\nImplementationDetails Wefollowandmakemodificationstotheimplementationprovidedby\nPyGaggle[26]andTILDE[28]. ForDLM-basedreranking,weusemonoT5[26]basedonT5-base,\nmonoBERT[25]basedonBERT-largeandRankLLaMA[27]basedonLlama-2-7b. ForTILDE\nreranking,weuseTILDEv2[29]basedonBERT-base.\nTypically,50documentsareretrievedasinputforthererankingmodule. Thedocumentsremaining\nafterthererankingandrepackingphasecanbefurtherconcentratedbyassigningatop-kvalueora", "metadata": {"id": "b102c547533e9bfcf2c470d2815f409d8eeabb5e", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 19, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "afterthererankingandrepackingphasecanbefurtherconcentratedbyassigningatop-kvalueora\nrelevancyscorethreshold.\nResultAnalysis RerankingresultsareshowninTable9. Wecompareourresultswitharandomly\nshuffledorderingandtheBM25retrievalbaseline. Allrerankingmethodsdemonstrateanotable\n9https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha\n19", "metadata": {"id": "36d04049cc8779cd4a50081dc7c20d4a48465e29", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 19, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "Context Model NQ TriviaQA HotpotQA ASQA Avg.\nM 29.78 60.44 23.73 37.89 37.96\nb\nM 26.23 58.26 26.67 32.30 35.87\ng\nD ∅ M r 31.10 61.37 28.40 39.96 40.21\nM 25.92 57.62 26.43 32.99 35.70\ngr\nM 26.69 58.07 27.04 33.75 36.39\ngg\nM 44.78 79.90 56.72 71.64 63.26\nb\nM 85.72 88.16 79.82 85.51 84.80\ng\nD g M r 60.98 80.20 65.73 67.49 68.60\nM 87.60 87.94 81.07 87.58 86.05\ngr\nM 86.72 88.35 79.59 83.44 84.53\ngg\nM 16.49 50.03 21.57 28.79 29.22\nb\nM 22.15 46.98 24.36 29.40 30.72\ng\nD r M r 36.92 58.42 29.64 39.54 41.13\nM 23.63 45.01 24.17 27.95 30.19\ngr\nM 21.08 43.83 23.23 27.33 28.87\ngg", "metadata": {"id": "ef6c89bd6e8985cb372077cba0f9db45ccb84497", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 20, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "M 23.63 45.01 24.17 27.95 30.19\ngr\nM 21.08 43.83 23.23 27.33 28.87\ngg\nM 34.65 81.27 52.75 65.42 58.52\nb\nM 85.00 87.33 78.18 83.02 83.38\ng\nD gr M r 60.28 79.32 63.82 67.29 67.68\nM 87.63 87.14 79.95 87.78 85.63\ngr\nM 86.31 86.90 78.10 83.85 83.79\ngg\nTable12: ResultsofthemodelaugmentedwithdifferentcontextsonvariousQAdatasets.\nincreaseinperformanceacrossallmetrics. ApproximatelyequalperformanceisachievedbymonoT5\nandmonoBERT,andRankLLaMAperformsbest,eachascendinginlatency. TILDEv2isthefastest,\ntaking approximately 10 to 20 milliseconds per query at the cost of performance. Additionally,", "metadata": {"id": "6497b115e6d678482e7fe5685d4138336d88593c", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 20, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "taking approximately 10 to 20 milliseconds per query at the cost of performance. Additionally,\nTILDEv2 requires that the passages reranked be identically included in the previously indexed\ncollection.Preprocessingmustberedoneatinferencefornewunseenpassages,negatingtheefficiency\nadvantages.\nA.4 ExperimentalDetailsofSummarizationMethods\nSelective Context Selective Context enhances LLM efficiency by identifying and removing\nredundantinformationintheinputcontext. Itevaluatestheinformativenessoflexicalunitsusing", "metadata": {"id": "853701a37b7c862ebfee218b7b38b9690f9ef71c", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 20, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "redundantinformationintheinputcontext. Itevaluatestheinformativenessoflexicalunitsusing\nself-information computed by a base causal language model. This method is non-query-based,\nallowingacomparisonbetweenquery-basedandnon-query-basedapproaches.\nDatasets We evaluated these methods on three datasets: Natural Questions (NQ) [57], Trivi-\naQA[58],andHotpotQA[59].\nMetrics EvaluationmetricsincludetheF1scoreandthenumberoftokenschangedaftersumma-\nrizationtomeasureconciseness.\nImplementation Details For all methods, we use Llama3-8B-Instruct as the generator model", "metadata": {"id": "024cea43696fda28344e66cc25616cc66a2c7ccb", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 20, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "Implementation Details For all methods, we use Llama3-8B-Instruct as the generator model\nand set a summarization ratio of 0.4. For extractive methods, importance scores determine the\nsentencesretained. Forabstractivemethods,wecontrolthemaximumgenerationlengthusingthe\nsummarizationratiotoalignwithextractivemethods. ExperimentsareconductedontheNQtestset,\nTriviaQAtestset,andHotpotQAdevelopmentset.\nA.5 ExperimentalDetailsofGeneratorFine-tuning\nDatasets Wefine-tuneourmodelonseveralquestionanswering(QA)andreadingcomprehension", "metadata": {"id": "8d1fec490fc457ee9d04ecdfabca1ff4744dea32", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 20, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "Datasets Wefine-tuneourmodelonseveralquestionanswering(QA)andreadingcomprehension\ndatasets,includingASQA[60],HotpotQA[59],NarrativeQA[61],NQ[57],SQuAD[62],Trivi-\naQA[58],TruthfulQA[63]. Weusetheirtrainsplits(forthosecontainingsignificantlymoredata\n20", "metadata": {"id": "44d95cd3db97e04a64a73e17ebebe7a8af6f4652", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 20, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "[Instruction] Pleasegeneratetendescriptionsforthecontinuationtask.\n[Context] Forexample:\n1.“French.WashingtonplayedacrucialroleintheAmericanRevolutionaryWar,leading\ntheContinentalArmyagainsttheBritish.”Pleasecontinuewritingtheaboveparagraph.\n2.“ThediscoveryofthedoublehelixstructureofDNAbyJamesWatsonandFrancis\nCrickrevolutionizedthefieldofgenetics,layingthefoundationformodernmolecular\nbiology and biotechnology.” Please continue by discussing recent developments in\ngeneticresearch,suchasCRISPRgeneediting,andtheirpotentialethicalimplications.\nTable14: Templateforgeneratingtaskclassificationdata.", "metadata": {"id": "88e5b1e4eb8605162e99fdc97e3564b3c06e72f9", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 21, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "Table14: Templateforgeneratingtaskclassificationdata.\nentriesthanothers,weconductedarandomsample). Forevaluation,ASQA[60],HotpotQA[59],\nNQ[57],TriviaQA[58]areused. Weevaluateourmodelontheirvalidationsplitsormanuallysplita\nsubsetfromthetrainingsettoavoidoverlapping.\nThe exact number of entries in each train and Dataset #Train #Eval\ntestsetisdetailedinTable13. ASQA 2,090 483\nHotpotQA 15,000 7,405\nWeusethedataset-provideddocumentsasd\ngold TriviaQA 9,000 6,368\nforeachdataentry. Toobtaind wesam-\nrandom NQ 15,000 8,006\nple the context of different entries within the\nNarrativeQA 7,000 −−", "metadata": {"id": "7357a7cf149f1f13dc0b93cc593f4c42b7b182be", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 21, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "random NQ 15,000 8,006\nple the context of different entries within the\nNarrativeQA 7,000 −−\nsamedataset,tomakesurethedistributionsof SQuAD 67,00 −−\nd random andd gold areroughlysimilar. TruthfulQA 817 −−\nMetrics We use the ground-truth coverage\nTable 13: Number of examples in each Dataset\nas our evaluation metric, considering that the\nusedinthefine-tuningexperiments.\nanswersofQAtasksarerelativelyshort,while\nthegenerationlengthofthemodelissometimeshardtolimit.\nImplementationDetails WeselectLlama-2-7b[50]asthebasemodel. Forefficiency,weuse", "metadata": {"id": "9bb70f61368bc1e09678541dac3cdb0c85370207", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 21, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "ImplementationDetails WeselectLlama-2-7b[50]asthebasemodel. Forefficiency,weuse\nLoRA[64]andint8quantizationduringtraining. Theprompttemplatesusedforfine-tuningand\nevaluation mainly follow Lin et al. [37]. We train our generator for 3 epochs and constrain the\nmaximumlengthofthesequenceto1600,usingabatchsizeof4andalearningrateof5e-5. During\ntesting,weuseazero-shotsetting.\nDetailedResults Table12showsourevaluationresultsoneachdataset.\nA.6 ExperimentalDetailsofComprehensiveEvaluation\nTasksandDatasets WeconductedextensiveexperimentsacrossvariousNLPtasksanddatasetsto", "metadata": {"id": "2e1bd4cd68de3a1333c19f80058a468680bc1b86", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 21, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "TasksandDatasets WeconductedextensiveexperimentsacrossvariousNLPtasksanddatasetsto\nassesstheperformanceofRAGsystems. Specifically: (1)CommonsenseReasoning: Weevaluated\nonMMLU[65],ARC-Challenge[66],andOpenbookQA[67]datasets. (2)FactChecking: Our\nevaluation encompassed the FEVER [68] and PubHealth [69] datasets. (3) Open-Domain QA:\nWe assessed on NQ [57], TriviaQA [58], and WebQuestions [70] datasets. (4) MultiHop QA:\nOurevaluationincludedtheHotPotQA[59],2WikiMultiHopQA[71],andMuSiQue[52]datasets.\nForMuSiQue,wefollowedtheapproachoutlinedin[72]andfocusedsolelyonanswerable2-hop", "metadata": {"id": "cf18ba6cb8f5dda63690bd63df4bcbe6e892b804", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 21, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "ForMuSiQue,wefollowedtheapproachoutlinedin[72]andfocusedsolelyonanswerable2-hop\nquestions. (5)MedicalQA:WealsoassessedonthePubMedQA[73]dataset. Ineachdataset,we\nrandomlysub-sample500entriesfromthetestsetforourexperiments. Fordatasetswithouttestset,\nweusedevelopsetinstead.\nToassessRAGcapabilities,weevenlycollectatotalof500entriesfromNQ,TriviaQA,HotPotQA,\n2WikiMultiHopQAandMuSiQue. Eachentryisa“question,golddocument,goldanswer”triple.\nMetrics Weusetoken-levelF1scoreandEMscoreforOpen-DomainQAandMultiHopQAtasks,\nandaccuracyforothers. WeuseamorelenientEMscore,whichevaluatesperformancebasedon", "metadata": {"id": "fd7f1544ba2af34da29f0fd7525a471fadf17da1", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 21, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "andaccuracyforothers. WeuseamorelenientEMscore,whichevaluatesperformancebasedon\nwhetherthemodelgenerationsincludegoldanswersinsteadofstrictlyexactmatching[74].\nTowardsRAGcapabilitiesevaluation,weadoptfourmetricsfromRAGAs,includingFaithfulness,\nContextRelevancy, AnswerRelevancy, andAnswerCorrectness. Faithfulnessmeasureshow\nfactually consistent the generated answer is with the retrieved context. An answer is considered\nfaithfulifallclaimsmadecanbedirectlyinferredfromtheprovidedcontext. ContextRelevancy\nevaluateshowrelevanttheretrievedcontextistotheoriginalquery. AnswerRelevancyassessesthe\n21", "metadata": {"id": "686c4a412db28d345092e185af00bf5edb516304", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 21, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "pertinenceofthegeneratedanswertotheoriginalquery. AnswerCorrectnessinvolvestheaccuracy\nofthegeneratedanswerwhencomparedtothegroundtruth. Forexample,ContextRelevancyis\ncalculatedfromtheproportionofsentenceswithintheretrievedcontextthatarerelevantforanswering\nthegivenquestiontoallsentences:\n|S|\ncontextrelevancy = (2)\n|Total|\nwhere|S|denotesthenumberofrelevantsentences,|Total|denotesthetotalnumberofsentences\nretrieved. AllthesemetricsareevaluatedusingtheRAGAsframework,withGPT-4servingasthe\njudge.\nAdditionally,wecomputethecosinesimilaritybetweentheretrieveddocumentandthegolddocument", "metadata": {"id": "b5e4f91bd568e90688092aba9230febf238cfd5b", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 22, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "judge.\nAdditionally,wecomputethecosinesimilaritybetweentheretrieveddocumentandthegolddocument\nas Retrieval Similarity. The retrieved document and gold document are fed into an embedding\nmodel,thentheresultingembeddingsareusedtocomputethecosinesimilarity.\nImplementationDetails ForOpen-DomainQAandMultiHopQAdatasets,wesetthegeneration\nmodel’smaximumnewtokennumberto100tokens. Forotherdatasets,wesetitto50tokens. To\ndealwithexcessivelylongretrieveddocuments,wetruncatedthedocumentsto2048wordswhen\nevaluatingRankLLaMAandLongLLMLingua.", "metadata": {"id": "02e19c220e08de0b10565ebeca3e7f7126fbf78d", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 22, "created_at": "2025-09-10T20:16:47Z"}}
{"content": "evaluatingRankLLaMAandLongLLMLingua.\nForalldatasets,weusegreedydecodingduringgeneration. Tobettercomparethecapabilitiesof\ndifferent RAG modules, we adopt the 0-shot evaluation setting, i.e., no in-context examples are\noffered. Inthemultiplechoiceandfactcheckingtasks,answersgeneratedbythemodelmaytake\navarietyofforms(e.g.,“theanswerisA”insteadof“A”).Therefore,wepreprocesstheresponses\ngeneratedbythemodel,applyingregularexpressiontemplatestomatchthemwithgoldlabels.\n22", "metadata": {"id": "299089ccc2f2075b53c3c1e56bc690c4727cd6c3", "source": "Searching for Best Practices in Retrieval-Augmented (4).pdf", "page": 22, "created_at": "2025-09-10T20:16:47Z"}}
