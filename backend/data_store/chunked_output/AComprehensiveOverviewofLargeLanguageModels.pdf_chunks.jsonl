{"content": "See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/372341712\nA Comprehensive Overview of Large Language Models\nPreprint · July 2023\nDOI: 10.48550/arXiv.2307.06435\nCITATIONS READS\n117 17,753\n8 authors, including:\nHumza Naveed Ayesha Atta\nUniversity of Engineering and Technology Lahore Namal College", "metadata": {"id": "0c553ee034c0b3e27d2a6347c4ef87d0d7f6d858", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 1, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "Humza Naveed Ayesha Atta\nUniversity of Engineering and Technology Lahore Namal College\n9 PUBLICATIONS 617 CITATIONS 11 PUBLICATIONS 520 CITATIONS\nSEE PROFILE SEE PROFILE\nMuhammad Saqib Saeed Anwar\nUniversity of Technology Sydney Australian National University\n30 PUBLICATIONS 1,167 CITATIONS 147 PUBLICATIONS 8,281 CITATIONS\nSEE PROFILE SEE PROFILE", "metadata": {"id": "cda4612c092945112a2ec1d3ba82c543cd0a5f03", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 1, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "30 PUBLICATIONS 1,167 CITATIONS 147 PUBLICATIONS 8,281 CITATIONS\nSEE PROFILE SEE PROFILE\nAll content following this page was uploaded by Humza Naveed on 14 September 2023.\nThe user has requested enhancement of the downloaded file.", "metadata": {"id": "a58879570c848d8654c1708aed59408242a8bc06", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 1, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "JOURNALOFLATEX 1\nA Comprehensive Overview of Large Language\nModels\nHumza Naveed, Asad Ullah Khan*, Shi Qiu*, Muhammad Saqib*,\nSaeed Anwar, Muhammad Usman, Naveed Akhtar, Nick Barnes, Ajmal Mian\nAbstract—\nLarge Language Models (LLMs) have recently demonstrated\nremarkablecapabilitiesinnaturallanguageprocessingtasksand\nbeyond. This success of LLMs has led to a large influx of", "metadata": {"id": "ae5dc51af2e908582b3e0a3503d543fd249d8b71", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 2, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "beyond. This success of LLMs has led to a large influx of\nresearch contributions in this direction. These works encompass\ndiversetopicssuchasarchitecturalinnovationsoftheunderlying\nneuralnetworks,contextlengthimprovements,modelalignment,\ntraining datasets, benchmarking, efficiency and more. With the\nrapid development of techniques and regular breakthroughs in", "metadata": {"id": "03fa800e5700924247c46b30165264b72ec4b8fb", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 2, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "rapid development of techniques and regular breakthroughs in\nLLMresearch,ithasbecomeconsiderablychallengingtoperceive\nthe bigger picture of the advances in this direction. Considering\nthe rapidly emerging plethora of literature on LLMs, it is\nimperativethattheresearchcommunityisabletobenefitfroma\nconcise yet comprehensive overview of the recent developments", "metadata": {"id": "58daed387939607d55ef412fd881c66ec46fbb84", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 2, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "concise yet comprehensive overview of the recent developments\nin this field. This article provides that overview to the research\ncommunity. It not only focuses on a systematic treatment of the\nexistingliteratureonabroadrangeofLLMrelatedconcept,but\nalsopaysspecialattentiontoprovidingcomprehensivesummaries Fig. 1: The trends in the number of LLM models introduced", "metadata": {"id": "c9d81b956d35ca78e2501160a4fd3ac2fed44970", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 2, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "with extensive details about the individual existing models,\nover the years.\ndatasets and major insights. We also pay heed to aligning our\noverview with the emerging outlook of this research direction\nby accounting for the other recently materializing reviews of\nthe broader research direction of LLMs. Our self-contained neuralarchitecturesliketransformers,increasedcomputational", "metadata": {"id": "f310e43d9d9323085fc7c98f739bba64a58d1e33", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 2, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "comprehensiveoverviewofLLMsdiscussesrelevantbackground\ncapabilities, and the accessibility of training data extracted\nconcepts along with covering the advanced topics at the frontier\nfrom the internet [2]. These developments have brought about\nof this research direction. This review article is intended to not", "metadata": {"id": "d216c6866323dd75914b79e58384cb68b7d1df21", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 2, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "of this research direction. This review article is intended to not\nonlyprovideasystematicsurvey,butalsoaquickcomprehensive a revolutionary transformation by enabling the creation of\nreference for the researchers and practitioners to draw insights LargeLanguageModels(LLMs)thatcanapproximatehuman-", "metadata": {"id": "77afc0a39e7702c260b90884f62087af99db3ec5", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 2, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "from extensive informative summaries of the existing works to level performance on certain evaluation benchmarks [3], [4].\nadvance the LLM research direction.\nLLMs, particularly pre-trained language models (PLM),\nIndex Terms— haveshowntremendousgeneralizationabilitiesfortextunder-\nLarge Language Models, LLMs, chatGPT, LLM training,\nstandingandgenerationtaskswhiletrainedinaself-supervised", "metadata": {"id": "9bd26a54cfa77a90e6031b4816b07f09b9ae01c8", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 2, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "standingandgenerationtaskswhiletrainedinaself-supervised\nLLM Benchmarking\nsettingonalargecorpusoftext[5],[6],[7].Theperformance\nofpre-trainedlanguagemodels(PLMs)improvessignificantly\nI. INTRODUCTION when fine-tuned for downstream tasks, surpassing the perfor-\nmanceofmodelstrainedfromscratch.Thesecharacteristicsof\nLanguage plays a fundamental role in facilitating commu-", "metadata": {"id": "ff3604e23049130574a9527fcd53b8e1638f388a", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 2, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "Language plays a fundamental role in facilitating commu-\nlanguagemodelsmotivatedresearcherstotrainlargerPLMson\nnication and self-expression for humans, and likewise, com-\neven bigger datasets and found that scaling model and dataset\nmunication holds paramount importance for machines in their\nsize further improve the generalization abilities.", "metadata": {"id": "1be79ed02c3d2e2ea48f13ab861f522ed61f98df", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 2, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "size further improve the generalization abilities.\ninteractions with humans and other systems. Large Language\nModels (LLMs) have emerged as cutting-edge artificial intel- NowmodernLLMsarecapableofperformingvarioustasks\nligence systems designed to process and generate text, aiming like code generation, text generation, tool manipulation, rea-", "metadata": {"id": "288efa5d95a21a67bb685e2aebd7cb73be37c315", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 2, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "to communicate coherently [1]. The need for LLMs stems soning, and understanding in zero-shot and few-shot settings\nfromthegrowingdemandformachinestohandlecomplexlan- in diverse domains, even without requiring any fine-tuning\nguagetasks,includingtranslation,summarization,information on downstream tasks [8], [9], [10]. Such generalization was", "metadata": {"id": "e396344f181804f20210dbfe5ae69b6e843cc1d6", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 2, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "retrieval, and conversational interactions. Recently, significant previouslyunattainablewithsmallermodels,markingasignif-\nbreakthroughs have been witnessed in language models, pri- icant advancement in language modeling. This development\nmarilyattributedtodeeplearningtechniques,advancementsin has sparked enthusiasm and excitement within the research", "metadata": {"id": "ab7ac0b29524399755c260fe883c6cf84ef9344c", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 2, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "community for the enhancement of LLM architectures and\nVersion:01(updateonJuly10,2023). training strategies, leading to the development of numerous\nGitHublink:https://github.com/humza909/LLM_Survey.git LLMs [11], [12], [13], [8], [9], [10], [14].\n*isforequalcontribution.\nContacte-mail:humza_naveed@yahoo.com The graph presented in Fig 1 depicts an increasing trend\n3202\npeS\n31\n]LC.sc[", "metadata": {"id": "418f1d770a4d722e3604c480928b83a2711e93d4", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 2, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "3202\npeS\n31\n]LC.sc[\n3v53460.7032:viXra", "metadata": {"id": "bfe7358cf0b77a7b0bd5650ae52d415eef5b086f", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 2, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "JOURNALOFLATEX 2\nAug Code LlaMA\nJul LLaMA 2\nJun WizardCoder\nMPT\nGoat\nDec OPT-IML CodeT5+\nmT0 StarCoder\nGalactica May Xuan Yuan 2.0\nOct GLM Koala\nOPT WizardLM\nMay UL2 Apr HuaTuo\nT0 Tk-Instruct Vicuna\nJun CPM-2 Apr GPT-NeoX-20B Alpaca\nOct T5 Oct mT5 Apr PanGu-α Mar CodeGen Feb LLaMA\n2019 2020 2021 2022 2023 2024\nMay GPT-3 Jul Codex Jan MT-NLG Mar PanGu-Σ Jan\nERNIE 3.0 Feb AlphaCode BloombergGPT", "metadata": {"id": "14e26043bb3a00e1ac9d4b6cc89c1e2bf44d865d", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 3, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "May GPT-3 Jul Codex Jan MT-NLG Mar PanGu-Σ Jan\nERNIE 3.0 Feb AlphaCode BloombergGPT\nAug Jurassic-1 Chinchilla GPT-4\nSep HyperCLOVA PaLM Claude\nOct Yuan 1.0 Aug AlexaTM Bard\nDec Gopher Sep Sparrow\nERNIE 3.0 Titan U-PaLM\nGLaM Flan-U-PaLM\nLaMDA Nov BLOOM\nWebGPT ChatGPT\nFig. 2: Chronological display of LLM releases: light blue rectangles represent ’pre-trained’ models, while dark rectangles", "metadata": {"id": "380c59cc231a95edea13fc4ad5d61eef803c9e9c", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 3, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "correspond to ’instruction-tuned’ models. Models on the upper half signify open-source availability, whereas those on the\nbottom half are closed-source. The chart illustrates the increasing trend towards instruction-tuned models and open-source\nmodels, highlighting the evolving landscape and trends in natural language processing research.", "metadata": {"id": "4f190efaa10e575d8c312733e31707eec80d3dc2", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 3, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "models, highlighting the evolving landscape and trends in natural language processing research.\nin the number of released LLMs, including open-source and datasets have been curated for instruction fine-tuning. These\nclosed-source models, over the years. Furthermore, Fig 2 datasetsincludemoreinstancesandtasksthatfurtherimprove", "metadata": {"id": "185c8b1d81fabd24a6db53e44b1d65be7da7bcf5", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 3, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "highlights the names of significant releases of various LLMs the performance over baselines [24], [23], [25], [26]. When\nand Fig 3 provides a broader overview of LLMs. performing instruction fine-tuning, all the model parameters\nDuring the early days of Large Language Models (LLMs), need to be updated. However, parameter-efficient fine-tuning", "metadata": {"id": "aeeb9c1f58551b3ace8539e27426e0f9ed4a4bdd", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 3, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "many research efforts focused on developing models for takes a different approach by updating only a small number\ntransfer learning to downstream tasks [11], [12], [15] until of parameters while still maintaining good performance. This\nthe emergence of models like GPT-3 [8], which demonstrated method keeps the original model frozen and adds a few extra", "metadata": {"id": "92d1336bcb89a0eec0400e2e8adc03d388b38632", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 3, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "impressive performance even without fine-tuning. Due to the parameters at different locations within the model [27], [28],\nclosed-source nature of GPT-3, there was a demand for open- [29], [30], [31]. This approach helps achieve efficient fine-\nsource alternatives, leading to the development of various tuning while minimizing the impact on the model’s overall", "metadata": {"id": "9a280955d36022e9d524fd1f93261f62f51111c8", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 3, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "models [9], [10] operating at the scale of GPT-3 and trained performance.\non extensive web-based datasets [16], [17], [18], [19]. Subse- Due to the success of LLMs on a wide variety of tasks, the\nquently,researchersproposedseveralarchitecturaldesignsand research literature has recently experienced a large influx of", "metadata": {"id": "764ec7ca78eb1fbb05cb9467e92073bb078f251c", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 3, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "trainingstrategiesthatshowedsuperiorperformancecompared LLMrelatedcontributions.Naturally,theresearchcommunity\nto GPT-3 across various tasks [15], [14], [20], [21]. has started the effort of organizing this literature as survey\nThe performance of LLMs improves further with instruc- articles. For instance, Zhou et al. [32] presented an overview", "metadata": {"id": "bfe2f6cbe3ea2ef43436ba243b520026ce1156d6", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 3, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "tion fine-tuning, outperforming pre-trained LLMs on various of the foundation models. An impressive effort is recently\nbenchmarks [22], [23]. Instruction fine-tuning of LLMs refers made by Zhou et al. [33] in their survey that also discusses\nto a specific training approach by incorporating additional aspects related to model architectures, fine-tuning, emergent", "metadata": {"id": "fe12e1dc443d1bdff3cc092b5395a5b8fa2086a9", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 3, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "prompts or instructions during the fine-tuning phase to guide abilities, and more. Another recent survey on augmented lan-\nthe output and thus enable the users to have more fine- guage models provides a historical account of the foundation\ngrained control over the outputs of LLMs. These prompts can models [34]. In contrast to these surveys, our contribution", "metadata": {"id": "43b9d28d61642f3a88d90f95825f79eadd315767", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 3, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "be natural language instructions or example demonstrations focuses on providing a comprehensive yet concise overview\nbased on the task’s requirement. In the literature, different of the general direction of LLM research. On one hand, this", "metadata": {"id": "b17288d437493219c42b813a310549e0133a2f63", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 3, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "JOURNALOFLATEX 3\nFig. 3: A broader overview of LLMs, dividing LLMs into five branches: 1. Training 2. Inference 3. Evaluation 4. Applications\n5. Challenges\narticle summarizes more details of the individual models as detailed discussion on the key design and deployment\ncompared to the existing efforts. On the other, it also covers aspects of LLMs to help practitioners to effectively", "metadata": {"id": "575287ea3fc108d2eaec9f83665e6cba8dcf62c2", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 4, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "more models in providing their summaries. It also delves leverage this technology.\ninto the details of model development, architectures, training It is noteworthy that although this article is the first contri-\ndatasets,andotherrelatedconceptstoprovideaself-contained bution in its own right in terms of providing a concise yet", "metadata": {"id": "26e11a0094ee5273dc690a5437c64d04beda60ef", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 4, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "comprehensive overview of this direction. Hence, this article comprehensive overview of LLMs, our work complements\naddressesanimportantgapofprovidingaconciseyetcompre- the recent (and emerging) surveys of this direction, e.g.,\nhensive overview of the rapidly developing general direction [33], [32]. Infrequently, we also loosely follow the existing", "metadata": {"id": "cd6b06b5ac9f1c2cf2e4bbd437c5ef7e02570998", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 4, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "of LLM research. Our key contributions are summarized as terminologiestoensureprovidingamorestandardizedoutlook\nfollows. of this research direction. For instance, following [33], our\n• We present the first survey on the developments in LLM survey considers a language model to be large if it has 10B", "metadata": {"id": "cbeccbaf17dbf63186113a06788da488aeacd2a2", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 4, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "research with the specific aim of providing concise yet parameters or more. Hence, we discuss such models in detail\ncomprehensive overview of the direction. We present inthissurvey.Wereferthereadersinterestedinsmallermodels\nextensive summaries that include fine-grained details of to [35], [36], [32].\nthe reviewed contributions. The organization of this paper is as follows. Section II dis-", "metadata": {"id": "70d4d0da8fe9a274858271ccfde8e05d4bc494d4", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 4, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "the reviewed contributions. The organization of this paper is as follows. Section II dis-\n• Inthisself-containedarticle,wecoverarangeofconcepts cussesthebackgroundofLLMs.SectionIIIfocusesonLLMs\nto comprehend the general direction of LLMs, including overview, architectures, and training pipelines and strategies.", "metadata": {"id": "18fcce761c15dd8416f36dbc69fb4b5f9f06f100", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 4, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "background concepts, popular models, crucial discover- Section IV presents the key findings derived from each LLM.\nies, related datasets and evaluation details etc. Section V highlights the configuration and parameters that\n• Besides paying special attention to the chronological play a crucial role in the functioning of these models. The", "metadata": {"id": "c6a0295e10deeac2e124bcf3180226aea8fe189a", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 4, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "orderofLLMsthroughoutthearticle,wealsosummarize LLMtrainingandevaluationbenchmarksarediscussedinsec-\nmajor findings of the popular contributions, and provide tion VI, followed by concluding remarks and future direction", "metadata": {"id": "f32f5be3d3cf4ea2f3d3ad50980840d3e33378cc", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 4, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "JOURNALOFLATEX 4\nin the conclusion section. 1. Self-Attention [44]: The self-attention is also known as\nintra-attentionsinceallthequeries,keysandvaluescomefrom\nII. BACKGROUND\nthe same block (encoder or decoder). The self-attention layer\nWe provide the relevant background to understand the key connects all the sequence positions to each other with O(1)", "metadata": {"id": "2f2aeb730feadb1feef7161bcaf8f6097e70b271", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 5, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "concepts related to LLM in this section. Aligned with our space complexity which is highly desirable for learning long-\nobjective of providing a comprehensive overview of this di- range dependencies in the input.\nrection,thissectionoffersacomprehensiveyetconciseoutline 2. Cross Attention: In encoder-decoder architectures, the", "metadata": {"id": "9de6202684f891283ed6b15472411c45c7fa4043", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 5, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "of the fundamental concepts. In natural language processing outputs of the encoder blocks act as the queries to the\nliterature, these concepts are of standard nature. Hence, we intermediaterepresentationofthedecoder,whichprovidesthe\nfocus more on the intuitive aspects and refer the readers keys and values to calculate a representation of the decoder", "metadata": {"id": "642d069a1fa37c0f788cd0d76f79c05507be9242", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 5, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "interested in details to the original works we cite in our conditioned on the encoder. This attention is called cross-\ndiscussion. attention.\n3. Full Attention: The naive implementation of calculating\nA. Tokenization\nself-attention is known as full attention.\nLLMs are trained on text to predict text, and similar to 4. Sparse Attention [45]: The self-attention has a time", "metadata": {"id": "3fb516b354c370cf20f0fd39f0d9f67f66e3b439", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 5, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "other natural language processing systems, they use tokeniza- complexityofO(n2),whichbecomesprohibitivewhenscaling\ntion [37] as the essential preprocessing step. It aims to parse the LLMs to large context windows. An approximation to the\nthe text into non-decomposing units called tokens. Tokens self-attention was proposed in [45], which greatly enhanced", "metadata": {"id": "d8798dc55dd7027d8442af8cc9a7a6bece5b04f7", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 5, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "can be characters, subwords [38], symbols [39], or words, the capacity of GPT series LLMs to process a greater number\ndepending on the size and type of the model. Some of the of input tokens in a reasonable time.\ncommonly used tokenization schemes in LLMs are briefed 5. Flash Attention [46]: The bottleneck for calculating the", "metadata": {"id": "bef0dbcb75ff71a5a7193c10ca3ae5d47d57edb7", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 5, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "here. Readers are encouraged to refer to [40] for a detailed attentionusingGPUsliesinthememoryaccessratherthanthe\nsurvey. computational speed. Flash Attention uses the classical input\n1. WordPiece[41]: Itwasintroducedin[41]asanoveltext tiling approach in order to process the blocks of the input", "metadata": {"id": "697f90fe78c704ed527f61f1467e845014fa8d8e", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 5, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "segmentation technique for Japanese and Korean languages to in GPU on-chip SRAM rather than doing IO for every token\nimprove the language model for voice search systems. Word- from the High Bandwith Memory (HBM). An extension of\nPiece selects tokens that increase the likelihood of an n-gram- thisapproachtosparseattentionfollowsthespeedgainsofthe", "metadata": {"id": "a089ae52528ab34ce53af5fa94354726ba6fd4fb", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 5, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "based language model trained on the vocabulary composed of full attention implementation. This trick allows even greater\ntokens. context-length windows in the LLMs as compared to those\n2. BPE [39]: Byte Pair Encoding (BPE) has its origin in LLMs with sparse attention.\ncompressionalgorithms.Itisaniterativeprocessofgenerating\ntokens where pairs of adjacent symbols are replaced by a new", "metadata": {"id": "fb60cf35174e2c2be9956f24ec010b6b169cd37e", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 5, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "tokens where pairs of adjacent symbols are replaced by a new\nD. Encoding Positions\nsymbol,andtheoccurrencesofthemostoccurringsymbolsin\nthe input text are merged together. Theattentionmodulesdonotconsidertheorderofprocess-\n3. UnigramLM[38]: Inthistokenization,asimpleunigram ingbydesign.Transformer[44]introduced“positionalencod-", "metadata": {"id": "92a4746773cde427ebda2dc831adb7ca8a36967b", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 5, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "language model (LM) is trained using an initial vocabulary ings” to feed information about the position of the tokens in\nof subword units. The vocabulary is pruned iteratively by input sequences. Several variants of positional encoding have\nremovingthelowest-probabilityitemsfromthelist,whichare been proposed [47], [48]. Interestingly, a recent study [49]", "metadata": {"id": "240fe1c9741c8c5a2bf10d170388f5fcd82dde4a", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 5, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "the worst performing on the unigram LM. suggests that adding this information may not matter for the\nstate-of-the-art decoder-only Transformers.\nB. Attention\n1. Absolute: This is the most straightforward approach to\nAttention, particularly selective attention, has been widely adding the sequence order information by assigning a unique", "metadata": {"id": "01a18b62f2bb072ffa104020170cccc23404a6a7", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 5, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "studied under perception, psychophysics and psychology. Se- identifier to each position of the sequence before passing it to\nlective attention can be conceived as “the programming by the attention module.\nthe O of which stimuli will be processed or encoded and in 2. Relative: In order to pass the information of the rel-", "metadata": {"id": "a97462e7c91063d58b0e99b4ad4db55af1f8444b", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 5, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "what order this will occur” [42]. While this definition has its ative dependencies of different tokens appearing at different\nroots in visual perception, it has uncanny similarities with the locations in the sequence, a relative positional encoding is\nrecently formulated attention [43], [44] (which stimuli will calculated by some kind of learning. Two famous types of", "metadata": {"id": "8fa2de2c1f01a5895f9c9f7b3bf0b5312b9e8cb4", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 5, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "be processed) and positional encoding (in what order this relative encodings are:\nwill occur) [44] in LLMs. We discuss both in sections II-C Alibi[47]Inthisapproach,ascalarbiasissubtractedfromthe\nand II-D, respectively. attention score calculated using two tokens which increases\nwith the distance between the positions of the tokens. This", "metadata": {"id": "b9aa799b3aea724e13477b832f70ebd9cee54ef5", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 5, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "with the distance between the positions of the tokens. This\nC. Attention in LLMs learned approach effectively favors using recent tokens for\nThe attention mechanism computes a representation of the attention.\ninputsequencesbyrelatingdifferentpositions(tokens)ofthese RoPE Keys, queries and values are all vectors in the LLMs.", "metadata": {"id": "6d0ac40058e949674efa6351df81f9c3db86bf22", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 5, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "sequences. There are various approaches to calculating and RoPE[48]involvestherotationofthequeryandkeyrepresen-\nimplementing attention, out of which some famous types are tations at an angle proportional to their absolute positions of\ngiven below. thetokensintheinputsequence.Thisstepresultsinarelative", "metadata": {"id": "5d77b0dd24ff5452e0e4ae17f1ca6ba8f9f41f84", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 5, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "JOURNALOFLATEX 5\npositional encoding scheme which decays with the distance where gl is the gain parameter. RMSNorm [57] modifies al\ni i\nbetween the tokens. as\nE. Activation Functions (cid:118)\nThe activation functions serve a crucial role in the curve- al = al i gl, whereRMS(al)= (cid:117) (cid:117) (cid:116) 1 (cid:88) n (al)2. (5)\ni RMS(al) i n i", "metadata": {"id": "6c5466feaa9c3de9a379bb50ade7f049a2d75ac7", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 6, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "i RMS(al) i n i\nfitting abilities of the neural networks, as proved in [50]. The i\nmodern activation functions used in LLMs are different from\n3. Pre-Norm and Post-Norm: LLMs use transformer [44]\nthe earlier squashing functions but are critical to the success\narchitecture with some variations. The original implementa-\nofLLMs.Wediscusstheseactivationfunctionsinthissection.", "metadata": {"id": "1b041eba39ea91db12422156b55173ff7c5d7cdb", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 6, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "ofLLMs.Wediscusstheseactivationfunctionsinthissection.\ntion [44] used layer normalization after the residual con-\n1. ReLU [51]: Rectified linear unit (ReLU) is defined as\nnection, commonly called post-LN, concerning the order of\nReLU(x)=max(0,x) (1) Multihead attention – Residual – LN. There is another order\nof the normalization, referred to as pre-LN [58] due to the", "metadata": {"id": "9d76282e4c5bad38e7dbe187e89c7f88fb50c8f7", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 6, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "of the normalization, referred to as pre-LN [58] due to the\n2. GeLU [52]: Gaussian Error Linear Unit (GeLU) is the\nposition of the normalization step before the self-attention\ncombinationofReLU,dropout[53]andzoneout[54].Itisthe\nlayer as in LN – Multihead attention – Residual. Pre-LN is\nmost widely used activation function in contemporary LLM\nknown to provide more stability in the training [59].", "metadata": {"id": "acc8e5933ac81093953a74cbbdf4b05dc3b768d3", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 6, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "known to provide more stability in the training [59].\nliterature.\n4. DeepNorm: Whilepre-LNhascertainbenefitsoverpost-\n3. GLU variants [55]: Gated Linear Unit [56] is a neural\nLN training, pre-LN training has an unwanted effect on the\nnetwork layer that is an element-wise product (⊗) of a linear\ngradients [59]. The earlier layers have larger gradients than", "metadata": {"id": "dc0c93d7f713ebb028c77881c9d0c62a38eecddd", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 6, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "gradients [59]. The earlier layers have larger gradients than\ntransformationandasigmoidtransformed(σ)linearprojection\nthose at the bottom. DeepNorm [60] mitigates these adverse\nof the input given as\neffects on the gradients. It is given as\nGLU(x,W,V,b,c)=(xW +b)⊗σ(xV +c), (2)\nxlf =LN(αxlp +Glp(xlp,θlp), (6)\nwhereX istheinputoflayerandl,W,b,V and carelearned\nparameters.", "metadata": {"id": "460e86a4a508cbcc3148c1c73c6bed55d5d79cba", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 6, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "xlf =LN(αxlp +Glp(xlp,θlp), (6)\nwhereX istheinputoflayerandl,W,b,V and carelearned\nparameters.\nwhere α is a constant and θlp represents the parameters of\nlayer l . These parameters are scaled by another constant β.\nGLUwasmodifiedin[55]toevaluatetheeffectofdifferent p\nBoth of these constants depend only on the architecture.\nvariations in the training and testing of transformers, resulting", "metadata": {"id": "70a772958ea7e586eecc8fc1a2155f120adebbf0", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 6, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "variations in the training and testing of transformers, resulting\nin better empirical results. Here are the different GLU varia-\ntions introduced in [55] and used in LLMs. G. Distributed LLM Training\nThis section describes distributed LLM training approaches\nReGLU(x,W,V,b,c)=max(0,xW +b)⊗, briefly. More details are available in [9], [61], [62], [63].", "metadata": {"id": "19eb7237b832bcc04acdad153e7963e5117324b8", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 6, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "ReGLU(x,W,V,b,c)=max(0,xW +b)⊗, briefly. More details are available in [9], [61], [62], [63].\nGEGLU(x,W,V,b,c)=GELU(xW +b)⊗(xV +c), 1. Data Parallelism: Data parallelism replicates the model\non multiple devices where data in a batch gets divided across\nSwiGLU(x,W,V,b,c,β)=Swishβ(xW +b)⊗(xV +c).\ndevices. At the end of each training iteration weights are\nsynchronized across all devices.", "metadata": {"id": "6e87bca90cc28a8d10a7cfee381b8ff218272901", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 6, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "devices. At the end of each training iteration weights are\nsynchronized across all devices.\nF. Layer Normalization\n2. Tensor Parallelism: Tensor parallelism shards a tensor\nLayer normalization leads to faster convergence and is a computation across devices. It is also known as horizontal\nwidely used component in transformers. In this section, we parallelism or intra-layer model parallelism.", "metadata": {"id": "31498376a98c8b099eddd9dbcd8516a7a007c458", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 6, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "provide different normalization techniques widely used in 3. Pipeline Parallelism: Pipeline parallelism shards model\nLLM literature. layers across different devices. This is also known as vertical\n1. LayerNorm: Layer norm computes statistics over all the parallelism.\nhidden units in a layer (l) as follows: 4. ModelParallelism: Acombinationoftensorandpipeline", "metadata": {"id": "b7d15c402b7cafbe1391585d4fbfa826d10c1261", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 6, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "hidden units in a layer (l) as follows: 4. ModelParallelism: Acombinationoftensorandpipeline\n(cid:118) parallelism is known as model parallelism.\nn (cid:117) n\nul = 1 (cid:88) al σl = (cid:117) (cid:116) 1 (cid:88) (al −ul)2, (3) 5. 3D Parallelism: A combination of data, tensor, and\nn i n i model parallelism is known as 3D parallelism.\ni i\n6. Optimizer Parallelism: Optimizer parallelism also", "metadata": {"id": "0e429277253a1472d42f16ad6fa58b63745d573a", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 6, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "i i\n6. Optimizer Parallelism: Optimizer parallelism also\nwhere n is the number of neurons in the layer l and al i is the known as zero redundancy optimizer [61] implements opti-\nsummed input of the i neuron in layer l. LayerNorm provides mizer state partitioning, gradient partitioning, and parameter", "metadata": {"id": "343a14d1e3140f7575629155acdb10416a6d2848", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 6, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "invariance to rescaling of the weights and re-centering of the partitioning across devices to reduce memory consumption\ndistribution. while keeping the communication costs as low as possible.\n2. RMSNorm: [57]proposedthattheinvarianceproperties\nof LayerNorm are spurious, and we can achieve the same\nH. Libraries\nperformance benefits as we get from LayerNorm by using a", "metadata": {"id": "c63d5f04a884ca0487a881f84ee739dd8c78a5ce", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 6, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "H. Libraries\nperformance benefits as we get from LayerNorm by using a\ncomputationally efficient normalization technique that trades Some commonly used libraries for LLM training are: 1)\noff re-centering invariance with speed. LayerNorm gives the Transformer [64], 2) DeepSpeed [65], 3) Megatraon-LM [62],", "metadata": {"id": "c91f860eb2760cc78df6ff65dbad870d8bb0d187", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 6, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "normalized summed input to layer l as follows 4) JAX [66], 5) Colossal-AI [67], 6) BMTrain [63], 7)\nal −ul FastMoE [68], and frameworks are 1) MindSpore [69], 2)\nal = i gl (4) PyTorch [70], 3) Tensorflow [71], 4) MXNet [72].\ni σ i", "metadata": {"id": "77aae67c514a24b7c5cce99d03eb863c90950ce2", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 6, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "JOURNALOFLATEX 6\nI. Data PreProcessing\nThis section briefly summarizes data preprocessing tech-\nniques used in LLMs training.\n1. QualityFiltering: Forbetterresults,trainingdataquality\nisessential.Someapproachestofilteringdataare:1)classifier-\nbased and 2) heuristics-based. Classifier-based approaches\ntrainaclassifieronhigh-qualitydataandpredictthequalityof", "metadata": {"id": "4ce6817822293b77927e98330870e86378db7a06", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 7, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "trainaclassifieronhigh-qualitydataandpredictthequalityof\ntext for filtering, whereas heuristics-based employ some rules\nfor filtering like language, metrics, statistics, and keywords. Fig. 4: An example of attention patterns in language models,\n2. Data Deduplication: Duplicated data can affect model image is taken from [74].\nperformance and increase data memorization; therefore, to", "metadata": {"id": "12b2794dada1325214f492cde43bd74823bffdc9", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 7, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "performance and increase data memorization; therefore, to\ntrain LLMs, data deduplication is one of the preprocessing\nsteps.Thiscanbeperformedatmultiplelevels,likesentences,\ndocuments, and datasets.\n3. Privacy Reduction: Most of the training data for LLMs\nis collected through web sources. This data contains private\ninformation; therefore, many LLMs employ heuristics-based", "metadata": {"id": "ddb72783681d6a8861561206cbb8dbb45b0322f0", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 7, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "information; therefore, many LLMs employ heuristics-based\nmethods to filter information such as names, addresses, and\nphone numbers to avoid learning personal information. Fig. 5: An example of language model training objectives,\nimage from [74].\nJ. Architectures\nHerewediscussthevariantsofthetransformerarchitectures\nK. Pre-Training Objectives", "metadata": {"id": "788821075e957ae780045129b61fe91337935ec4", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 7, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "J. Architectures\nHerewediscussthevariantsofthetransformerarchitectures\nK. Pre-Training Objectives\nat a higher level which arise due to the difference in the\napplication of the attention and the connection of transformer This section describes LLMs pre-training objectives. For\nblocks. An illustration of attention patterns of these architec- more details see the paper [74].", "metadata": {"id": "9032fdeb406c2fe10977d681041a473820ef6043", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 7, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "blocks. An illustration of attention patterns of these architec- more details see the paper [74].\ntures is shown in Figure 4. 1. Full Language Modeling: An autoregressive language\n1. Encoder Decoder: Transformers were originally de- modeling objective where the model is asked to predict future", "metadata": {"id": "180687d7a8cd7a9faccc4ce37012d39447e84200", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 7, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "signed as sequence transduction models and followed other tokens given the previous tokens, an example is shown in\nprevalentmodelarchitecturesformachinetranslationsystems. Figure 5.\nThey selected encoder-decoder architecture to train human 2. PrefixLanguageModeling: Anon-causaltrainingobjec-", "metadata": {"id": "9e27091f8d37fbb4c2192c9b435126b15a91e85b", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 7, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "languagetranslationtasks.Thisarchitectureisadoptedby[11], tive, where a prefix is chosen randomly and only remaining\n[15]. In this architectural scheme, an encoder encodes the target tokens are used to calculate the loss. An example is\ninput sequences to variable length context vectors, which are shown in Figure 5.", "metadata": {"id": "7fc806c541156b85cfc0418b8d6dd6b6da0eb8fa", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 7, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "input sequences to variable length context vectors, which are shown in Figure 5.\nthen passed to the decoder to maximize a joint objective of 3. Masked Language Modeling: In this training objective,\nminimizing the gap between predicted token labels and the tokens or spans (a sequence of tokens) are masked randomly", "metadata": {"id": "42c9fc466da76761f20a75311888dce34b3eccc2", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 7, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "actual target token labels. and the model is asked to predict masked tokens given the\n2. Causal Decoder: The underlying objective of an LLM past and future context. An example is shown in Figure 5.\nistopredictthenexttokenbasedontheinputsequence.While 4. Unified Language Modeling: Unified language model-", "metadata": {"id": "df38eca5a597846d363b7ed344080634aa4b0352", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 7, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "additional information from the encoder binds the prediction ing [75] is a combination of causal, non-causal, and masked\nstrongly to the context, it is found in practice that the LLMs language training objectives. Here in masked language mod-\ncan perform well in the absence of encoder [73], relying eling, the attention is not bidirectional but unidirectional,", "metadata": {"id": "a814053c6e2089c0e462e45536070772ade50eea", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 7, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "only on the decoder. Similar to the original encoder-decoder attending either left-to-right or right-to-left context.\narchitecture’s decoder block, this decoder restricts the flow\nof information backward, i.e., the predicted token t only\nk\nL. Model Adaptation\ndepends on the tokens preceded by and up to t . This is\nk−1", "metadata": {"id": "05f8ea7bebb5924846170c562e11aba459d15172", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 7, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "k\nL. Model Adaptation\ndepends on the tokens preceded by and up to t . This is\nk−1\nthe most widely used variant in the state-of-the-art LLMs. Thissectiondiscussesvariousmodeladaptationtechniques,\n3. Prefix Decoder: The causal masked attention is reason- where a model is pre-trained on large data and then adapted", "metadata": {"id": "46a6bc8449b0d166b733fac1a9a3f3ec186d1ce9", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 7, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "able in the encoder-decoder architectures where the encoder for downstream tasks. An example of different training stages\ncanattendtoallthetokensinthesentencefromeveryposition and inference in LLMs is shown in Figure 6.\nusing self-attention. This means that the encoder can also 1. TransferLearning: Fine-tuningapre-trainedmodelwith", "metadata": {"id": "b9a9eafd762d62faa16f75f2637b43826a859165", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 7, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "attend to tokens t to t in addition to the tokens from t data for the downstream task is known as transfer learning. In\nk+1 n 1\nto t while calculating the representation for t . But when this type of model adaptation, the model is initialized with\nk−1 k\nwe drop the encoder and only keep the decoder, we also lose pre-trained weights and updated according to the new data.", "metadata": {"id": "a751a453697d437695bbf3dd837767d68dafea21", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 7, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "this flexibility in attention. A variation in the decoder-only Some of the LLMs employing this technique are [11], [12],\narchitectures is by changing the mask from strictly causal to [15], [20].\nfully visible on a portion of the input sequence, as shown 2. Parameter Efficient Learning: The parameter efficient", "metadata": {"id": "0c035d4c9bac0490438ca97d541e002a958b4522", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 7, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "in Figure 4. The Prefix decoder is also known as non-causal learning fine-tunes a few parameters either by adding new\ndecoder architecture. parameters to the model or the existing ones.", "metadata": {"id": "5951dd90c6fc547d60f44e531622294929ec2291", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 7, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "JOURNALOFLATEX 7\nFig. 6: A basic flow diagram depicting various stages of LLMs from pre-training to prompting/utilization. Prompting LLMs\nto generate responses is possible at different training stages like pre-training, instruction-tuning, or alignment tuning.\nPrompt Tuning: [30], [76] adds trainable prompt token em- generate text and make decisions, making it vital to control", "metadata": {"id": "8fdeb7872ed5d0bd60447b6fbaca20e488a403e0", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 8, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "beddings as prefixes or free-style to the input token embed- their behavior and outputs to avoid undesirable outcomes.\ndings. During fine-tuning only these embeddings parameters Alignment techniques aim to bridge the gap between what\nare trained for the downstream task while keeping the rest of humansexpectfromLLMsandtheiractualbehavior.Amodel", "metadata": {"id": "3cfe762f92be8aa7cb7d0ea468d41b30d393d20d", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 8, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "the weights frozen. is defined to be an “aligned” model if the model fulfills three\ncriteria of helpful, honest, and harmless or “HHH” [78].\nPrefix Tuning: [31] adds task-specific trainable prefix vectors\nTo align a model with human values, researchers widely\nto the transformer layers, where only prefix parameters are\nemploy reinforcement learning with human feedback", "metadata": {"id": "f8d85fd506112bb3d5e97ce68e4ff68efe8652bf", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 8, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "employ reinforcement learning with human feedback\nfine-tuned, and the rest of the model stays frozen. The input\n(RLHF)[79].InRLHF,afine-tunedmodelondemonstrations\nsequence tokens can attend prefixes acting as virtual tokens.\nis further trained with reward modeling (RM) and\nAdapter Tuning: module is an encoder-decoder architecture\nreinforcement learning (RL), shown in Figure 6. Below", "metadata": {"id": "783ec35cb2bbe6bebf2cb97f1c8c06f549c1f728", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 8, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "reinforcement learning (RL), shown in Figure 6. Below\nthat is placed either sequential or parallel to the attention and\nwe briefly discuss RM and RL pipelines in RLHF.\nfeed-forward layers in the transformer block [77], [28], [29].\n4.1 Reward modeling: Reward modeling trains a model\nOnly these layers are fine-tuned, and the rest of the model is", "metadata": {"id": "7014bf655471b629c0c149346f5abb2385df3ab3", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 8, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "Only these layers are fine-tuned, and the rest of the model is\nto rank generated responses according to human preferences\nkept frozen.\nusing a classification objective. To train the classifier humans\n3. InstructionFinetuning: Instructiontuningisanapproach annotate the responses based on HHH criteria.", "metadata": {"id": "7cd5e9303647789271f082fd82ab3d59bb8c90a2", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 8, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "to fine-tuning pre-trained models on instruction formatted 4.2 Reinforcement Learning: In this stage, the reward\ndata. Instructions generally comprise multiple tasks in plain modeltrainedpreviouslyranksLLM-generatedresponsesinto\nnatural language, guiding the model to respond according to preferred vs. dispreferred. The output of the reward model", "metadata": {"id": "18c5cf4cc728c7c18a2f84b80fba8991e2e85e4e", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 8, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "the prompt and the input. The training data consists of an is used to train the model with proximal policy optimization\ninstructionandaninput-outputpair.Moredetailsonformatting (PPO). This process repeats iteratively until convergence.\ninstruction data and its various styles are available in [33]. 5. Prompting/Utilization: Prompting is a method to query", "metadata": {"id": "152d95e88ec594cc255e02a2a90de0bd1414bf33", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 8, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "4. Alignment Tuning: Alignment techniques play a crucial trained LLMs for generating responses, as illustrated in\nrole in ensuring large language models (LLMs) operate Figure 6. LLMs can be prompted in various prompt setups,\naccording to human intentions and values. These models can where they can be adapted to the instructions without fine-", "metadata": {"id": "48c7ab735540bb391e726287a5a658558fabc6a0", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 8, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "JOURNALOFLATEX 8\ntuning and in other cases with fine-tuning on data containing\ndifferent prompt styles [25], [80], [81]. A good guide on\nprompt engineering is available at [82]. Below, we will\ndiscuss various widely used prompt setups.\nIn-context Learning: Multiple input-output demonstration\npairsareshowntothemodeltogeneratethedesiredresponse.", "metadata": {"id": "17a45dc816bd829a5cac64bb9ba2ae1cf98c7875", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 9, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "pairsareshowntothemodeltogeneratethedesiredresponse.\nThis adaptation style is also called few-shot learning. A\ndiscussion on formatting in-context learning (ICL) templates\nis available in [83], [33], [26], [25]. Fig. 7: Unified text-to-text training example, source image\nChain-of-Thought: Chain-of-thought prompting (CoT) is from [11].\na special case of prompting where demonstrations contain", "metadata": {"id": "252c4bf9fe14a9920b535d111ba6e317b707bd3a", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 9, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "a special case of prompting where demonstrations contain\nreasoning information aggregated with inputs and outputs so\nthat the model generates outcomes with reasonings. Some\nexamples in literature train LLMs with CoT reasoning,\nwhereas other utilizes LLMs’ CoT abilities without fine-\ntuning. More details on CoT prompts are available in [84],\n[85], [80].", "metadata": {"id": "6417e7d5b06c86073f702f5067979c9eb47b57cb", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 9, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "tuning. More details on CoT prompts are available in [84],\n[85], [80].\nSelf-Consistency: Improves CoT reasoning performance by\ngenerating multiple responses and selecting the most frequent\nanswer [86].\nTree-of-Thought: Explores multiple reasoning paths with\npossibilities to look ahead and backtrack for problem-\nsolving [87].\nFig. 8: The image is the article of [90], showing an example", "metadata": {"id": "9574e29043299023ff2a8c934b8aff7fc790aeaf", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 9, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "solving [87].\nFig. 8: The image is the article of [90], showing an example\nIII. LARGELANGUAGEMODELS of PanGu-α architecture.\nThissectionreviewsLLMs,brieflydescribingtheirarchitec-\ntures, training objectives, pipelines, datasets, and fine-tuning\n1.3 mT5 [12]: A multilingual T5 model [11] trained on\ndetails.\nthe mC4 dataset with 101 languages. The dataset is extracted", "metadata": {"id": "abd96a424a8dd98c25eac858381a2894517a0a0f", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 9, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "details.\nthe mC4 dataset with 101 languages. The dataset is extracted\nfrom the public common crawl scrape. The model uses a\nA. Pre-Trained LLMs larger vocab size of 250,000 to cover multiple languages.\nTo avoid over-fitting or under-fitting for a language, mT5\nHere, we provide summaries of various well-known pre-\nemploys a data sampling procedure to select samples from all", "metadata": {"id": "49739aea09a637690bab14f76f2f43ace74cfc23", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 9, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "employs a data sampling procedure to select samples from all\ntrainedLLMswithsignificantdiscoveries,changingthecourse\nlanguages. The paper suggests using a small amount of pre-\nof research and development in NLP. These LLMs have\ntraining datasets, including all languages when fine-tuning for\nconsiderably improved the performance in NLU and NLG", "metadata": {"id": "21025b0dbd68e0675227b9881ea19718702521a8", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 9, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "considerably improved the performance in NLU and NLG\na task using English language data. This allows the model to\ndomains, and are widely fine-tuned for downstream tasks.\ngenerate correct non-English outputs.\n1. General Purpose:\n1.4 PanGu-α [90]: An autoregressive model that has a\n1.1 T5 [11]: An encoder-decoder model employing a\nquerylayerattheendofstandardtransformerlayers,example", "metadata": {"id": "79335d720394087339d3be0673b180296a17ec40", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 9, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "querylayerattheendofstandardtransformerlayers,example\nunified text-to-text training for all NLP problems, shown in\nshowninFigure8,withaimtopredictnexttoken.Itsstructure\nFigure 7. T5 places layer normalization outside the residual\nis similar to the transformer layer but with an additional\npathinaconventionaltransformermodel[44].Itusesmasked", "metadata": {"id": "9f4def4cf2ea55ef03a792b17d76d560366ce675", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 9, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "pathinaconventionaltransformermodel[44].Itusesmasked\nembedding for the next position in the attention mechanism,\nlanguage modeling as a pre-training objective where spans\ngiven in Eq. 7.\n(consecutivetokens)arereplacedwithasinglemaskinsteadof\na=p WqWkTHT (7)\nseparate masks for each token. This type of masking speeds n h h L", "metadata": {"id": "b5f92229b3f9493620cd289b95ee3f072f63efa9", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 9, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "a=p WqWkTHT (7)\nseparate masks for each token. This type of masking speeds n h h L\nup the training as it produces shorter sequences. After pre- 1.5 CPM-2 [13]: Cost-efficient Pre-trained language\ntraining, the model is fine-tuned using adapter layers [77] for Models (CPM-2) pre-trains bilingual (English and Chinese)\ndownstream tasks. 11B and 198B mixture-of-experts (MoE) models on the Wu-", "metadata": {"id": "bad7ab41ed0d4f5db6a2230e20333b229e678477", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 9, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "downstream tasks. 11B and 198B mixture-of-experts (MoE) models on the Wu-\n1.2 GPT-3 [8]: The GPT-3 architecture is same as the DaoCorpus[91]dataset.Thetokenizationprocessremoves“_”\nGPT-2[88]butwithdenseandsparseattentionintransformer whitespacetokensinthesentencepiecetokenizer.Themodels", "metadata": {"id": "9d304ea80af052a3db45ff0d3fde30d0cd861a29", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 9, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "layers similar to the Sparse Transformer [45]. It shows that are trained with knowledge inheritance, starting with only the\nlarge models can train on larger batch sizes with a lower Chinese language in the first stage and then adding English\nlearningrate;inordertodecidethebatchsizeduringtraining, and Chinese data. This trained model gets duplicated multiple", "metadata": {"id": "d69e83a7691219f531cbabb90308724568a37e1a", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 9, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "GPT-3 uses the gradient noise scale as in [89]. Overall, times to initialize the 198B MoE model. Moreover, to use\nGPT-3 increases model parameters to 175B showing that the the model for downstream tasks, CPM-2 experimented with\nperformanceoflargelanguagemodelsimproveswiththescale both complete fine-tuning and prompt fine-tuning as in [27]", "metadata": {"id": "bdae93bb0fadf92cfa65e02cc8ed4f77b115e50a", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 9, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "and is competitive with the fine-tuned models. whereonlyprompt-relatedparametersareupdatedbyinserting", "metadata": {"id": "9f6914e06c4f77d47ea46407b0e20eaa9f88878c", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 9, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "JOURNALOFLATEX 9\nprompts at various positions, front, middle, and back. CPM-2 pre-training step, which enables ERNIE 3.0 Titan to beat\nalso proposes INFMOE, a memory-efficient framework with other LLMs in their manually selected Factual QA task set\na strategy to dynamically offload parameters to the CPU for evaluations.", "metadata": {"id": "cc9c6b3a054de1ea5574c2605f9af88571d3848e", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 10, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "a strategy to dynamically offload parameters to the CPU for evaluations.\ninference at a 100B scale. It overlaps data movement with 1.12 GPT-NeoX-20B [100]: An auto-regressive model\ninference computation for lower inference time. that largely follows GPT-3 with a few deviations in architec-", "metadata": {"id": "170e48267cd72ea8cb32879096b507eaf2429bca", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 10, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "1.6 ERNIE 3.0 [92]: ERNIE 3.0 takes inspiration from ture design, trained on the Pile dataset without any data dedu-\nmulti-task learning to build a modular architecture using plication. GPT-NeoX has parallel attention and feed-forward\nTransformer-XL [93] as the backbone. The universal repre- layers in a transformer block, given in Eq. 8, that increases", "metadata": {"id": "06b89f0d0ba92239d9559f5f48a337a0a41987b9", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 10, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "sentationmoduleissharedbyallthetasks,whichserveasthe throughput by 15%. It uses rotary positional embedding [48],\nbasicblockfortask-specificrepresentationmodules,whichare applying it to only 25% of embedding vector dimension as\nall trained jointly for natural language understanding, natural in [101]. This reduces the computation without performance", "metadata": {"id": "cab9263a53111cc6713c40571b8ac39019e2ffee", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 10, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "language generation, and knowledge extraction. This LLM is degradation. Opposite to GPT-3, which uses dense and sparse\nprimarily focused on the Chinese language, claims to train layers, GPT-NeoX-20B uses only dense layers. The hyperpa-\non the largest Chinese text corpora for LLM training, and rameter tuning at this scale is difficult; therefore, the model", "metadata": {"id": "dcccf9974fe896ac239ca8240dcbd5fca08deebb", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 10, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "achieved state-of-the-art in 54 Chinese NLP tasks. chooseshyperparametersfromthemethod[8]andinterpolates\n1.7 Jurassic-1 [94]: A pair of auto-regressive language valuesbetween13Band175Bmodelsforthe20Bmodel.The\nmodels, including a 7B-parameter J1-Large model and a model training is distributed among GPUs using both tensor", "metadata": {"id": "45272e66401bef2b2aa73ce540155b85f692a713", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 10, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "178B-parameter J1-Jumbo model. The training vocabulary of and pipeline parallelism.\nJurassic-1 comprise word pieces, complete words, and multi-\nx+Attn(LN (x))+FF(LN (x)) (8)\nwordexpressionswithoutanywordboundaries,wherepossible 1 2\nout-of-vocabulary instances are interpreted as Unicode bytes. 1.13 OPT [10]: It is a clone of GPT-3, developed with", "metadata": {"id": "3abb2ed30fb7e24e02880f9fb008a515f3574330", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 10, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "Compared to the GPT-3 counterparts, the Jurassic-1 models the intention to open-source a model that replicates GPT-3\napply a more balanced depth-to-width self-attention architec- performance. Training of OPT employs dynamic loss scaling\nture [95] and an improved tokenizer for a faster prediction [102] and restarts from an earlier checkpoint with a lower", "metadata": {"id": "558593e1da370f149178ff8033d548140ae219e1", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 10, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "based on broader resources, achieving a comparable perfor- learning rate whenever loss divergence is observed. Overall,\nmance in zero-shot learning tasks and a superior performance the performance of OPT-175B models is comparable to the\nin few-shot learning tasks given the ability to feed more GPT3-175B model.\nexamples as a prompt. 1.14 BLOOM [9]: A causal decoder model trained on", "metadata": {"id": "2675177ad96a6a8145536397a97dc556878f6a68", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 10, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "examples as a prompt. 1.14 BLOOM [9]: A causal decoder model trained on\n1.8 HyperCLOVA [96]: A Korean language model with ROOTS corpus with the aim of open-sourcing an LLM. The\nGPT-3 architecture. architectureofBLOOMisshowninFigure9,withdifferences\n1.9 Yuan 1.0 [97]: Trained on a Chinese corpus with like ALiBi positional embedding, an additional normalization", "metadata": {"id": "412dc5a21e1294adc9ec948a7b0bb8a4d584b5da", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 10, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "5TB of high-quality text collected from the Internet. A layer after the embedding layer as suggested by the bitsand-\nMassive Data Filtering System (MDFS) built on Spark is bytes1 library. These changes stabilize training with improved\ndeveloped to process the raw data via coarse and fine filtering downstream performance.", "metadata": {"id": "803942320aebcdae4a073ef6d2a18e361d4e634f", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 10, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "developed to process the raw data via coarse and fine filtering downstream performance.\ntechniques. To speed up the training of Yuan 1.0 with the 1.15 GLaM [103]: Generalist Language Model (GLaM)\naim of saving energy expenses and carbon emissions, various represents a family of language models using a sparsely acti-", "metadata": {"id": "ae50265c5eb5acbc61e027f6eb2581c61c390b76", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 10, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "factors that improve the performance of distributed training vated decoder-only mixture-of-experts (MoE) structure [104],\nare incorporated in architecture and training like increasing [105]. To gain more model capacity while reducing compu-\nthe number of hidden size improves pipeline and tensor par- tation, the experts are sparsely activated where only the best", "metadata": {"id": "58c54dd78e7703c31c214039447b73429f8bcda4", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 10, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "allelism performance, larger micro batches improve pipeline two experts are used to process each input token. The largest\nparallelismperformance,andhigherglobalbatchsizeimprove GLaMmodel,GLaM(64B/64E),isabout7×largerthanGPT-\ndata parallelism performance. In practice, the Yuan 1.0 model 3[8],whileonlyapartoftheparametersisactivatedperinput", "metadata": {"id": "d1cb3abdb06df14f286c9701ed0e2917fc0e4b4d", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 10, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "performswellontextclassification,WinogradSchema,natural token. The largest GLaM (64B/64E) model achieves better\nlanguage inference, and reading comprehension tasks. overall results as compared to GPT-3 while consuming only\n1.10 Gopher [98]: The Gopher family of models ranges one-third of GPT-3’s training energy.", "metadata": {"id": "1c0c489119e0e2b3ee6623f767df291de3b7bdf0", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 10, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "1.10 Gopher [98]: The Gopher family of models ranges one-third of GPT-3’s training energy.\nfrom 44M to 280B parameters in size to study the effect of 1.16 MT-NLG [21]: A 530B causal decoder based on\nscale onthe LLMsperformance. The280B modelbeats GPT- GPT-2 architecture that is roughly 3× GPT-3 model parame-\n3 [8], Jurrasic-1 [94], MT-NLG [21], and others on 81% of", "metadata": {"id": "c5b0cf73a0f5fccf705b3a71d628c24e1773814e", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 10, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "3 [8], Jurrasic-1 [94], MT-NLG [21], and others on 81% of\nters.MT-NLGistrainedonfilteredhigh-qualitydatacollected\nthe evaluated tasks.\nfrom various public datasets and blends various types of\n1.11 ERNIE 3.0 TITAN [99]: ERNIE 3.0 Titan extends\ndatasets in a single batch, which beats GPT-3 on a number\nERNIE3.0bytrainingalargermodelwith26xthenumberof\nof evaluations.", "metadata": {"id": "64555e2da5ed2762c506b1895467a1ae0e5edf9f", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 10, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "ERNIE3.0bytrainingalargermodelwith26xthenumberof\nof evaluations.\nparametersofthelatter.Thisbiggermodeloutperformedother\n1.17 Chinchilla [106]: A causal decoder trained on the\nstate-of-the-art models in 68 NLP tasks. LLMs produce text\nsame dataset as the Gopher [98] but with a little different\nwith incorrect facts. In order to have control of the generated", "metadata": {"id": "22401c8d965e2d8d016a80fb5d3b0b3bccd1dd53", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 10, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "with incorrect facts. In order to have control of the generated\ndata sampling distribution (sampled from MassiveText). The\ntext with factual consistency, ERNIE 3.0 Titan adds another\nmodel architecture is similar to the one used for Gopher,\ntask, Credible and Controllable Generations, to its multi-\nwith the exception of AdamW optimizer instead of Adam.", "metadata": {"id": "f3bc05f8802e9ab08b48785a29a0bcf8d434beff", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 10, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "with the exception of AdamW optimizer instead of Adam.\ntask learning setup. It introduces additional self-supervised\nadversarial and controllable language modeling losses to the 1https://github.com/TimDettmers/bitsandbytes", "metadata": {"id": "72961378115e88972863142cc326047b94c395c5", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 10, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "JOURNALOFLATEX 10\nregular denoising, and 25% extreme denoising loss functions.\n1.21 UL2 [15]: An encoder-decoder architecture trained\nusing a mixture of denoisers (MoD) objectives. Denoisers\ninclude1)R-Denoiser:aregularspanmasking,2)S-Denoiser:\nwhich corrupts consecutive tokens of a large sequence and\n3) X-Denoiser: which corrupts a large number of tokens", "metadata": {"id": "0e1cd9afb66216ff74f6cb4df48e0d2c9fb3ec6e", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 11, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "3) X-Denoiser: which corrupts a large number of tokens\nrandomly. During pre-training, UL2 includes a denoiser token\nfrom R,S,X to represent a denoising setup. It helps improve\nfine-tuning performance for downstream tasks that bind the\ntask to one of the upstream training modes. This MoD style\nof training outperforms the T5 model on many benchmarks.", "metadata": {"id": "b06103748c9624a7b26fc01eb5bd645927bcd56e", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 11, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "of training outperforms the T5 model on many benchmarks.\nFig. 9: The BLOOM architecture example sourced from [9].\n1.22 GLM-130B [109]: GLM-130B is a bilingual (En-\nglish and Chinese) model trained using an auto-regressive\nmaskinfillingpre-trainingobjectivesimilartotheGLM[110].\nChinchilla identifies the relationship that model size should\nThistrainingstylemakesthemodelbidirectionalascompared", "metadata": {"id": "e885b8a589b2dd510a34f7df721ec1b79f886849", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 11, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "Thistrainingstylemakesthemodelbidirectionalascompared\nbe doubled for every doubling of training tokens. Over 400\nto GPT-3, which is unidirectional. Opposite to the GLM, the\nlanguage models ranging from 70 million to over 16 billion\ntraining of GLM-130B includes a small amount of multi-task\nparameters on 5 to 500 billion tokens are trained to get the", "metadata": {"id": "8555f13f6ecff278161dad9f67c5ab1b6ea24861", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 11, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "parameters on 5 to 500 billion tokens are trained to get the\ninstruction pre-training data (5% of the total data) along with\nestimates for compute-optimal training under a given budget.\nthe self-supervised mask infilling. To stabilize the training, it\nThe authors train a 70B model with the same compute budget\napplies embedding layer gradient shrink.", "metadata": {"id": "948b176bcea82da09ab5ce87a3fcc25cfceb50b1", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 11, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "The authors train a 70B model with the same compute budget\napplies embedding layer gradient shrink.\nas Gopher (280B) but with 4 times more data. It outperforms\n1.23 LLaMA [111], [112]: A set of decoder-only lan-\nGopher [98], GPT-3 [8], and others on various downstream\nguage models varying from 7B to 70B parameters. LLaMA\ntasks, after fine-tuning.", "metadata": {"id": "0d4ad10d68bc3c2d70f15eb11032cef0d35f38e9", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 11, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "guage models varying from 7B to 70B parameters. LLaMA\ntasks, after fine-tuning.\nmodels series is the most famous among the community for\n1.18 AlexaTM[107]: Anencoder-decodermodel,where parameter-efficient and instruction tuning.\nencoder weights and decoder embeddings are initialized with LLaMA-1 [111]: Implements efficient causal attention [113]", "metadata": {"id": "6bb73959900073b8c82044265bad3e988182c22d", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 11, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "a pre-trained encoder to speedup training. The encoder stays by not storing and computing masked attention weights and\nfrozenforinitial100kstepsandlaterunfreezedforend-to-end key/queryscores.Anotheroptimizationisreducingnumberof\ntraining. The model is trained on a combination of denoising activations recomputed in backward pass, as in [114].", "metadata": {"id": "43ebee562d5c6ce12ea73b9159ac51c25758d015", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 11, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "and causal language modeling (CLM) objectives, concate- LLaMA-2 [112]: This work is more focused towards fine-\nnating [CLM] token at the beginning for mode switiching. tuning a safer and better LLaMA-2-Chat model for dialogue\nDuringtraining,the CLMtaskisappliedfor20%ofthetime, generation.Thepre-trainedmodelhas40%moretrainingdata", "metadata": {"id": "c0f9a12670c9647977d0c0b772d8305f07e4e44b", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 11, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "which improves the in-context learning performance. with a larger context length and grouped-query attention.\n1.19 PaLM [14]: A causal decoder with parallel atten- 1.24 PanGu-Σ [115]: An autoregressive model with\ntion and feed-forward layers similar to Eq. 8, speeding up parameters copied from PanGu-α and extended to a trillion", "metadata": {"id": "c812cfc2f3b6bbeb6e3d8e15fec9d8294e536543", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 11, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "training 15 times faster. Additional changes to the conven- scale with Random Routed Experts (RRE), the architectural\ntional transformer model include SwiGLU activation, RoPE diagram is shown in Figure 10. RRE is similar to the MoE\nembeddings,multi-queryattentionthatsavescomputationcost architecture,withdistinctionsatthesecondlevel,wheretokens", "metadata": {"id": "5d0ad6453ec0b3bd8911b29fae141898796b8aa9", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 11, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "duringdecoding,andsharedinput-outputembeddings.During are randomly routed to experts in a domain instead of using a\ntraining, loss spiking was observed, and to fix it, model learnablegatingmethod.Themodelhasbottomlayersdensely\ntraining was restarted from a 100 steps earlier checkpoint activatedandsharedacrossalldomains,whereastoplayersare", "metadata": {"id": "f8d457bd60d7f1fa5544b5eb47f7af5539df3079", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 11, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "by skipping 200-500 batches around the spike. Moreover, the sparselyactivatedaccordingtothedomain.Thistrainingstyle\nmodel was found to memorize around 2.4% of the training allowsextractingtask-specificmodelsandreducescatastrophic\ndata at the 540B model scale, whereas this number was lower forgetting effects in case of continual learning.\nfor smaller models. 2. Coding:", "metadata": {"id": "90924b3d47846973eb1b87a72aa673d64a588b04", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 11, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "for smaller models. 2. Coding:\nPaLM-2 [108]: A smaller multi-lingual variant of PaLM, 2.1 CodeGen[116]: CodeGenhassimilararchitectureto\ntrained for larger iterations on a better quality dataset. The the PaLM [14], i.e., parallel attention, MLP layers, and RoPE\nPaLM-2 shows significant improvements over PaLM, while embeddings. The model is trained on both natural language", "metadata": {"id": "28c6a0275c1148d7541b1edcf683fda06f48f627", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 11, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "reducing training and inference costs due to its smaller size. and programming language data sequentially (trained on the\nTolessentoxicityandmemorization,itappendsspecialtokens first dataset, then the second and so on) on the following\nwith a fraction of pre-training data, which shows reduction in datasets1)PILE,2)BIGQUERYand3)BIGPYTHON.Code-", "metadata": {"id": "f0d7354fa3a6306df1a887c5ba452b8ba80244fb", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 11, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "generating harmful responses. Genproposedamulti-stepapproachtosynthesizingcode.The\n1.20 U-PaLM [20]: This method trains PaLM for 0.1% purposeistosimplifythegenerationoflongsequenceswhere\nadditional compute with UL2 (also named as UL2Restore) thepreviouspromptandgeneratedcodearegivenasinputwith", "metadata": {"id": "caf5b133b64cd86ffb9687153d52514a23bc5b4c", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 11, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "objective[15]usingthesamedatasetandoutperformsbaseline thenextprompttogeneratethenextcodesequence.CodeGen\nsignificantly on various NLP tasks, including zero-shot, few- opensource a Multi-Turn Programming Benchmark (MTPB)\nshot, commonsense reasoning, CoT, etc. Training with UL2R to evaluate multi-step program synthesis.", "metadata": {"id": "c01c7d3333e764d0f98eb77d3357fd027e8c5944", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 11, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "shot, commonsense reasoning, CoT, etc. Training with UL2R to evaluate multi-step program synthesis.\ninvolves converting a causal decoder PaLM to a non-causal 2.2 Codex [117]: This LLM is trained on a subset\ndecoderPaLMandemploying50%sequentialdenoising,25% of public Python Github repositories to generate code from", "metadata": {"id": "6f1132067c241698567f9f5ce29cb60a545c3007", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 11, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "JOURNALOFLATEX 11\ndocstrings. Computer programming is an iterative process\nwhere the programs are often debugged and updated before\nfulfilling the requirements. Similarly to this, Codex generates\n100 versions of a program by repetitive sampling for a given\ndescription, which produces a working solution for 77.5% of\nthe problems passing unit tests. Its powerful version powers\nGithub Copilot2.", "metadata": {"id": "fd80386e15c2362f3af7f118da6af5d94e88c2d1", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 12, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "the problems passing unit tests. Its powerful version powers\nGithub Copilot2.\n2.3 AlphaCode [118]: A set of large language mod-\nels, ranging from 300M to 41B parameters, designed for\ncompetition-level code generation tasks. It uses the multi-\nquery attention [119] to reduce memory and cache costs.\nSince competitive programming problems highly require deep", "metadata": {"id": "8f3f8d1cde517f206f964251c4072dc95f212629", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 12, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "Since competitive programming problems highly require deep\nreasoning and an understanding of complex natural language\nalgorithms, the AlphaCode models are pre-trained on filtered\nGitHub code in popular languages and then fine-tuned on a\n(cid:80)\nFig. 10: This example illustrates the PanGu- architecture,\nnew competitive programming dataset named CodeContests.", "metadata": {"id": "4363f3c1a3c33adf820fff9c19a4b13a1173cce6", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 12, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "new competitive programming dataset named CodeContests.\nas depicted in the image sourced from [115].\nThe CodeContests dataset mainly contains problems, solu-\ntions, and test cases collected from the Codeforces platform3.\nThe pre-training employs standard language modeling objec-\n4.1 LaMDA [127]: A decoder-only model pre-trained\ntives, while GOLD [120] with tempering [121] serves as the", "metadata": {"id": "c554fa2111710dc00222cbabd63fbbc54459c2bc", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 12, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "tives, while GOLD [120] with tempering [121] serves as the\non public dialog data, public dialog utterances, and public\ntrainingobjectiveforthefine-tuningonCodeContestsdata.To\nweb documents, where more than 90% of the pre-training\nevaluate the performance of AlphaCode, simulated program-\ndata is in English. LaMDA is trained with the objective", "metadata": {"id": "864ffc3cff44e104ed48a69842da084578081212", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 12, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "data is in English. LaMDA is trained with the objective\nming competitions are hosted on the Codeforces platform:\nof producing responses that exhibit high levels of quality,\noverall, AlphaCode ranks at the top 54.3% among over 5000\nsafety, and groundedness. To achieve this, discriminative and\ncompetitors,whereitsCodeforcesratingiswithinthetop28%", "metadata": {"id": "fef466e1e5494b8831c08b2387f8ddd6d477ebac", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 12, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "competitors,whereitsCodeforcesratingiswithinthetop28%\ngenerative fine-tuning techniques are incorporated to enhance\nof recently participated users.\nthemodel’ssafetyandqualityaspects.Asaresult,theLaMDA\n2.4 CodeT5+ [122]: CodeT5+ is based on modelscanbeutilizedasagenerallanguagemodelperforming\nCodeT5 [123], with shallow encoder and deep decoder, various tasks.", "metadata": {"id": "32853fad19dbf2758afb4ee97e6f8ed625bcf225", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 12, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "CodeT5 [123], with shallow encoder and deep decoder, various tasks.\ntrained in multiple stages initially unimodal data (code) and 5. Finance:\nlater bimodal data (text-code pairs). Each training stage has 5.1 BloombergGPT [128]: A non-causal decoder model\ndifferent training objectives and activates different model trained using both financial (\"FINPILE\" from the Bloomberg", "metadata": {"id": "d6b313557668a679b5c2793b01e09afd15dc13e8", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 12, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "blocks encoder, decoder, or both according to the task. The archive) and general-purpose datasets. The model’s architec-\nunimodal pre-training includes span denoising and CLM ture is similar to the BLOOM [9] and OPT [10]. It allocates\nobjectives, whereas bimodal pre-training objectives contain 50B parameters to different blocks of the model using the", "metadata": {"id": "14f2f75e5d37760d32e99b44bc30f916eb674fe3", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 12, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "contrastive learning, matching, and CLM for text-code pairs. approach [129]. For effective training, BloombergGPT packs\nCodeT5+ adds special tokens with the text to enable task documents together with < |endoftext| > to use maximum\nmodes, for example, [CLS] for contrastive loss, [Match] for sequence length, use warmup batch size starting from 1024 to", "metadata": {"id": "0e61b175230102c97082daa80542aa7e9d90a875", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 12, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "text-code matching, etc. 2048, and manually reduces the learning rate multiple times\n2.5 StarCoder [124]: A decoder-only model with San- during the training.\ntaCoder architecture, employing Flash attention to scale up 5.2 Xuan Yuan 2.0 [130]: A Chinese financial chat\nthe context length to 8k. The StarCoder trains an encoder to model with BLOOM’s [9] architecture trained on a combina-", "metadata": {"id": "3899221420e6c80a8a361c1df3a39d4d87710721", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 12, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "filter names, emails, and other personal data from the training tionofgeneralpurpose,financial,generalpurposeinstructions,\ndata. Its fine-tuned variant outperforms PaLM, LLaMA, and and financial institutions datasets. Xuan Yuan 2.0 combined\nLAMDA on HumanEval and MBPP benchmarks. the pre-training and fine-tuning stages to avoid catastrophic\n3. Scientific Knowledge: forgetting.", "metadata": {"id": "5104b0699c4885691e65ac8251f2bf4cc640108b", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 12, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "3. Scientific Knowledge: forgetting.\n3.1 Galactica [125]: A large curated corpus of human\nscientificknowledgewith48millionpapers,textbooks,lecture B. Fine-Tuned LLMs\nnotes,millionsofcompoundsandproteins,scientificwebsites, Pre-trained LLMs have excellent generalization abilities to\nencyclopedias, and more are trained using metaseq library3, unseentasks.However,becausetheyaregenerallytrainedwith", "metadata": {"id": "5cef6c2654c95464597d152b32603a77dca2116a", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 12, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "which is built on PyTorch and fairscale [126]. The model the objective of next token prediction, LLMs have limited\nwraps reasoning datasets with < work > token to provide capacity to follow user intent and are prone to generate un-\nstep-by-step reasoning context to the model, which has been ethical,toxicorinaccurateresponses[137].Fortheireffective", "metadata": {"id": "017d00ccc52c0d1e1d0a61abce077d999512ddc0", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 12, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "shown to improve the performance on reasoning tasks. utilization, LLMs are fine-tuned to follow instructions [25],\n4. Dialog: [22],[24]andgeneratesaferesponses[137],whichalsoresults\nin increasing zero-shot, few-shot, and cross-task generaliza-\n2https://github.com/features/copilot tion [24], [25], [26], with minimal compute increment, e.g.,", "metadata": {"id": "acc19fa45f2a94db28082518b5bcae3e70e261ec", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 12, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "2https://github.com/features/copilot tion [24], [25], [26], with minimal compute increment, e.g.,\n3https://codeforces.com/ 0.2% of the total pre-training for PaLM 540B [25].", "metadata": {"id": "68517516c5e6dfe779d81f38e379c5e5052f95c1", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 12, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "JOURNALOFLATEX 12\nTABLE I: Noteworthy findings and insights from pre-trained Large Language Model.\nModels Findings&Insights\n• Encoderanddecoderwithsharedparametersperformequivalentlywhenparametersarenotshared\nT5 • Fine-tuningmodellayers(adapterlayers)workbetterthantheconventionalwayoftrainingononlyclassificationlayers", "metadata": {"id": "deb1c909b80f882606dcad12a56511e8e826d494", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 13, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "GPT-3 • Few-shotperformanceofLLMsisbetterthanthezero-shot,suggestingthatLLMsaremeta-learners\n• Large multi-lingual models perform equivalently to single language models on downstream tasks. However, smaller multi-\nmT5 lingualmodelsperformworse\nPanGu-α • LLMsaregoodatafewshotcapabilities\n• Promptfine-tuningrequiresupdatingveryfewparameterswhileachievingperformancecomparabletofullmodelfine-tuning", "metadata": {"id": "52edb7cd186ae2b0a62c65a83c101db34b511db6", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 13, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "• Promptfine-tuningtakesmoretimetoconvergeascomparedtofullmodelfine-tuning\n• Inserting prompt tokens in-between sentences can allow the model to understand relations between sentences and long\nCPM-2 sequences\n• Inananalysis,CPM-2findsthatpromptsworkasaprovider(additionalcontext)andaggregator(aggregateinformationwith\ntheinputtext)forthemodel", "metadata": {"id": "db9dba1e8eabfd65edbd057a04948e57a58337e3", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 13, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "theinputtext)forthemodel\n• ThisLLMfocusesoncodeevaluationsandintroducesanovelwayofselectingthebestcodesamples.\nCodex • Theresultsindicateitispossibletoaccuratelyselectcodesamplesusingheuristicrankinginlieuofadetailedevaluationof\neachsample,whichmaynotbefeasibleorfeasibleinsomesituations.\n• ERNIE3.0showsthatamodularLLMarchitecturewithauniversalrepresentationmoduleandtask-specificrepresentation", "metadata": {"id": "6ae76468efb03becc92efbd7bdc361ff70c94238", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 13, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "modulehelpsinfinetuningphase.\nERNIE3.0 • Optimizingtheparametersofatask-specificrepresentationnetworkduringthefine-tuningphaseisanefficientwaytotake\nadvantageofthepowerfulpretrainedmodel.\n• TheperformanceofanLLMishighlyrelatedtothenetworksize.\n• Toimproveruntimeperformance,moreoperationscanbeperformedinparallel(width)ratherthansequentially(depth).", "metadata": {"id": "0ec64929c6acf93f94b7527953065aa12a62cab2", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 13, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "Jurassic-1 • Toefficientlyrepresentandfitmoretextinthesamecontextlength,themodelusesalargervocabularytotrainaSentencePiece\ntokenizerwithoutrestrictingittowordboundaries.Thistokenizerimprovementcanfurtherbenefitfew-shotlearningtasks.\n• Byemployingprompt-basedtuning,theperformancesofmodelscanbeimproved,oftensurpassingthoseofstate-of-the-art", "metadata": {"id": "c53a6659e71820cc2e3aa5e9b9e89bda4bfd1e1a", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 13, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "HyperCLOVA modelswhenthebackwardgradientsofinputsareaccessible.\n• The model architecture that excels in pre-training and fine-tuning cases may exhibit contrasting behavior in zero-shot and\nYuan1.0 few-shotlearning.\nGopher • Relativeencodingsenablemodelstobeevaluatedforlongersequencesthanthoseonwhichitwastrained.", "metadata": {"id": "6eaaab252d31b814a036403803252ed8803907f0", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 13, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "Gopher • Relativeencodingsenablemodelstobeevaluatedforlongersequencesthanthoseonwhichitwastrained.\n• ThisLLMbuildsontopofERNIE3.0andaddaself-supervisedadversariallosstodistinguishwhetheratextisgenerated\northeoriginalone.\nERNIE3.0Titan\n• ThisdistinctionabilitybetweenrealandgeneratetextimprovestheLLM’sperformanceascomparedtoERNIE3.0.", "metadata": {"id": "a7e63fe22ed5b47397710dc13ceedd534e4225e2", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 13, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "• ThisdistinctionabilitybetweenrealandgeneratetextimprovestheLLM’sperformanceascomparedtoERNIE3.0.\n• Parallelattention+FFlayersspeed-uptraining15%withthesameperformanceaswithcascadedlayers\n• Initializingfeed-forwardoutputlayersbeforeresidualswithschemein[131]avoidsactivationsfromgrowingwithincreasing\nGPT-NeoX-20B depthandwidth\n• TrainingonPileoutperformsGPT-3onfive-shot", "metadata": {"id": "558aef70370588b2e3ba1bf04fd5fc6c240874ea", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 13, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "GPT-NeoX-20B depthandwidth\n• TrainingonPileoutperformsGPT-3onfive-shot\n• Restarttrainingfromanearliercheckpointwithalowerlearningrateiflossdiverges\nOPT • Modelispronetogeneraterepetitivetextandstuckinaloop\nBLOOM • None\n• Galactica’s performance has continued to improve across validation set, in-domain, and out-of-domain benchmarks, even", "metadata": {"id": "ed68805443fd29c87930d61d5cc5913b15a2c8dd", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 13, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "withmultiplerepetitionsofthecorpus,whichissuperiortoexistingresearchonLLMs.\nGalactica • A working memory token approach can achieve strong performance over existing methods on mathematical MMLU and\nMATHbenchmarks.Itsetsanewstate-of-the-artonseveraldownstreamtaskssuchasPubMedQA(77.6%)andMedMCQA\ndev(52.9%).", "metadata": {"id": "b9190d756939f28efd7f4137ce3308b057d8c6bb", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 13, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "dev(52.9%).\n• Thefeed-forwardcomponentofeachTransformerlayercanbereplacedwithamixture-of-experts(MoE)moduleconsisting\nofasetofindependentfeed-forwardnetworks(i.e.,the‘experts’).Bysparselyactivatingtheseexperts,themodelcapacity\ncanbemaintainedwhilemuchcomputationissaved.\n• By leveraging sparsity, we can make significant strides toward developing high-quality NLP models while simultaneously", "metadata": {"id": "776fc1e0d43770fdc155f13872289646f69b298a", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 13, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "reducingenergyconsumption.Consequently,MoEemergesasarobustcandidateforfuturescalingendeavors.\nGLaM • ThemodeltrainedonfiltereddatashowsconsistentlybetterperformancesonbothNLGandNLUtasks,wheretheeffectof\nfilteringismoresignificantontheformertasks.\n• FilteredpretrainingcorporaplaysacrucialroleinthegenerationcapabilityofLLMs,especiallyforthedownstreamtasks.", "metadata": {"id": "b7b9cd8d9e78f8103eb1d890866a4b2e1a483686", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 13, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "• ThescalingofGLaMMoEmodelscanbeachievedbyincreasingthesizeornumberofexpertsintheMoElayer.Givena\nfixedbudgetofcomputation,moreexpertscontributetobetterpredictions.\nLaMDA • Themodelcanbefine-tunedtolearntocalldifferentexternalinformationresourcesandtools.\nMT-NLG • None.\n• Forhighereffectivenessandefficiency,atransformermodelcanbeasymmetricallyconstructedwithashallowerencoderand\nadeeperdecoder.", "metadata": {"id": "5354a4815359c15d4be43283e078657572d93685", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 13, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "adeeperdecoder.\n• Toachievebetterperformances,itisnecessarytoemploystrategiessuchasmassivelyscalingupsampling,followedbythe\nAlphaCode filteringandclusteringofsamplesintoacompactset.\n• Theutilizationofnovelsampling-efficienttransformerarchitecturesdesignedtofacilitatelarge-scalesamplingiscrucial.\n• Simplifyingproblemdescriptionscaneffectivelyimprovethemodel’sperformance.\nTableContinuedonNextPage", "metadata": {"id": "dbe066ed1b4c6685f5f96ac6b163a47a62182bba", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 13, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "JOURNALOFLATEX 13\nModels Findings&Insights\n• TheexperimentsthatculminatedinthedevelopmentofChinchilladeterminedthatforoptimalcomputationduringtraining,\nthemodelsizeandthenumberoftrainingtokensshouldbescaledproportionately:foreachdoublingofthemodelsize,the\nChinchilla\nnumberoftrainingtokensshouldbedoubledaswell.", "metadata": {"id": "a40dc6f30a47514f36a77d5e6f04bf3549daefa3", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 14, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "Chinchilla\nnumberoftrainingtokensshouldbedoubledaswell.\n• English-centricmodelsproducebettertranslationswhentranslatingtoEnglishascomparedtonon-English\n• Generalizedmodelscanhaveequivalentperformanceforlanguagetranslationtospecializedsmallmodels\nPaLM • Largermodelshaveahigherpercentageoftrainingdatamemorization", "metadata": {"id": "fb9fbb5d212bf9a91297a2a5e87158b47102dbdc", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 14, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "PaLM • Largermodelshaveahigherpercentageoftrainingdatamemorization\n• Performancehasnotyetsaturatedevenat540Bscale,whichmeanslargermodelsarelikelytoperformbetter\n• ComparedtocommonlyusedDecoder-onlyTransformermodels,seq2seqarchitectureismoresuitablefortraininggenerative\nLLMsgivenstrongerbidirectionalattentiontothecontext.", "metadata": {"id": "4bc5f374ede944eff998bbd2c6c50eb79ded349f", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 14, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "LLMsgivenstrongerbidirectionalattentiontothecontext.\n• AnextraCausalLanguageModeling(CLM)taskcanbeaddedtobenefitthemodelwithamoreefficientin-contextlearning,\nAlexaTM especiallyforfew-shotlearningtasks.\n• Thekeytotrainingpowerfulseq2seq-basedLLMsliesinmixedpre-training,ratherthanadditionalmultitasktraining.", "metadata": {"id": "63e23de95fa1d5334cb3875b66fe7a819beab141", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 14, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "• Placinglayernormsatthebeginningofeachtransformerlayercanimprovethetrainingstabilityoflargemodels.\n• TrainingwithamixtureofdenoisersoutperformsPaLMwhentrainedfurtherforafewmoreFLOPs\nU-PaLM • Trainingwithamixtureofdenoisersimprovestheinfillingabilityandopen-endedtextgenerationdiversity\n• Modeswitchingtrainingenablesbetterperformanceondownstreamtasks", "metadata": {"id": "3e7eefeb1a83b7653c372dcd5ad9d3cb9dbd16c3", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 14, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "• Modeswitchingtrainingenablesbetterperformanceondownstreamtasks\nUL2 • CoTpromptingoutperformsstandardpromptingforUL2\nGLM-130B • Pre-trainingdatawithasmallproportionofmulti-taskinstructiondataimprovestheoverallmodelperformance\nCodeGen • Multi-steppromptingforcodesynthesisleadstoabetteruserintentunderstandingandcodegeneration", "metadata": {"id": "030ceacdf4c03eb53f92f5765186713eed98dc0a", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 14, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "CodeGen • Multi-steppromptingforcodesynthesisleadstoabetteruserintentunderstandingandcodegeneration\n• LLaMAisopen-sourceandcanbefine-tunedorcontinuallypre-trainedtodevelopnewmodelsorinstruction-basedtools.\n• AfewoptimizationsareproposedtoimprovethetrainingefficiencyofLLaMA,suchasefficientimplementationofmulti-head\nself-attentionandareducedamountofactivationsduringback-propagation.", "metadata": {"id": "fd57d149ff5a96015f02b5ec91daf85685816e30", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 14, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "self-attentionandareducedamountofactivationsduringback-propagation.\nLLaMA • Trainingexclusivelyonpublicdatacanalsoachievestate-of-the-artperformance.\n• Aconstantperformanceimprovementisgainedwhenscalingthemodel.\n• Smallermodelscanalsorealizegoodperformancesusingmoretrainingdataandtime.\n• Sparsemodelsprovidethebenefitsoflargemodelsatalowercomputationcost", "metadata": {"id": "f3f771774a89e0b78df07d4ac18f12ed8cd00803", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 14, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "• Sparsemodelsprovidethebenefitsoflargemodelsatalowercomputationcost\n• RandomlyRoutedExpertsreducescatastrophicforgettingeffectswhichinturnisessentialforcontinuallearning\nPanGu-Σ • Randomly Routed Experts allow extracting a domain-specific sub-model in deployment which is cost-efficient while\nmaintainingaperformancesimilartotheoriginal", "metadata": {"id": "34c86fc45cd6499df615e47e7d9dcb94da034f5c", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 14, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "maintainingaperformancesimilartotheoriginal\nBloombergGPT • Pre-trainingwithgeneral-purposeandtask-specificdataimprovestaskperformancewithouthurtingothermodelcapabilities\nXuanYuan2.0 • Combiningpre-trainingandfine-tuningstagesinsingletrainingavoidscatastrophicforgetting\n• CausalLMiscrucialforamodel’sgenerationcapabilityinencoder-decoderarchitectures", "metadata": {"id": "354d1fb70a7c1cfb05cbe6b04a48ff7a9985f886", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 14, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "• CausalLMiscrucialforamodel’sgenerationcapabilityinencoder-decoderarchitectures\nCodeT5+ • Multipletrainingobjectiveslikespancorruption,CausalLM,matching,etccomplementeachotherforbetterperformance\nStarCoder • HHHpromptbyAnthropicallowsthemodeltofollowinstructionswithoutfine-tuning\n• Modeltrainedonunfiltereddataismoretoxicbutmayperformbetterondownstreamtasksafterfine-tuning", "metadata": {"id": "fe3b64fcd9ed949035713fb2443bd03f79ffa6da", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 14, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "• Modeltrainedonunfiltereddataismoretoxicbutmayperformbetterondownstreamtasksafterfine-tuning\nLLaMA-2 • Modeltrainedonunfiltereddatarequiresfewersamplesforsafetyalignment\n• Dataqualityisimportanttotrainbettermodels\nPaLM-2 • Modelanddatasizeshouldbescaledwith1:1proportions\n• Smallermodelstrainedforlargeriterationsoutperformlargermodels", "metadata": {"id": "68c70a0a69ca72a79c0726ab17daadd079b62743", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 14, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "• Smallermodelstrainedforlargeriterationsoutperformlargermodels\nWereviewvariousfine-tunedLLMsandstrategiesforeffective shot performance improves significantly by expanding task\nfine-tuning in this section. collection and prompt styles. OPT-IML [24] and Flan [25]\ncurated larger 2k and 1.8k task datasets, respectively. While\n1. Instruction-Tuning with Manually Created Datasets:", "metadata": {"id": "74d68b23fd35ec89527ad534fb7a2e322659856c", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 14, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "1. Instruction-Tuning with Manually Created Datasets:\nincreasing task size alone is not enough, OPT-IML and\nNumerous hand-crafted instruction-tuning datasets with\nFlan add more prompting setups in their datasets, zero-shot,\ndifferent design choices are proposed in the literature to\nfew-shot, and CoT. In continuation, CoT Collection [80]\ninstruction-tune LLMs. The performance of fine-tuned LLMs", "metadata": {"id": "13b1a95ed6cb438b38be993fa78a3e0f1cbf49eb", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 14, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "instruction-tune LLMs. The performance of fine-tuned LLMs\nfine-tunes Flan-T5 further on 1.88M CoT samples. Another\ndepends on multiple factors, such as dataset, instruction\nmethod [81] uses symbolic tasks with tasks in T0, Flan, etc.\ndiversity, prompting templates, model size, and training\nobjectives. Keeping this in view, diverse fine-tuned models", "metadata": {"id": "c81c534f5d98001beabbdc7e8d2decc84da26b11", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 14, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "objectives. Keeping this in view, diverse fine-tuned models\nhaveemergedintheliteratureusingmanuallycreateddatasets. 2. Instruction-Tuning with LLMs Generated Datasets:\nThe models T0 [22] and mT0 (multi-lingual) [134] employ Generating an instruction-tuning dataset requires carefully", "metadata": {"id": "0be111f066aaee30b2e6fe196e1f02680a5a5c6a", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 14, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "templates to convert existing datasets into prompt datasets. writing instructions and input-output pairs, which are often\nTheyhaveshownimprovementsingeneralizationtozero-shot written by humans, smaller in size, and less diverse. To\nand held-out tasks. Tk-Instruct [26] fine-tuned the T5 model overcome this, self-instruct [138] proposed an approach to", "metadata": {"id": "2444cbb9dc8c72bb7c23a269ede8ff931db40a7b", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 14, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "with in-context instructions to study generalization on unseen promptavailableLLMstogenerateinstruction-tuningdatasets.\ntasks when given in-context instructions during test time. The Self-instructoutperformedmodelstrainedonmanuallycreated\nmodel outperformed Instruct-GPT, despite being smaller in dataset SUPER-NATURALINSTRUCTIONS (a dataset with", "metadata": {"id": "af381ac00f78539021e997eccf39e1df8493bb03", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 14, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "size, i.e., 11B parameters as compared to 175B of GPT-3. 1600+ tasks) [26] by 33%. It starts with a seed of 175 tasks,\nIncreasing Tasks and Prompt Setups: Zero-shot and few- 1 instruction, and 1 sample per task and iteratively generates", "metadata": {"id": "d980adeb8e60112e00fa7006850fe06647724d1c", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 14, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "JOURNALOFLATEX 14\nTABLE II: Key insights and findings from the study of instruction-tuned Large Language Models.\nModels Findings&Insights\n• Multi-taskpromptingenableszero-shotgeneralizationandoutperformsbaselines\nT0 • Evenasinglepromptperdatasettaskisenoughtoimproveperformance\n• TheanswerqualityofLLMscanbefurtherimprovedwithhumanfeedback.", "metadata": {"id": "b53625354d53b4c0c9d04b7e968443fed653413e", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 15, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "• TheanswerqualityofLLMscanbefurtherimprovedwithhumanfeedback.\n• Toaidthemodelineffectivelyfilteringandutilizingrelevantinformation,humanlabelersplayacrucialroleinanswering\nquestionsregardingtheusefulnessoftheretrieveddocuments.\nWebGPT • Interactingafine-tunedlanguagemodelwithatext-basedweb-browsingenvironmentcanimproveend-to-endretrievaland\nsynthesisviaimitationlearningandreinforcementlearning.", "metadata": {"id": "e1dc3e5d8532b00d8209427ebc5bc791c0604614", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 15, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "synthesisviaimitationlearningandreinforcementlearning.\n• Generatinganswerswithreferencescanmakelabelerseasilyjudgethefactualaccuracyofanswers.\n• Instructiontuningleadstoastrongergeneralizationofunseentasks\n• Moretasksimprovegeneralizationwhereasonlyincreasingtaskinstancesdoesnothelp\nTk-INSTRUCT • Supervisedtrainedmodelsarebetterthangeneralizedmodels", "metadata": {"id": "abe4d4f2f52c6c014bc0b344df447298c8100c7f", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 15, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "Tk-INSTRUCT • Supervisedtrainedmodelsarebetterthangeneralizedmodels\n• Modelspre-trainedwithinstructionsandexamplesperformwellfordifferenttypesofinputs\n• Instructiontuningenableszero-shotgeneralizationtothetasksneverseenbefore\n• Multi-lingualtrainingleadstoevenbetterzero-shotgeneralizationforbothEnglishandnon-English", "metadata": {"id": "44593a3cbad09eabb8cfbff3cdeb39c3bbd40a83", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 15, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "• Multi-lingualtrainingleadstoevenbetterzero-shotgeneralizationforbothEnglishandnon-English\nmT0andBLOOMZ • Trainingonmachine-translatedpromptsimprovesperformanceforheld-outtaskswithnon-Englishprompts\n• English only fine-tuning on multilingual pre-trained language model is enough to generalize to other pre-trained language\ntasks", "metadata": {"id": "f582ca03ea773c4a80745985a94f5099b59931b7", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 15, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "tasks\n• Tasksizesamplingtocreateabatchwithmostofthetaskexamplesisimportantforbetterperformance\n• Only example proportional sampling is not enough, training datasets/benchmarks should also be proportional for better\ngeneralization/performance\n• Fullyheld-outandpartiallysupervisedtasksperformanceimprovesbyscalingtasksorcategorieswhereasfullysupervised\nOPT-IML taskshavenoeffect", "metadata": {"id": "aeb805ee12d591b2152dbd69c7046f068437de28", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 15, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "OPT-IML taskshavenoeffect\n• Includingsmallamountsi.e.5%ofpretrainingdataduringfine-tuningiseffective\n• Only1%reasoningdataimprovestheperformance,addingmoredeterioratesperformance\n• Addingdialoguedatamakestheperformanceworse\n• FinetuningwithCoTimprovesperformanceonheld-outtasks\n• Fine-tuningalongwithCoTdataimprovesreasoningabilities\n• CoTtuningimproveszero-shotreasoning", "metadata": {"id": "92076d37c380670c6c5c1525d8989195fd3c334e", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 15, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "• Fine-tuningalongwithCoTdataimprovesreasoningabilities\n• CoTtuningimproveszero-shotreasoning\nFlan • Performanceimproveswithmoretasks\n• Instructionfine-tuningimprovesusabilitywhichotherwiseischallengingforpre-trainedmodels\n• Improvingthemodel’sperformancewithinstructiontuningiscompute-efficient\n• Multitaskpromptingenableszero-shotgeneralizationabilitiesinLLM", "metadata": {"id": "a034a380613b438b9a14b4aa22aef21f61591cfa", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 15, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "• Multitaskpromptingenableszero-shotgeneralizationabilitiesinLLM\n• Thejudgmentsoflabelersandthealignmentswithdefinedrulescanhelpthemodelgeneratebetterresponses.\n• Gooddialoguegoalscanbebrokendownintodetailednaturallanguagerulesfortheagentandtheraters.\nSparrow • Thecombinationofreinforcementlearning(RL)withrerankingyieldsoptimalperformanceintermsofpreferencewinrates", "metadata": {"id": "87132fff84debb35fba0c7fc45c91fb446b91677", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 15, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "andresilienceagainstadversarialprobing.\nWizardCoder • Fine-tuningwithre-writteninstruction-tuningdataintoacomplexsetimprovestheperformancesignificantly\n• Modellearnstowritesaferesponseswithfine-tuningonsafedemonstrations,whileadditionalRLHFstepfurtherimproves\nLLaMA-2-Chat modelsafetyandmakeitlesspronetojailbreakattacks\nLLaMA [140] with GPT-3 [8] or GPT-4 [141] generated", "metadata": {"id": "c0b6af9734699144a09212e6692b28fe5a3b6a54", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 15, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "LLaMA [140] with GPT-3 [8] or GPT-4 [141] generated\ndatasets. Among these, Alpaca, Vicuna, and LLaMA-GPT-4\nare a few general-purpose fine-tuned models, where Alpaca\nis trained on 52k samples from text-davinci-003, Vicuna\non 70k samples from ShareGPT.com, and LLaMA-GPT-4\nby re-creating Alpaca instructions from GPT-4. Goat [142]\nfine-tunes LLaMA for arithmetic tasks (1 million samples)", "metadata": {"id": "5b4c9066a25fcd90bff2a707c07dfbb71f8a394f", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 15, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "fine-tunes LLaMA for arithmetic tasks (1 million samples)\nby generating data from ChatGPT and outperforms GPT-4,\nPaLM, BLOOM, OPT, etc, attributing its success to the\nLLaMA’s consistent tokenization of numbers. HuaTuo [143]\nis a medical knowledge model, fine-tuned with a generated\nFig. 11: An example image shows an instance of the Flan QA dataset of 8k instructions.", "metadata": {"id": "4cf078805c673734e23b4d4e60c9682258512a01", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 15, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "Fig. 11: An example image shows an instance of the Flan QA dataset of 8k instructions.\ntraining paradigm, taken from [25]. Complex Instructions Evol-Instruct [144], [136] prompts\nLLMs to convert given instructions into a more complex\nset. The instructions are iteratively evolved with re-\nwriting instructions in complex wording and creating", "metadata": {"id": "38010daa8711bfcd12fa40e5faabdd193fcc29df", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 15, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "writing instructions in complex wording and creating\nnew instructions (52k) and instances (82k input-output pairs)\nnew instructions. With this style of automated instruction\nusing GPT-3 [8]. Contrary to this, Dynosaur [139] uses the\ngeneration, WizardLM [144] (fine-tuned LLaMA on\nmeta-data of datasets on Huggingface to prompt LLMs to\n250k instructions), outperforms Vicuna and Alpaca, and", "metadata": {"id": "5f0e2051c957e28b4a73606df34275a43a6bbc17", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 15, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "250k instructions), outperforms Vicuna and Alpaca, and\ngenerate multiple task instruction-tuning datasets.\nWizardCoder [136] (fine-tuned StarCoder) beats Claude-Plus,\nLLaMA Tuned Various models in literature instruction-tune", "metadata": {"id": "042b9df67592fb8eb0c326358ee6496c968c268e", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 15, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "JOURNALOFLATEX 15\nTABLE III: Summary of pre-trained LLMs. Only the LLMs discussed individually in the previous sections are summarized.\n“Data/Tokens” is the model’s pre-training data which is either the number of tokens or data size. “Data Cleaning” indicates\nwhether the data cleaning is performed or not. This includes heuristics (Heur), deduplication (Dedup), quality filtering (QF),", "metadata": {"id": "d0685785c7579ada010264cad21997261220837c", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 16, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "and privacy filtering (PF), “Cost” is the calculated training cost obtained by multiplying the GPUs/TPUs hourly rate with the\nnumber of GPUs and the training time. The actual cost may vary due to many reasons such as using in-house GPUs or getting\na discounted rate, re-training, number of employees working on the problem, etc. “Training Parallelism” indicates distributed", "metadata": {"id": "eb5520bdc7c502fe5dec57b1492dc0a0204c096d", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 16, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "trainingusingdataparallelism(D),tensorparallelism(T),pipelineparallelism(P),modelparallelism(M),optimizerparallelism\n(OP), and rematerialization (R), where for “Library” column, “DS” is a short form for Deep Speed. In column “Commercial\nUse”, we assumed a model is for non-commercial purposes if its license is not available.", "metadata": {"id": "90ff9660d60c8ee5b3d7a630be6fa416ebd56da0", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 16, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "Use”, we assumed a model is for non-commercial purposes if its license is not available.\nPublication License Model No.of Commercial Steps Data/ Data No.of ProcessingTraining Calculated Training\nModels\nVenue Type Creators Purpose Params Use Trained Tokens Cleaning ProcessingUnits UnitType Time Train.Cost Parallelism Library", "metadata": {"id": "8548d2f69d5be92d79a411e9e02a67d1b937f593", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 16, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "T5[11] JMLR'20 Apache-2.0 Google General 11B ✓ 1M 1T Heur+Dedup 1024 TPUv3 - - D+M MeshTensorFlow\nGPT-3[8] NeurIPS'20 - OpenAI General 175B × - 300B Dedup+QF - V100 - - M -\nmT5[12] NAACL'21 Apache-2.0 Google General 13B ✓ 1M 1T - - - - - - -\nPanGu-α[90] arXiv'21 Apache-2.0 Huawei General 200B ✓ 260k 1.1TB Heur+Dedup 2048 Ascend910 - - D+OP+P+O+R MindSpore", "metadata": {"id": "404199bd4ee892b61b6818f2e8584425b0927d64", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 16, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "CPM-2[13] AIOpen'21 MIT Tsinghua General 198B ✓ 1M 2.6TB Dedup - - - - D+M JAXFormer\nCodex[117] arXiv'21 - OpenAI Coding 12B × - 100B Heur - - - - - -\nERNIE3.0[92] arXiv'21 - Baidu General 10B × 120k∗ 375B Heur+Dedup 384 V100 - - M∗ PaddlePaddle\nJurassic-1[94] White-Paper'21Apache-2.0 AI21 General 178B ✓ - 300B - 800 GPU - - D+M+P Megatron+DS", "metadata": {"id": "7fd4bf12238c12a9a87009eb345c04f2834d8bab", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 16, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "Jurassic-1[94] White-Paper'21Apache-2.0 AI21 General 178B ✓ - 300B - 800 GPU - - D+M+P Megatron+DS\nHyperCLOVA[96] EMNLP'21 - Naver General 82B × - 300B Clf+Dedup+PF 1024 A100 321h 1.32Mil M Megatron\nYuan1.0[97] arXiv'21 Apache-2.0 - General 245B ✓ 26k∗ 180BHeur+Clf+Dedup 2128 GPU - - D+T+P -\nGopher[98] arXiv'21 - Google General 280B × - 300B QF+Dedup 4096 TPUv3 920h 13.19Mil D+M JAX+Haiku", "metadata": {"id": "8bc084b8e5bd83392a19260fefc7ce93ae945b4c", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 16, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "Gopher[98] arXiv'21 - Google General 280B × - 300B QF+Dedup 4096 TPUv3 920h 13.19Mil D+M JAX+Haiku\nERNIE3.0Titan[99]arXiv'21 - Baidu General 260B × - 300B Heur+Dedup - Ascend910 - - D+M+P+D* PaddlePaddle\nGPT-NeoX-20B[132]BigScience'22 Apache-2.0 EleutherAI General 20B ✓ 150k 825GB None 96 40GA100 - - M Megatron+DS+PyTorch", "metadata": {"id": "59b616594bab76308946ddb3c38725531c91da57", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 16, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "OPT[10] arXiv'22 MIT Meta General 175B ✓ 150k 180B Dedup 992 80GA100 - - D+T Megatron\nBLOOM[9] arXiv'22 RAIL-1.0 BigScience General 176B ✓ - 366B Dedup+PR 384 80GA100 2520h 3.87Mil D+T+P Megatron+DS\nGalactica[125] arXiv'22 Apache-2.0 Meta Science 120B × 225k 106B Dedup 128 80GBA100 - - - Metaseq\nGLaM[103] ICML'22 - Google General 1.2T × 600k∗ 600B Clf 1024 TPUv4 - - M GSPMD", "metadata": {"id": "360dc8c1b965a42017b32e724489cea8799896ad", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 16, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "GLaM[103] ICML'22 - Google General 1.2T × 600k∗ 600B Clf 1024 TPUv4 - - M GSPMD\nLaMDA[127] arXiv'22 - Google Dialog 137B × 3M 2.81T Filtered 1024 TPUv3 1384h 4.96Mil D+M Lingvo\nMT-NLG[21] arXiv'22 Apache-v2.0MS.+Nvidia General 530B × - 270B - 4480 80GA100 - - D+T+P Megatron+DS\nAlphaCode[118] Science'22 Apache-v2.0 Google Coding 41B ✓ 205k 967B Heur+Dedup - TPUv4 - - M JAX+Haiku", "metadata": {"id": "2af223fd67fbb9e737459cc6ffc6de6b5329aacd", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 16, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "Chinchilla[106] arXiv'22 - Google General 70B × - 1.4T QF+Dedup - TPUv4 - - - JAX+Haiku\nPaLM[14] arXiv'22 - Google General 540B × 255k 780B Heur 6144 TPUv4 - - D+M JAX+T5X\nAlexaTM[107] arXiv'22 Apachev2.0 Amazon General 20B × 500k 1.1T Filtered 128 A100 2880h 1.47Mil M DS\nU-PaLM[20] arXiv'22 - Google General 540B × 20k - - 512 TPUv4 120h 0.25Mil - -", "metadata": {"id": "aabca4fb712578ecd31f135b76b33efcb72fdc28", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 16, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "U-PaLM[20] arXiv'22 - Google General 540B × 20k - - 512 TPUv4 120h 0.25Mil - -\nUL2[15] ICLR'23 Apache-2.0 Google General 20B ✓ 2M 1T - 512 TPUv4 - - M JAX+T5X\nGLM[109] ICLR'23 Apache-2.0 Multiple General 130B × - 400B - 768 40GA100 1440h 3.37Mil M -\nCodeGen[116] ICLR'23 Apache-2.0 Salesforce Coding 16B ✓ 650k 577B Heur+Dedup - TPUv4 - - D+M JAXFormer", "metadata": {"id": "4457d758dff03f045fa1758bcf9334064e5c374f", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 16, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "LLaMA[111] arXiv'23 - Meta General 65B × 350k 1.4T Clf+Heur+Dedup 2048 80GA100 504h 4.12Mil D+M xFormers\nPanGuΣ[115] arXiv'23 - Huawei General 1.085T × - 329B - 512 Ascend910 2400h - D+OP+P+O+R MindSpore\nBloombergGPT[128]arXiv23 - Bloomberg Finance 50B × 139k 569B Dedup 512 40GA100 1272h 1.97Mil M PyTorch", "metadata": {"id": "1a465264bba3a71fa0acc6834acc0d745b1bc76b", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 16, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "XuanYuan2.0[130] arXiv23 RAIL-1.0 DuXiaoman Finance 176B ✓ - 366B Filtered 80GB A100 - - P DS\nCodeT5+[122] arXiv'23 BSD-3 Salesforce Coding 16B ✓ 110k 51.5B Dedup 16 40GA100 - - - DS\nStarCoder[124] arXiv'23 OpenRAIL-M BigCode Coding 15.5B ✓ 250k 1T Dedup+QF+PF 512 80GA100 624h 1.28Mil D+T+P Megatron-LM", "metadata": {"id": "9d997c99f48e8710ce6c12dd6bdc13ec08a6186c", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 16, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "LLaMA-2[112] arXiv'23 LLaMA-2.0 Meta General 70B ✓ 500k 2T MinimalFiltering - 80GA100 1.7Mh - - -\nPaLM-2[108] arXiv'23 - Google General - × - - Ddedup+PF+QF - - - - - -\nTABLEIV:SummaryofinstructiontunedLLMs.AllabbreviationsarethesameasTableIII.Entriesin“Data/Tokens”starting\nwith “S-” represents the number of training samples.", "metadata": {"id": "4386415ede9ff1c424a987fd26f4cb970125fdc4", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 16, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "with “S-” represents the number of training samples.\nPublication License Model No.of Commercial Pre-trained Steps Data/ No.of ProcessingTrain. Calculated Train.\nModels\nVenue Type Creators Purpose Params Use Models TrainedTokens ProcessingUnits UnitType Time Train.Cost Parallelism Library\nWebGPT[133] arXiv'21 - OpenAI General 175B × GPT-3 - - - - - - - -", "metadata": {"id": "ceaaa385f1a2a7810d07aa8d21a6316193b95f1e", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 16, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "WebGPT[133] arXiv'21 - OpenAI General 175B × GPT-3 - - - - - - - -\nT0[22] ICLR'22 Apache-2.0 BigScience General 11B ✓ T5 - 250B 512 TPUv3 270h 0.48Mil - -\nTk-Instruct[26] EMNLP'22 MIT AI2+ General 11B ✓ T5 1000 - 256 TPUv3 4h 0.0036Mil - GoogleT5\nOPT-IML[24] arXiv'22 - Meta General 175B × OPT 8k 2B 128 40GA100 - - D+T Megatron", "metadata": {"id": "8c62baea9f860622fd808b27997eaa13d38b6725", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 16, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "OPT-IML[24] arXiv'22 - Meta General 175B × OPT 8k 2B 128 40GA100 - - D+T Megatron\nFlan-U-PaLM[25] ICLR'22 Apache-2.0 Google General 540B ✓ U-PaLM 30k - 512 TPUv4 - - - JAX+T5X\nmT0[134] ACL'23 Apache-2.0HuggingFace+ General 13B ✓ mT5 - - - - - - - -\nSparrow[135] arXiv'22 - Google Dialog 70B × Chinchilla - - 64 TPUv3 - - M -", "metadata": {"id": "41a9499b4fa74f21fc6febae4e16c7ea6d0817cf", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 16, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "Sparrow[135] arXiv'22 - Google Dialog 70B × Chinchilla - - 64 TPUv3 - - M -\nWizardCoder[136]arXiv'23 Apache-2.0 HKBapt. Coding 15B × StarCoder 200 S-78k - - - - - -\nBard, and others. modeling into helpfulness and safety rewards and using\nrejection sampling in addition to PPO. The initial four\nversions of LLaMA 2-Chat are fine-tuned with rejection", "metadata": {"id": "d73e5b751a2dc1b7d31d69dc890708fa769400ef", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 16, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "versions of LLaMA 2-Chat are fine-tuned with rejection\nsampling and then with PPO on top of rejection sampling.\n3. Aligning with Human Preferences: Incorporating\nAligning with Supported Evidence: This style of alignment\nhuman preferences into LLMs presents a significant\nallows the model to generate responses with proofs and facts,\nadvantage in mitigating undesirable behaviors and ensuring", "metadata": {"id": "e70ade6705c979a0f398c0aa8a763c3adc0472c8", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 16, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "advantage in mitigating undesirable behaviors and ensuring\nreduces hallucination, and assists humans more effectively,\naccurate outputs. The initial work on alignment, such as\nwhich increases trust in the model’s output. Similar to the\nInstructGPT [137] aligns GPT-3 using a 3-step approach,\nRLHF training style, a reward model is trained to rank", "metadata": {"id": "b67fee3756b8fb9bbe11efb4f5b3270ecf46e8ff", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 16, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "RLHF training style, a reward model is trained to rank\ninstruction-tuning, reward modeling, and fine-tuning with\ngenerated responses containing web citations in answers\nreinforcement learning (RL). The supervised fine-tuned\nto questions, which is later used to train the model, as in\nGPT-3 on demonstrations is queried to generate responses,\nGopherCite [145], WebGPT [133], and Sparrow [135]. The", "metadata": {"id": "4a5e1440eb8d030ff09014f0466d335f0010151c", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 16, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "GopherCite [145], WebGPT [133], and Sparrow [135]. The\nwhich human labelers rank according to human values, and\nranking model in Sparrow [135] is divided into two branches,\na reward model is trained on the ranked data. Lastly, the\npreference reward and rule reward, where human annotators\nGPT-3 is trained with proximal policy optimization (PPO)", "metadata": {"id": "eebaa717b85825930d765f72ecc1df0e9a5beaa6", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 16, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "GPT-3 is trained with proximal policy optimization (PPO)\nadversarial probe the model to break a rule. These two\nusing rewards on the generated data from the reward model.\nrewards together rank a response to train with RL.\nLLaMA 2-Chat [112] improves alignment by dividing reward", "metadata": {"id": "c8b926808248bad4c93d4c4faca2839c427ed732", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 16, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "JOURNALOFLATEX 16\nAligning Directly with SFT: The PPO in the RLHF pipeline data is small and the original capacity is to be maintained.\nis complex, memory-intensive, and unstable, requiring Prompt-basedcontinuedpre-training(PCP)[161]trainsmodel\nmultiple models, reward, value, policy, and reference with text and instructions related to tasks and then finally", "metadata": {"id": "8c0377b426d15b79534dd49ff0733ebac55353b0", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 17, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "models. Avoiding this sophisticated alignment pipeline is instruction-tunes the model for downstream tasks.\npossible by incorporating minimal changes in supervised 5. Sample Efficiency: While fine-tuning data is generally\nfine-tuning (SFT) pipeline as in [146], [147], [148], with many-fold smaller than the pre-training data, it still has to", "metadata": {"id": "08fdab273880c2e8f57ae25d19e253970379972f", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 17, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "better or comparable performance to PPO. Direct preference be large enough for acceptable performance [25], [24], [26]\noptimization (DPO) [146] trains a model directly on the and requires proportional computing resources. To study the\nhuman-preferred responses to maximize the likelihood of effectsonperformancewithlessdata,existingliterature[162],", "metadata": {"id": "fe19c3f247bc69fc9d29b9eadde885f808a048ef", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 17, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "preferred against unpreferred responses, with per-sample [163] finds that the models trained on lesser data can out-\nimportance weight. Reward ranked fine-tuning RAFT [147] perform models trained with more data. In [162], 25% of\nfine-tunes the model on ranked responses by the reward the total downstream data is found enough for state-of-the-", "metadata": {"id": "fb0a3ea02d6bb95b9da348a81b9baa514d2caab3", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 17, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "model. Preference ranking optimization (PRO) [149] and art performance. Selecting coreset-based 0.5% of the total\nRRHF [148] penalize the model to rank responses with instruction-tuning data improves the model performance by\nhuman preferences and supervised loss. On the other hand, 2% in [163], as compared to the complete data tuning. Less", "metadata": {"id": "3cca798f6da43c23a2e0c9316d22ae2e8009da27", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 17, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "chain-of-hindsight (CoH) [150] provides feedback to the is more for alignment (LIMA) [164] uses only 1000 carefully\nmodel in language rather than reward, to learn good versus createddemonstrationstofine-tunethemodelandhasachieved\nbad responses. comparable performance to GPT-4.\nAligning with Synthetic Feedback: Aligning LLMs with\nC. Robotics", "metadata": {"id": "02d25de19581a5a07fb54f5939bedd793c450d86", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 17, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "Aligning with Synthetic Feedback: Aligning LLMs with\nC. Robotics\nhuman feedback is slow and costly. The literature suggests a\nsemi-automatedprocesstoalignLLMsbypromptingLLMsto LLMshavebeenrapidlyadoptedacrossvariousdomainsin\ngenerate helpful, honest, and ethical responses to the queries, the scientific community due to their multipurpose capabili-", "metadata": {"id": "f6c95d7c519ee8ce4b91f025eb519ecdf62eed56", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 17, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "andfine-tuningusingthenewlycreateddataset.Constitutional ties [33]. In robotics research, the LLMs have very promising\nAI [151] replaces human feedback in RLHF with AI, calling applications as well, such as enhancing human-robot inter-\nit RL from AI feedback (RLAIF). AlpacaFarm [152] designs action [165], [166], [167], [168], task planning [169], [170],", "metadata": {"id": "0bdbc1105c160b47d72d32714c05341c50873666", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 17, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "prompts to imitate human feedback using LLMs APIs. [171], navigation [172], [173], and learning [174], [175].\nOpposite to constitutional AI, AlpacaFarm injects noise They can enable robots to understand and generate natural\nin feedback to replicate human mistakes. Self-Align [153] language,aidingininstructionfollowing,dataannotation,and", "metadata": {"id": "80eacbaba72c8d03ae803e9bee10d36a408833ef", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 17, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "prompts the LLM with ICL examples, instructing the LLM collaborative problem-solving. They can facilitate continuous\nabout what the response should contain to be considered learningbyallowingrobotstoaccessandintegrateinformation\nuseful and ethical. The same LLM is later fine-tuned with the fromawiderangeofsources.Thiscanhelprobotsacquirenew", "metadata": {"id": "ec31a3fe5f4db0f34ba464fb5817426909cc7866", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 17, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "new dataset. skills,adapttochanges,andrefinetheirperformancebasedon\nAligning with Prompts: LLMs can be steered with prompts real-time data.\nto generate desirable responses without training [154], LLMs have also started assisting in simulating environments\n[155]. The self-correction prompting in [155] concatenates for testing and offer potential for innovative research in", "metadata": {"id": "9b20cd42045e33a8c884cb2c10664024a0c3851f", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 17, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "instructions and CoT with questions, guiding the model to robotics,despitechallengeslikebiasmitigationandintegration\nanswer its instruction following strategy to ensure moral complexity. The work in [176] focuses on personalizing robot\nsafety before the actual answer. This strategy is shown to household cleanup tasks. By combining language-based plan-", "metadata": {"id": "9e94e0f179f0c839b80463fbca8a28ec21585674", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 17, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "reduce the harm in generated responses significantly. ningandperceptionwithLLMs,suchthathavingusersprovide\nRed-Teaming/Jailbreaking/Adversarial Attacks: LLMs object placement examples, which the LLM summarizes to\nexhibit harmful behaviors, hallucinations, leaking personal generate generalized preferences, they show that robots can", "metadata": {"id": "8426d5fdee035ca537b23e1e16b976c9bf4e6a8c", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 17, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "information, and other shortcomings through adversarial generalize user preferences from a few examples. An embod-\nprobing. The models are susceptible to generating harmful iedLLMisintroducedin[177],whichemploysaTransformer-\nresponses even though they are aligned for safety [156], based language model where sensor inputs are embedded", "metadata": {"id": "bf53190bd60ea21e8d02ceadfaac854452e512fd", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 17, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "[157]. Red-teaming is a common approach to address alongside language tokens, enabling joint processing to en-\nillicit outputs, where the LLMs are prompted to generate hance decision-making in real-world scenarios. The model\nharmful outputs [157], [158]. The dataset collected through is trained end-to-end for various embodied tasks, achieving", "metadata": {"id": "c5cc0bb094465b0aceb2f9dbf55a06b1e9acd81d", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 17, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "red-teaming is used to fine-tune models for safety. While positive transfer from diverse training across language and\nred-teaming largely relies on human annotators, another vision domains. LLMs have also been explored as zero-shot\nwork [159] red-team LLMs to find prompts that lead to human models for enhancing human-robot interaction.", "metadata": {"id": "1d004461fadecc34ff2dc55590fb962d042d7883", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 17, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "harmful outputs of other LLMs. The study in [165] demonstrates that LLMs, trained on vast\ntext data, can serve as effective human models for certain\n4. Continue Pre-Training: Although fine-tuning boosts a HRI tasks, achieving predictive performance comparable to\nmodel’s performance, it leads to catastrophic forgetting of specialized machine-learning models. However, limitations", "metadata": {"id": "86c86b51173d801cfeddc1a47ebcbec8abba520f", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 17, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "previouslylearnedinformation.Concatenatingfine-tuningdata were identified, such as sensitivity to prompts and difficulties\nwith a few randomly selected pre-training samples in ev- with spatial/numerical reasoning. In another study [178], the\nery iteration avoids network forgetting [160], [130]. This is authors enable LLMs to reason over sources of natural lan-", "metadata": {"id": "9bd38e13c04bdd14b00d2a5de6e1e1c8aa6456cb", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 17, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "also effective in adapting LLMs for cases where fine-tuning guage feedback, forming an “inner monologue” that enhances", "metadata": {"id": "0a58763b2db4b4b498e84ec5b22266857f15fc04", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 17, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "JOURNALOFLATEX 17\ntheir ability to process and plan actions in robotic control language modality and additional modalities, the learnable\nscenarios. They combine LLMs with various forms of textual interface is introduced to connect different modalities from\nfeedback, allowing the LLMs to incorporate conclusions into frozen pre-trained models. Particularly, the learnable interface", "metadata": {"id": "079b529e5e9df7b728321e509e35e3036dcef7b0", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 18, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "their decision-making process for improving the execution of is expected to work in a parameter-efficient tuning manner:\nuserinstructionsindifferentdomains,includingsimulatedand e.g., LLaMA-Adapter [200] applies an efficient transformer-\nreal-world robotic tasks involving tabletop rearrangement and based adapter module for training, and LaVIN [199] dynam-", "metadata": {"id": "b496963ee6a01b2e46cb8523056d6a6f34bd3423", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 18, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "mobilemanipulation.AllofthesestudiesemployLLMsasthe ically learns the multimodal feature weights using a mixture-\ncoremechanismforassimilatingeverydayintuitiveknowledge of-modalityadapter.Differentfromthelearnableinterface,the\ninto the functionality of robotic systems. expert models can directly convert multimodalities into lan-\nguage:e.g.,VideoChat-Text[182]incorporatesWhisper[201],", "metadata": {"id": "083e13f3e0d351c1b8ad2e0e22f77d6e7c7262af", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 18, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "guage:e.g.,VideoChat-Text[182]incorporatesWhisper[201],\nD. Multimodal LLMs aspeechrecognitionexpertmodel,togeneratethecaptionsof\nInspired by the success of LLMs in natural language pro- given videos for the understanding of following LLMs.\ncessing applications, an increasing number of research works Prompting Different from the fine-tuning technique that", "metadata": {"id": "e9e493672937b23b5d19d755483f105d968f046a", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 18, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "are now facilitating LLMs to perceive different modalities directly updates the model parameters given task-specific\nof information like image [179], [180], [181], video [182], datasets, the prompting technique provides certain context,\n[183],[184],audio[185],[184],[186],etc.MultimodalLLMs examples, or instructions to the model, fulfilling specialized", "metadata": {"id": "161d9eb419ea9652b4775a933b54b56a571772f3", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 18, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "(MLLMs) present substantial benefits compared to standard taskswithoutchangingthemodelparameters.Sinceprompting\nLLMs that process only text. By incorporating information can significantly reduce the needs of large-scale multimodal\nfrom various modalities, MLLMs can achieve a deeper un- data, this technique is widely used to construct MLLMs.", "metadata": {"id": "a89c3216b5c7c7c7566b3cfc02b2df0da971b62b", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 18, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "derstanding of context, leading to more intelligent responses Particularly, to solve multimodal Chain of Thought (CoT)\ninfused with a variety of expressions. Importantly, MLLMs problems [85], LLMs are prompted to generate both the rea-\nalign closely with human perceptual experiences, leveraging soningprocessandtheanswergivenmultimodalinputs[202].", "metadata": {"id": "f6ea79d816d02eac321b4cb18fdd68233e493227", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 18, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "the synergistic nature of our multisensory inputs to form On this front, different learning paradigms are exploited in\na comprehensive understanding of the world [186], [177]. practice: for example, Multimodal-CoT [202] involves two\nCoupled with a user-friendly interface, MLLMs can offer stagesofrationalegenerationandanswerinference,wherethe", "metadata": {"id": "f1aaae13ec5260a196f686ef3699ecfb45f48628", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 18, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "intuitive, flexible, and adaptable interactions, allowing users inputofthesecondstageisacombinationoftheoriginalinput\nto engage with intelligent assistants through a spectrum of and the output of the first stage; and CoT-PT [203] applies\ninputmethods.Accordingtothewaysofconstructingmodels, bothprompttuningandspecificvisualbiastogenerateachain", "metadata": {"id": "2f8cf8a88f3f8c8688c37db83fbd7f3d0108942b", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 18, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "current MLLMs can be generally divided into three streams: of reasoning implicitly. In addition to CoT problems, LLMs\npre-training, fine-tuning, and prompting. In this section, we can also be prompted with multimodal descriptions and tools,\nwilldiscussmoredetailsofthesemainstreams,aswellasthe effectively dividing complex tasks into sub-tasks [204], [205].", "metadata": {"id": "ee3cb2636a3e8554190bfeb9e54d7f546a6f0741", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 18, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "important application of MLLMs in visual reasoning. Visual Reasoning Application Recent visual reasoning sys-\nPre-trainingThisstreamofMLLMsintendstosupportdiffer- tems [206], [207], [208], [209] tend to apply LLMs for better\nent modalities using unified end-to-end models. For instance, visual information analysis and visual-language integration.", "metadata": {"id": "1559cb00339a779309d321bd32938922fc2ff13b", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 18, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "Flamingo[179]appliesgatedcross-attentiontofusevisionand Differentfrompreviousworks[210],[211]thatrelyonlimited\nlanguage modalities, which are collected from pre-trained and VQA datasets and small-scale neural networks, current LLM-\nfrozenvisualencoderandLLM,respectively.Moreover,BLIP- aidedmethodsofferbenefitsofstrongergeneralizationability,", "metadata": {"id": "61ea0e0cafae809d82521e723e94190f61c502a2", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 18, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "2 [180] proposes a two-stage strategy to pre-train a Querying emergent ability, and interactivity [194]. To realize visual\nTransformer (Q-Former) for the alignment between vision reasoning with the help of LLMs, the prompting and the fine-\nand language modalities: in the first stage, vision-language tuning techniques can also be utilized: for example, PointClip", "metadata": {"id": "0db534cf9427ffc1957ea1fcebd8a2c05c9f6b4d", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 18, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "representation learning is bootstrapped from a frozen visual V2 [207] applies LLMs to generate 3D-specific prompts,\nencoder; and in the second stage, a frozen LLM bootstraps whichareencodedastextualfeaturesandthencombinedwith\nvision-to-language generative learning for zero-shot image- visual features for 3D recognition; and GPT4Tools [197] em-", "metadata": {"id": "524ba6a368e4f3954e0b6d8e8d4f2ac76c8f97e7", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 18, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "to-text generation. Similarly, MiniGPT-4 [187] also deploys ploysLoRA[212]tofine-tuneLLMsfollowingtool-relatedin-\npre-trained and frozen ViT [188], Q-Former and Vicuna structions.Servingasacontroller[209],decisionmaker[213],\nLLM [189], while only a linear projection layer needs to be orsemanticsrefiner[206],[214],LLMssignificantlyfacilitates", "metadata": {"id": "3ad7a85c255231f148d397dbb5dfe410c23ad7d6", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 18, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "trained for vision and language modalities alignment. the progress of visual reasoning research.\nFine-tuning Derived from instruction tuning [25] for NLP\ntasks [137], [25], [24], researchers are now fine-tuning pre-\nIV. FINDINGS&INSIGHTS\ntrained LLMs using multimodal instructions. Following this", "metadata": {"id": "12d93738c6d8d8c1a32de2d86439fc03ac562449", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 18, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "IV. FINDINGS&INSIGHTS\ntrained LLMs using multimodal instructions. Following this\nmethod, LLMs can be easily and effectively extended as Training a billion-scale model is difficult as compared to\nmultimodal chatbots [187], [181], [190] and multimodal task a smaller model. LLMs are prone to various instabilities", "metadata": {"id": "e8ad2bc81200f1586fd039e4a546f45744706af8", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 18, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "solvers [191], [192], [193]. The key issue of this stream of duringtraining,suchashardwarefailureandinstability.Other\nMLLMsistocollectmultimodalinstruction-followingdatafor than this, LLMs exhibit different behaviors such as emergent\nfine-tuning[194].Toaddressthisissue,thesolutionsofbench- abilities,improvedzero-shot,few-shot,andreasoningabilities.", "metadata": {"id": "14d0aa08c8567f9e97f68fb4eca7197ea9312983", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 18, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "mark adaptation [191], [195], [196], self-instruction [138], Researchers report these essential details in their papers for\n[197], [198], and hybrid composition [199], [193] are em- results reproduction and field progress. We identify critical\nployed, respectively. To mitigate the gap between the original information in Table I and II such as architecture, training", "metadata": {"id": "dc9bc2c85c810a55993ebe6f6663ab41e74a1b99", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 18, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "JOURNALOFLATEX 18\nstrategies, and pipelines that improve LLMs’ performance B. Evaluation Datasets\nor other abilities acquired because of changes mentioned in\nThe role of specific datasets, particularly those commonly\nsection III.\nused, is fundamental in the evaluation of Large Language\nModels.Thesedatasets,eachwithitsuniquedesignandsetof", "metadata": {"id": "05bc25381cd5c5ca080e2a7d73dd1125f68d21e6", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 19, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "Models.Thesedatasets,eachwithitsuniquedesignandsetof\nchallenges, serve as the basis for assessing the capabilities of\nV. MODELCONFIGURATIONS\nLLMs. They offer a comprehensive measure of performance\nWeprovidedifferentstatisticsofpre-trainedandinstruction- across a variety of tasks, providing insights into the models’", "metadata": {"id": "80f122842a9b7dcf7527a0fe7e58c161000d5bbb", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 19, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "tunedmodelsinthissection.Thisincludesinformationsuchas proficiency. In the following discussion, we provide a concise\npublication venue, license type, model creators, steps trained, overview of a selection of these key datasets. While the\nparallelism, etc in Table III and Table IV. Architecture details Tables IX and X include a larger set of datasets, we focus on", "metadata": {"id": "a0545f9ccbfd497ac9f4f57b1efd83583b8996e5", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 19, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "of pre-trained LLMs are available in Table V. Providing themostcommonlyusedonesintheevaluationofLLMs.Each\nthese details for instruction-tuned models is unnecessary datasetdescriptionencapsulatesthecoreaspectsitevaluatesin\nbecause it fine-tunes pre-trained models for instruction anLLM,offeringasnapshotofthemodel’spotentialstrengths", "metadata": {"id": "3c3a31337d9018e25473fff02175ea2f131dc10e", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 19, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "datasets. Hence, architectural details are the same as the and limitations.\nbaselines. Moreover, optimization settings for various LLMs 1. HellaSwag [215]: A dataset that challenges models to\nare available in Table VI and Table VII. We do not include pick the best ending to a context uses Adversarial Filtering", "metadata": {"id": "3df49c16c02072c0f1b852c49277938ab311c2a1", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 19, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "details on precision, warmup, and weight decay in Table VII. to create a ‘Goldilocks’ zone of complexity, where generated\nNeither of these details are important as others to mention text is absurd to humans but often misclassified by models.\nfor instruction-tuned models nor provided by the papers. 2. PIQA [216]: A dataset that probes the physical knowl-", "metadata": {"id": "adfcd4cd9b66a647666d0e43d8a5341d7a20e4a7", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 19, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "edge of models, aiming to understand how well they are\nlearning about the real world.\n3. TriviaQA [217]: A dataset that tests models on reading\nVI. DATASETSANDEVALUATION\ncomprehension and open domain question answering (QA)\ntasks, with a focus on Information Retrieval (IR)-style QA.\nLLMs are known to require a huge amount of data for", "metadata": {"id": "c7babe3a8c1b43ee26dc2e4e6014b8e430f8dd10", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 19, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "LLMs are known to require a huge amount of data for\ntraining. Hence, datasets for training and benchmarking these 4. LAMBADA [218]: This dataset evaluates contextual text\nmodels are currently a topic of key importance. In Fig. 12, understanding through a word prediction task. Models must", "metadata": {"id": "d96c6befc4a18803ed68318a4eee52082207e44a", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 19, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "we show the distribution of datasets currently available for predict the last word of a passage, which is easy for humans\nbenchmarking language models for a variety of natural lan- when given the whole passage, but not when given only the\nguageprocessingtasks.Itisnoteworthythatthisdistributionis last sentence.", "metadata": {"id": "4f54131462b5ec0fae8f165c95f1475961e1e26e", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 19, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "guageprocessingtasks.Itisnoteworthythatthisdistributionis last sentence.\nrestricted to only the tasks for which at least 20 datasets have 5. WinoGrande[219]: Alarge-scaledatasetinspiredbythe\nalready been proposed in the literature. LLMs can directly original Winograd [220] Schema Challenge tests models on", "metadata": {"id": "2d4d56e83fe22db66568f1fde016382c1a153e4e", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 19, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "benefit from these dataset for training and evaluation. In their ability to resolve pronoun ambiguity and encourages the\ngeneral, the performance of LLMs greatly depends on the development of models that understand the broad context in\ntraining dataset. A model trained on a good-quality data is natural language text.", "metadata": {"id": "66408064ec1861a49460b0381ff628082b226d79", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 19, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "training dataset. A model trained on a good-quality data is natural language text.\nlikely to perform better on evaluation benchmarks. Specific 6. MMLU [221]: A benchmark that measures the knowl-\ntrainingandevaluationdatasetsusedbyLLMsaresummarized edge acquired by models during pretraining and evaluates\nin Table IX and X. models in zero-shot and few-shot settings across 57 subjects,", "metadata": {"id": "8a003a6715035772fe9f9adbb8bec66c7cbae1ab", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 19, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "in Table IX and X. models in zero-shot and few-shot settings across 57 subjects,\ntesting both world knowledge and problem-solving ability.\n7. SuperGLUE [3]: A more challenging and diverse suc-\nA. Evaluation Tasks\ncessor to the GLUE [222] benchmark, SuperGLUE includes\na variety of language understanding tasks, such as question\nThe evaluation of LLMs is a critical step in gauging their", "metadata": {"id": "d26148f49cca61541627415a9f21f51b5c34acad", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 19, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "The evaluation of LLMs is a critical step in gauging their\nanswering, natural language inference, and coreference reso-\nproficiency and identifying their limitations. This process\nlution. It is designed to provide a rigorous test of language\nprovides a measure of the model’s ability to comprehend,\nunderstanding and requires significant progress in areas like", "metadata": {"id": "d74b4f6a4598482171679d2b0252c9a6f102f6d2", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 19, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "understanding and requires significant progress in areas like\ngenerate, and interact with human language across a spec-\nsample-efficient, transfer, multitasking, and unsupervised or\ntrum of tasks. For Natural Language Understanding (NLU),\nself-supervised learning.\nthese tasks encompass sentiment analysis, natural language", "metadata": {"id": "a9a488b1df06c211062984b01316de8e533beb27", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 19, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "self-supervised learning.\nthese tasks encompass sentiment analysis, natural language\ninference, semantic understanding, closed book question an- 8. StoryCloze[223]: Itintroducesanew“StoryClozeTest”,\nswering, and reading comprehension, among others. While a commonsense reasoning framework for evaluating story", "metadata": {"id": "4b19f3a8c1299572addc87a452961614c88ca85d", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 19, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "Natural Language Generation (NLG) is commonly associated understanding, generation, and script learning. It considers\nwith tasks like text summarization and translation, it is also a model’s ability to understand and generate coherent and\nintrinsicallyinvolvedinotherfunctionalitieslikerespondingto sensible stories.", "metadata": {"id": "0e1dd5ec6108237efff288e14f55569f67d6caf8", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 19, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "intrinsicallyinvolvedinotherfunctionalitieslikerespondingto sensible stories.\nqueriesandgeneratingcontextuallyappropriatedialogue.Both 9. BoolQ [224]: A dataset derived from Google search\nNLUandNLGtasksformpartofestablishedbenchmarksthat queries, BoolQ challenges models to answer binary (yes/no)", "metadata": {"id": "4519856a5ca957d02ee3cbcb8c43e4ef92084156", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 19, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "facilitate the comparison of different models. For a detailed questions.Thequestionsarenaturallyoccurringandarepaired\nperformance comparison of the LLMs on these tasks, please with a paragraph from a Wikipedia article containing the\nrefer to Table VIII. answer. It’s a test of reading comprehension and reasoning.", "metadata": {"id": "2178a8a661b5b16fe7e46d588ac60a207c9a2574", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 19, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "JOURNALOFLATEX 19\nTABLE V: Architecture details of LLMs. Here, “PE” is the positional embedding, “nL” is the number of layers, “nH” is the\nnumber of attention heads, “HS” is the size of hidden states.\nTraining\nModels Type Attention Vocab Tokenizer Norm PE Activation Bias nL nH HS\nObjective\nT5(11B) Enc-Dec SpanCorruption Standard 32k SentencePiece Pre-RMS Relative ReLU × 24 128 1024", "metadata": {"id": "a6fd2b6c10b05157fd00f8ece001a4b208dfccb8", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 20, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "T5(11B) Enc-Dec SpanCorruption Standard 32k SentencePiece Pre-RMS Relative ReLU × 24 128 1024\nGPT3(175B) Causal-Dec NextToken Dense+Sparse - - Layer Learned GeLU ✓ 96 96 12288\nmT5(13B) Enc-Dec SpanCorruption Standard 250k SentencePiece Pre-RMS Relative ReLU - - - -\nPanGu-α(200B) Causal-Dec NextToken Standard 40k BPE Layer - - - 64 128 16384", "metadata": {"id": "b77ceb8c19b443d4b231b8bcc45d9f3f796bb320", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 20, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "PanGu-α(200B) Causal-Dec NextToken Standard 40k BPE Layer - - - 64 128 16384\nCPM-2(198B) Enc-Dec SpanCorruption Standard 250k SentencePiece Pre-RMS Relative ReLU - 24 64 -\nCodex(12B) Causal-Dec NextToken Standard - BPE+ Pre-Layer Learned GeLU - 96 96 12288\nERNIE3.0(10B) Causal-Dec NextToken Standard - WordPiece Post-Layer Relative GeLU - 48 64 4096", "metadata": {"id": "b18063b5168c9bf8150a0d99bcf9bf5c2692d54d", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 20, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "ERNIE3.0(10B) Causal-Dec NextToken Standard - WordPiece Post-Layer Relative GeLU - 48 64 4096\nJurassic-1(178B) Causal-Dec NextToken Standard 256k SentencePiece∗ Pre-Layer Learned GeLU ✓ 76 96 13824\nHyperCLOVA(82B) Causal-Dec NextToken Dense+Sparse - BPE* Pre-Layer Learned GeLU - 64 80 10240\nYuan1.0(245B) Causal-Dec NextToken Standard - - - - - - 76 - 16384", "metadata": {"id": "d75eea56aaee494573f16139e559f459674d48d7", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 20, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "Yuan1.0(245B) Causal-Dec NextToken Standard - - - - - - 76 - 16384\nGopher(280B) Causal-Dec NextToken Standard 32k SentencePiece Pre-RMS Relative GeLU ✓ 80 128 16384\nERNIE3.0Titan(260B) Causal-Dec NextToken Standard - WordPiece Post-Layer Relative GeLU - 48 192 12288\nGPT-NeoX-20B Causal-Dec NextToken Parallel 50k BPE Layer Rotary GeLU ✓ 44 64 -", "metadata": {"id": "be0b6c085b00778467e0d2a039991be796214048", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 20, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "GPT-NeoX-20B Causal-Dec NextToken Parallel 50k BPE Layer Rotary GeLU ✓ 44 64 -\nOPT(175B) Causal-Dec NextToken Standard - BPE - - ReLU ✓ 96 96 -\nBLOOM(176B) Causal-Dec NextToken Standard 250k BPE Layer ALiBi GeLU ✓ 70 112 14336\nGalactica(120B) Causal-Dec NextToken Standard 50k BPE+custom Layer Learned GeLU × 96 80 10240", "metadata": {"id": "e3da61da7aad62b6d9ccc19a73c49f7fe35de504", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 20, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "Galactica(120B) Causal-Dec NextToken Standard 50k BPE+custom Layer Learned GeLU × 96 80 10240\nGLaM(1.2T) MoE-Dec NextToken Standard 256k SentencePiece Layer Relative GeLU ✓ 64 128 32768\nLaMDA(137B) Causal-Dec NextToken Standard 32k BPE Layer Relative GeGLU - 64 128 8192\nMT-NLG(530B) Causal-Dec NextToken Standard 50k BPE Pre-Layer Learned GeLU ✓ 105 128 20480", "metadata": {"id": "06615f08a22ae532c347720fe37d677c71144fd4", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 20, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "MT-NLG(530B) Causal-Dec NextToken Standard 50k BPE Pre-Layer Learned GeLU ✓ 105 128 20480\nAlphaCode(41B) Enc-Dec NextToken Multi-query 8k SentencePiece - - - - 64 128 6144\nChinchilla(70B) Causal-Dec NextToken Standard 32k SentencePiece-NFKC Pre-RMS Relative GeLU ✓ 80 64 8192\nPaLM(540B) Causal-Dec NextToken Parallel+Multi-query 256k SentencePiece Layer RoPE SwiGLU × 118 48 18432", "metadata": {"id": "595eeb74200627ab16e73304bbe7d64392e67322", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 20, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "AlexaTM(20B) Enc-Dec Denoising Standard 150k SentencePiece Pre-Layer Learned GeLU ✓ 78 32 4096\nSparrow(70B) Causal-Dec Pref.&RuleRM - 32k SentencePiece-NFKC Pre-RMS Relative GeLU ✓ 16∗ 64 8192\nU-PaLM(540B) Non-Causal-Dec MoD Parallel+Multi-query 256k SentencePiece Layer RoPE SwiGLU × 118 48 18432\nUL2(20B) Enc-Dec MoD Standard 32k SentencePiece - - - - 64 16 4096", "metadata": {"id": "1d9890baf92240f0279b0ee55303ba7c2af5a733", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 20, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "UL2(20B) Enc-Dec MoD Standard 32k SentencePiece - - - - 64 16 4096\nGLM(130B) Non-Causal-Dec ARBlankInfilling Standard 130k SentencePiece Deep RoPE GeGLU ✓ 70 96 12288\nCodeGen(16B) Causal-Dec NextToken Parallel - BPE Layer RoPE - - 34 24 -\nLLaMA(65B) Causal-Dec NextToken Standard 32k BPE Pre-RMS RoPE SwiGLU - 80 64 8192", "metadata": {"id": "db6b3990a6ce0f03372f33d037b5cf0f02b61d95", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 20, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "LLaMA(65B) Causal-Dec NextToken Standard 32k BPE Pre-RMS RoPE SwiGLU - 80 64 8192\nPanGu-Σ(1085B) Causal-Dec NextToken Standard - BPE FusedLayer - FastGeLU - 40 40 5120\nBloombergGPT(50B) Causal-Dec NextToken Standard 131k Unigram Layer ALiBi GeLU ✓ 70 40 7680\nXuanYuan2.0(176B) Causal-Dec NextToken Self 250k BPE Layer ALiBi GeLU ✓ 70 112 14336", "metadata": {"id": "25d8d61d9ebd7694b09cbf08d6dfbca557474197", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 20, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "XuanYuan2.0(176B) Causal-Dec NextToken Self 250k BPE Layer ALiBi GeLU ✓ 70 112 14336\nCodeT5+(16B) Enc-Dec SC+NT+Cont.+Match Standard - Code-Specific - - - - - - -\nStarCoder(15.5B) Causal-Dec FIM Multi-query 49k BPE - Learned - - 40 48 6144\nLLaMA(70B) Causal-Dec NextToken Grouped-query 32k BPE Pre-RMS RoPE SwiGLUE - - - -\nPaLM-2 - MoD Parallel - - - - - - - - -", "metadata": {"id": "bfbc6bb89613f4b0975565819fe5b6b677823940", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 20, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "PaLM-2 - MoD Parallel - - - - - - - - -\nTABLE VI: Summary of optimization settings used for pre-trained LLMs. The values for weight decay, gradient clipping, and\ndropout are 0.1, 1.0, and 0.1, respectively, for most of the LLMs.\nSequence LR Optimizers Precision Weight Grad\nModels BatchSize Length LR Warmup Decay AdaFactor Adam AdamW FP16 BF16 Mixed Decay Clip Dropout", "metadata": {"id": "bc96977b0a8d7f1399af28090565dfd81d195a17", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 20, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "Models BatchSize Length LR Warmup Decay AdaFactor Adam AdamW FP16 BF16 Mixed Decay Clip Dropout\nT5(11B) 211 512 0.01 × inversesquareroot ✓ - - - ✓\nGPT3(175B) 32K - 6e-5 ✓ cosine ✓ ✓ ✓ ✓ -\nmT5(13B) 1024 1024 0.01 - inversesquareroot ✓ - - - ✓\nPanGu-α(200B) - 1024 2e-5 - - - - ✓ - -\nCPM-2(198B) 1024 1024 0.001 - - ✓ - - - ✓\nCodex(12B) - - 6e-5 ✓ cosine ✓ ✓ ✓ - -", "metadata": {"id": "6c661f980d58c2e4d903933e3698fcbfb649b708", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 20, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "CPM-2(198B) 1024 1024 0.001 - - ✓ - - - ✓\nCodex(12B) - - 6e-5 ✓ cosine ✓ ✓ ✓ - -\nERNIE3.0(12B) 6144 512 1e-4 ✓ linear ✓ - ✓ - -\nJurassic-1(178B) 3.2M 2048 6e-5 ✓ cosine ✓ ✓ ✓ ✓ -\nHyperCLOVA(82B) 1024 - 6e-5 - cosine ✓ - ✓ - -\nYuan1.0(245B) <10M 2048 1.6e-4 ✓ cosinedecayto10% ✓ - ✓ - -\nGopher(280B) 3M 2048 4e-5 ✓ cosinedecayto10% ✓ ✓ - ✓ -\nERNIE3.0Titan(260B) - 512 1e-4 ✓ linear ✓ ✓ ✓ ✓ -", "metadata": {"id": "433d2bcc8ae19c9d8d4d8556d0058df130c5e7bc", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 20, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "ERNIE3.0Titan(260B) - 512 1e-4 ✓ linear ✓ ✓ ✓ ✓ -\nGPT-NeoX-20B 1538 2048 0.97e-5 ✓ cosine ✓ ✓ ✓ ✓ ×\nOPT(175B) 2M 2048 1.2e-4 - linear ✓ ✓ ✓ ✓ ✓\nBLOOM(176B) 2048 2048 6e-5 ✓ cosine ✓ ✓ ✓ ✓ ×\nGalactica(120B) 2M 2048 7e-6 ✓ lineardecayto10% ✓ - ✓ ✓ ✓\nGLaM(1.2T) 1M 1024 0.01 - inversesquareroot ✓ FP32+✓ - ✓ ×\nLaMDA(137B) 256K - - - - - - - - - - - - -", "metadata": {"id": "5fc7e8ddf64ba874f4e95c87abddd57fff2b3be2", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 20, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "LaMDA(137B) 256K - - - - - - - - - - - - -\nMT-NLG(530B) 1920 2048 5e-5 ✓ cosinedecayto10% ✓ ✓ ✓ ✓ -\nAlphaCode(41B) 2048 1536+768 1e-4 ✓ cosinedecayto10% ✓ ✓ ✓ ✓ -\nChinchilla(70B) 1.5M 2048 1e-4 ✓ cosinedecayto10% ✓ ✓ - - -\nPaLM(540B) 2048 2048 0.01 - inversesquareroot ✓ - ✓ ✓ ×\nAlexaTM(20B) 2M 1024 1e-4 - lineardecayto5% ✓ ✓ ✓ - ✓\nSparrow(70B) RM:8+16,RL:16 - 2e-6 ✓ cosinedecayto10% ✓ ✓ ✓ - ✓ ×", "metadata": {"id": "d77d2dc3006655be0d371f9fabe29d800b1f1db6", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 20, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "Sparrow(70B) RM:8+16,RL:16 - 2e-6 ✓ cosinedecayto10% ✓ ✓ ✓ - ✓ ×\nU-PaLM(540B) 32 2048 1e-4 - cosine ✓ - - - -\nUL2(20B) 1024 1024 - - inversesquareroot - - - - - - ×\nGLM(130B) 4224 2048 8e-5 ✓ cosine ✓ ✓ ✓ ✓ ✓\nCodeGen(16B) 2M 2048 5e-5 ✓ cosine ✓ - ✓ ✓ -\nLLaMA(65B) 4MTokens 2048 1.5e-4 ✓ cosinedecayto10% ✓ - ✓ ✓ -\nPanGu-Σ(1.085T) 512 1024 2e-5 ✓ - ✓ ✓ - - -", "metadata": {"id": "acbebe99b8c54f00ca33397d3ce3f63418c2be72", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 20, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "PanGu-Σ(1.085T) 512 1024 2e-5 ✓ - ✓ ✓ - - -\nBloombergGPT(50B) 2048 2048 6e-5 ✓ cosine ✓ ✓ ✓ ✓ ×\nXuanYuan2.0(176B) 2048 2048 6e-5 ✓ cosine ✓ ✓ ✓ ✓ -\nCodeT5+(16B) 2048 1024 2e-4 - linear ✓ ✓ ✓ - -\nStarCoder(15.5B) 512 8k 3e-4 ✓ cosine ✓ ✓ ✓ - -\nLLaMA-2(70B) 4MTokens 4k 1.5e-4 ✓ cosine ✓ ✓ ✓ ✓ -", "metadata": {"id": "2fbedd7a8cb96860f1de0b3412f3adfb2ac60875", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 20, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "JOURNALOFLATEX 20\nTABLE VII: Summary of optimization settings used for instruction-tuned LLMs. Values for gradient clipping and dropout are\nthe same as the pre-trained models, while no model is using weight decay for instruction tuning.\nSequence Optimizer Grad\nModels BatchSize Length LR Warmup LR_Decay AdaFactor Adam Clip Dropout\nWebGPT(175B) BC:512,RM:32 - 6e-5 - - ✓ - -", "metadata": {"id": "b940551b739cd4054b622a96bbdff36318be181e", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 21, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "WebGPT(175B) BC:512,RM:32 - 6e-5 - - ✓ - -\nT0(11B) 1024 1280 1e-3 - - ✓ - ✓\nTk-Instruct(11B) 1024 - 1e-5 - constant - - - -\nOPT-IML(175B) 128 2048 5e-5 × linear ✓ ✓ ✓\nFlan-U-PaLM(540B) 32 - 1e-3 - constant ✓ - ✓\nWizardCoder(15B) 512 2048 2e-5 ✓ cosine - - - -\nFig. 12: Distribution of benchmark datasets available for different natural language processing tasks. We include only the tasks", "metadata": {"id": "49e1a1d8a27d055839879db3dd0d74921bf7a7fe", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 21, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "for which at least 20 datasets have already been proposed.\nTABLE VIII: Performance comparison of top performing LLMs across various NLU and NLG tasks. Here, ‘N-Shots’ indicate\nthenumberofexamplepromptsprovidedtothemodelduringtheevaluation,representingitscapabilityinfew-shotorzero-shot\nlearning settings, and ‘B’ represents the benchmark.\nTask Dataset/Benchmark Model ModelSize N-Shots Score", "metadata": {"id": "f651bb937ffcda082550118fffacc1d5b719e004", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 21, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "Task Dataset/Benchmark Model ModelSize N-Shots Score\nChinchilla 70B 5-shot 65.1\nMulti-Task\nBIG-bench(B) Gopher 280B 5-shot 53.97\nPaLM 540B 5-shot 53.7\nPaLM 540B 5-shot 69.3\nMMLU(B) Chinchilla 70B 5-shot 67.6\nLLaMA 65B 5-shot 63.4\nLanguageUnderstanding ERNIE3.0 12B - 90.6\nSuperGLUE(B) T5 11B - 88.9\nGPT3 175B 32-shot 71.8\nLLaMa 65B zeroshot 84.2\nStoryComprehensionandGeneration", "metadata": {"id": "7084332407258eb1871cee8b21527af626751671", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 21, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "GPT3 175B 32-shot 71.8\nLLaMa 65B zeroshot 84.2\nStoryComprehensionandGeneration\nHellaSwag PaLM 540B zeroshot 83.6\nChinchilla 70B zeroshot 80.8\nGPT3 175B fewshot 87.7\nStoryCloze\nOPT 175B - 79.82\nChinchilla 70B zeroshot 85.0\nPhysicalKnowledgeandWorldUnderstanding PIQA LLaMa 65B zeroshot 82.8\nMT-NLG 530B zeroshot 81.8\nPaLM 540B oneshot 81.4\nTriviaQA GlaM 62B oneshot 75.8\nLLaMa 65B 64-shot 73.0", "metadata": {"id": "6fdf91f1ee225ba6632d6bae4a498882c3a22b42", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 21, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "PaLM 540B oneshot 81.4\nTriviaQA GlaM 62B oneshot 75.8\nLLaMa 65B 64-shot 73.0\nAlexaTM 20B - 94.4\nOpenBookQA OPT 175B fewshot 65.4\nGPTNeoX-20B 20B oneshot 44.2\nContextualLanguageUnderstanding PaLM 540B fewshot 89.7\nLAMBADA GPT3 175B fewshot 86.4\nGLM 130B - 80.2\nPaLM 540B zeroshot 81.1\nCommonsenseReasoning\nLLaMa 65B zeroshot 77.0\nWinoGrande\nChinchilla 70B zeroshot 74.9\nLLaMA 65B zeroshot 52.3", "metadata": {"id": "29e9f00f8af62b81d41c5eb3ae7df29b52de72c5", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 21, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "LLaMa 65B zeroshot 77.0\nWinoGrande\nChinchilla 70B zeroshot 74.9\nLLaMA 65B zeroshot 52.3\nSIQA Chinchilla 70B zeroshot 51.3\nGopher 280B zeroshot 50.6\nReadingComprehension LLaMA 65B zeroshot 85.3\nBoolQ\nChinchilla 70B zeroshot 83.7\nTruthfulness Truthful-QA LLaMA 65B - 57", "metadata": {"id": "74812441e7041091232982691ca4c595804ee2d6", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 21, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "JOURNALOFLATEX 21\n10. RACE-High [225]: A subset of the RACE [225] entailment, predicting whether a given sentence logically fol-\ndataset, RACE-High consists of high school-level English lows from another and evaluating a model’s understanding of\nexam questions. It is designed to evaluate the comprehension logical relationships in a text.", "metadata": {"id": "2af45d4aa9b380233b2734a3060b8621342d16ed", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 22, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "exam questions. It is designed to evaluate the comprehension logical relationships in a text.\nabilityofmodelsinamoreacademicandchallengingcontext. 22. BIG-bench [236]: The BIG-bench (Behavior of Intel-\n11. RACE-Middle [225]: Another subset of the ligent Generative Models Benchmark) is a large-scale bench-", "metadata": {"id": "dceb8e66cdec18b91089929724ebb120ff10d556", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 22, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "RACE [225] dataset, RACE-Middle, contains middle mark designed to test the abilities of LLMs across a wide\nschool-level English exam questions. It offers a slightly less range of tasks, including reasoning, creativity, ethics, and\nchallenging but academically oriented evaluation of a model’s understanding of specific domains.", "metadata": {"id": "b364c58546490bf86a361be5f65335e7df41a939", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 22, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "challenging but academically oriented evaluation of a model’s understanding of specific domains.\ncomprehension skills. 23. SQUADv2 [237]: The Stanford Question Answering\n12. Truthful-QA[226]: Auniquebenchmarkthatmeasures Dataset (SQuAD) [238] is a collection of questions posed by\nalanguagemodel’struthfulnesswhengeneratinganswers.The crowdworkersonasetofWikipediaarticles,wheretheanswer", "metadata": {"id": "157fa784325ac317fe5d0a247d4c13646e634166", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 22, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "datasetincludesquestionsacrossvariouscategorieslikehealth, to every question is a segment of text from the corresponding\nlaw,andpolitics,someofwhicharedesignedtotestthemodel reading passage. SQuADv2 combines the original SQuAD1.1\nagainst common human misconceptions. datasetwithover50,000unanswerablequestions.Theaimisto", "metadata": {"id": "3d5196ed5a46a2222967184d5300b29df42cb548", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 22, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "against common human misconceptions. datasetwithover50,000unanswerablequestions.Theaimisto\n13. ANLI [227]: A large-scale dataset designed to test the evaluate a model’s ability to understand and answer questions\nrobustness of machine learning models in Natural Language based on a given context and to determine when a question is", "metadata": {"id": "5f1c11d0c7e066aecb6fb1768fd3417986e88947", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 22, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "Inference (NLI) is created through an iterative, adversarial unanswerable.\nprocess where humans try to generate examples that models 24. GSM8K [239]: A dataset of diverse grade school math\ncannot correctly classify. wordproblems,testingamodel’sabilitytoperformmulti-step\n14. ARC-Challenge [228]: A rigorous question-answering mathematical reasoning.", "metadata": {"id": "0fda6fa49e3ab59e9871f34887f113f194246cb7", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 22, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "14. ARC-Challenge [228]: A rigorous question-answering mathematical reasoning.\ndataset, ARC-Challenge includes complex, grade-school level 25. WiC [240]: This dataset assesses a model’s ability\nquestions that demand reasoning beyond simple retrieval, to discern word meanings based on context, aiding in tasks", "metadata": {"id": "1980a8a587faa51ec811bce843b21d8bba995c6a", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 22, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "testing the true comprehension capabilities of models. related to Word Sense Disambiguation.\n15. XNLI[229]: Across-lingualbenchmark,XNLIextends 26. Math23k [241]: This one challenges a model’s ability\nthe MultiNLI [230] corpus to 15 languages, including low- to understand and solve mathematical word problems. It con-", "metadata": {"id": "3435a67119f957b9be02fee315599af585ec2316", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 22, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "resource ones like Urdu. It tests models on cross-lingual tains 23,000 Chinese arithmetic word problems that require\nsentence understanding, with 112,500 annotated pairs across models to perform reasoning and computation based on the\nthree categories: entailment, contradiction, and neutral. problem description.", "metadata": {"id": "84ca78c7fd31c02ff0583f995c9de386116dc6c3", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 22, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "three categories: entailment, contradiction, and neutral. problem description.\n16. PAWS-X [231]: PAWS-X, or Cross-lingual Paraphrase 27. LCQMC [242]: The Large-scale Chinese Question\nAdversaries from Word Scrambling, is a multilingual version Matching Corpus (LCQMC) is a dataset for evaluating the", "metadata": {"id": "2b45098e94ac149637573729b52148617b8e38cc", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 22, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "of the PAWS [232] dataset for paraphrase identification. It performanceofmodelsinsemanticmatchingtasks.Itcontains\nincludesexamplesinsevenlanguagesandisdesignedtoeval- pairs of questions in Chinese and their matching status,\nuatetheperformanceofcross-lingualparaphraseidentification makingitavaluableresourceforresearchinChineselanguage\nmodels. understanding.", "metadata": {"id": "183cceff3ea45fdc1293b46be9d6cd7d0ecbb458", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 22, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "models. understanding.\n17. ARC [228]: A larger version of the ARC-Challenge, 28. MATH [243]: This dataset is a platform for evaluating\nthis dataset contains both easy and challenging grade-school the mathematical problem-solving abilities of AI models. It\nlevel, multiple-choice science questions. It’s a comprehensive contains a diverse set of math problems, ranging from arith-", "metadata": {"id": "62b32b5e603884fd7815198708dd0a68c7d2866c", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 22, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "test of a model’s ability to understand and answer complex metic to calculus, and is designed to test the model’s ability\nquestions. to understand and solve complex mathematical problems.\n18. ARC-Easy [228]: A subset of the ARC dataset, ARC- 29. ETHOS [244]: ETHOS is a hate speech detection", "metadata": {"id": "1a95ccaf8a6d71f8687c57fbc820f5f90463bbd2", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 22, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "Easy, contains questions that are answered correctly by either dataset built from YouTube and Reddit comments. It’s a tool\naretrieval-basedalgorithmorawordco-occurrencealgorithm. in the fight against online hate speech, offering binary and\nIt’s a great starting point for models beginning to explore multi-label variants for robust content moderation.", "metadata": {"id": "2892a8a4967ea4adf7a1245f10a38bfdb22bf865", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 22, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "advanced question-answering. 30. StereoSet [245]: StereoSet is a comprehensive dataset\n19. CoQA [233]: A conversational question-answering designedtomeasureandevaluatethepresenceofstereotypical\ndataset, CoQA challenges models with questions that rely biases in language models. It focuses on four key domains:", "metadata": {"id": "38ff9c16b9844af9166cdd5cde72fb6a7763d015", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 22, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "on conversation history and require free-form text answers. gender, profession, race, and religion. By contrasting stereo-\nIts diverse content from seven domains makes it a rigorous typical bias against language modeling ability, it provides a\ntest for models’ ability to handle a wide range of topics and valuable tool for understanding and mitigating biases in large", "metadata": {"id": "b51afa77e9071e2bf8e6e8aee2a4b9c95c2ce204", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 22, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "conversational contexts. language models.\n20. DROP [234]: DROP, or Discrete Reasoning Over the 31. HumanEval [246]: A dataset for the problem-solving\ncontent of Paragraphs, is designed to test a model’s ability to abilityofAImodels,whichincludesadiversesetoftasksthat\nunderstandawidevarietyofreadingphenomena.Itencourages require various cognitive abilities, makes it a comprehensive", "metadata": {"id": "3848a9150131f92a6b9e67e4dff1c4f02dc0c1ea", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 22, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "comprehensive and reliable evaluation of reading comprehen- tool for assessing general intelligence in AI.\nsion capabilities. 32. WebQA [247]: A dataset for open-domain question\n21. RTE[235]: TheRecognizingTextualEntailment(RTE) answering, WebQA offers a large collection of web-based", "metadata": {"id": "8202ad1e483d12fc0dd96077a1ece2d45bc11f1a", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 22, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "datasets come from a series of annual competitions on textual question-answer pairs. It is designed to assess the ability of", "metadata": {"id": "7902ff10b00b1d41b1c56caabdf6dde97778b3a7", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 22, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "JOURNALOFLATEX 22\nAI models to understand and answer questions based on web architectural modules used in various LLMs, leading to better\ncontent. performance, reduced training time and memory, and better\n33. CMRC2018 [248]: This dataset is a test of Chinese training stability.\nlanguage models’ ability to reason comprehensively and is Layer Normalization is found to have a significant effect on", "metadata": {"id": "1122c67b4c3efedfc193a53bb89b252111bb8f48", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 23, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "designedwithachallengingspan-extractionformatthatpushes the performance and training stability of LLMs. Pre-norm,\nthe boundaries of machine performance. that is normalizing inputs rather than outputs, is more\n34. Wikitext103 [249]: With over 100 million tokens from common among LLMs stabilizing the training [8], [111],", "metadata": {"id": "110644f98487a9313683bedca8c515d16b7cc380", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 23, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "Wikipedia’stoparticles,thisdatasetisarichresourcefortasks [90]. BLOOM [9] and AlexaTM [107] utilize an additional\nthat require understanding long-term dependencies, such as layer normalization before embedding layer to stabilize\nlanguage modeling and translation. the training of large-scale models, while the model’s zero-", "metadata": {"id": "e9a8b3ccfec394ffee3bd7049474b3f974f9c418", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 23, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "language modeling and translation. the training of large-scale models, while the model’s zero-\n35. PG19 [250]: This is a digital library of diverse books shot generalization ability can be negatively impacted [9].\nfrom Project Gutenberg. It’s specifically designed to facilitate However, another study [109] finds that pre-norm degrades", "metadata": {"id": "7efc5344ee5cc763c1614007db1a413747bfb8c0", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 23, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "researchinunsupervisedlearningandlanguagemodeling,with fine-tuned model performance as compared to post-norm,\na special focus on long-form content. and there are no stability benefits of pre-norm beyond the\n36. C4 [11]: A clean, multilingual dataset, C4 offers bil- 100B scale. Therefore, GLM-130B [109] used deep-norm", "metadata": {"id": "cce3b25f3bc20a240f3166d1bd5946855a975f3e", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 23, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "lions of tokens from web-crawled data. It’s a comprehensive which is a variant of post-norm for better downstream task\nresourcefortrainingadvancedTransformermodelsonvarious performance after fine-tuning.\nlanguages. Positional Encoding effect performance and training stability\n37. QuAC [251]: This dataset simulates an information- of LLMs like other building blocks of a model. BLOOM [9]", "metadata": {"id": "b45534af96f353455eb4cf05d05e8a7e97af9171", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 23, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "seeking dialog between students and teachers using hidden finds ALiBi outperforming learned and rotary positional\nWikipedia text. It introduces unique challenges not found encodings. Contrary to this, GLM-130B [109] identifies\nin machine comprehension datasets, making it a valuable rotary positional encoding better than ALiBi. So, there is no", "metadata": {"id": "23515483e6e1e8549dd3d481e571b1029c1f602d", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 23, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "resource for advancing dialog systems. conclusion in literature about the positional encodings yet.\n38. COPA[252]: Thisdatasetevaluatesamodel’sprogress Parallel Attention where attention and feed-forward layers\ninopen-domaincommonsensecausalreasoning.Eachquestion are parallel to each other rather than sequential in transformer", "metadata": {"id": "7c372b2bb503e5612cc52b818212d320d34e428c", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 23, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "comprisesapremiseandtwoalternatives,andthemodelmust block has shown to reduce training time by 15%. There is no\nselect the more plausible alternative, testing a model’s ability evidence of performance drop due to this change in literature\nto understand and reason about cause and effect. and used by the models PaLM [14], GPT-NeoX [100], and", "metadata": {"id": "8d892fc5bbc2edcd786f4db83522a1726c086f3f", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 23, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "39. WSC [220]: The Winograd Schema Challenge (WSC) CodeGen [116].\nis a reading comprehension task in which a system must Multi-Query Attention has shared key and value attention\nresolve references in a text, often requiring world knowledge heads in a transformer block while query attention heads are\nand reasoning about the text. projected as usual. This reduces memory usage and speeds", "metadata": {"id": "46ccbe0c62bf0965e81edf697def13c0cd87ff79", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 23, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "and reasoning about the text. projected as usual. This reduces memory usage and speeds\n40. RACE [225]: The RACE is a reading comprehension up sampling in autoregressive decoding. No performance\ndataset collected from English examinations in China, which degradation has been observed with this change and makes", "metadata": {"id": "da63d26e0327b6d9d101493be9bd24035d615e3d", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 23, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "benchmarksAImodelsforunderstandingandansweringques- the training efficient allowing larger batch sizes. Multi-query\ntions on long and complex passages, simulating the challenge attention is used in [14], [118].\nof a real-world examination. Mixture of Experts allows easily scaling model to trillion", "metadata": {"id": "b8148a64578c6f49902eeef39c60f4f0021a56ca", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 23, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "of a real-world examination. Mixture of Experts allows easily scaling model to trillion\n41. StrategyQA [253]: A question-answering dataset that of parameters [115], [103]. Only a few experts are activated\nrequiresreasoningovermultiplepiecesofevidencetoevaluate during the computation making them compute-efficient. The", "metadata": {"id": "ed57729b664f989197864735e1b955a6926a157a", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 23, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "the strategic reasoning ability of AI models, pushing the performance of MoE models is better than the dense models\nboundaries of what machines can understand and answer. for the same amount of data and requires less computation\n42. CSQA [254]: The CommonsenseQA is a question- duringfine-tuningtoachieveperformancesimilartothedense", "metadata": {"id": "8e135f1d08c3e8820be221582a30aee7cdf11d32", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 23, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "answering dataset that requires commonsense knowledge to models as discussed in [103]. MoE architectures are less\nanswer the ability of AI models to understand and answer prone to catastrophic forgetting, therefore are more suited for\nquestions that require commonsense reasoning. continual learning [115]. Extracting smaller sub-models for", "metadata": {"id": "882e4453ab54f05a554bb0bdd43f49bc914205fa", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 23, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "43. GLUE [222]: The General Language Understanding downstream tasks is possible without losing any performance,\nEvaluation (GLUE) benchmark is a collection of resources making MoE architecture hardware-friendly [115].\nfortraining,evaluating,andanalyzingnaturallanguageunder- Sparse vs Dense Activated GPT-3 [8] uses sparse\n(cid:80)", "metadata": {"id": "64c94cd430f5bedd283d23b33b3da70e5c66ece9", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 23, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "(cid:80)\nstandingsystems.Itincludesavarietyoftasksthattestawide transformers [45] whereas GLaM [103] and PanGu- [115]\nrangeoflinguisticphenomena,makingitacomprehensivetool use MoE [104] architecture to lower computational costs\nfor evaluating language understanding in AI. and increase the model size and capacity. According to\nthe literature, sparse modules do not degrade the model’s", "metadata": {"id": "42001e3656dd54a408305f6b75e6559ba9cb859b", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 23, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "the literature, sparse modules do not degrade the model’s\nperformance [45]. However, more experiments are required\nVII. SUMMARYANDDISCUSSION\nto verify this statement.\nA. Architecture\nDue to the gigantic scale of LLMs, minor changes\nin architecture and training strategies have a big impact\non performance and stability. Here, we summarize key", "metadata": {"id": "5128301f0cc67fa013a73a6df6063d5b5d6b1b4d", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 23, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "JOURNALOFLATEX 23\nTABLE IX: Training and evaluation dataset for pre-trained LLMs. Here,“D” denotes Dialogue, “QA” denotes question\nanswering, “CR” is for commonsense reasoning, “CoT” is for chain-of-thought, “RC” for reading comprehension, “LU”\nfor language understanding, “IRC” for in-context reading comprehension, “NLI” for natural language inference, “WT”", "metadata": {"id": "5f7516d426db1030794de0b496cb968c5c19c1c1", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 24, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "for winograd-style tasks, “SC” for sentence completion, “WSD” for word sense disambiguation, “CorefR” for coreference\nresolution.\nModels TrainingDataset EvaluationDataset\nGLUE[222],CNNDM,SQuAD[238],SuperGLUE[3],EnDe,ENFr,EnRo,\nQQP[255],MNLI-m[256],MNLI-mm[256],QNLI[238],\nT5 C4[11]\nWNLI[220],CB[257],\nWiC[240],WMT[258],CNN/DM\nQA:NaturalQS,WebQS,TriviaQA,ARC,CoQA,DROP", "metadata": {"id": "b096e3206b6708485375d330d452f8f0cf81c479", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 24, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "T5 C4[11]\nWNLI[220],CB[257],\nWiC[240],WMT[258],CNN/DM\nQA:NaturalQS,WebQS,TriviaQA,ARC,CoQA,DROP\nGPT-3 CommonCrawl,WebText,BooksCorpora,Wikipedia\nSuperGLUE,WMT,LAMBADA,StoryCloze,HellaSwag\nSP:XNLI[229],PAWS-X[231]S:WikiAnnNER[259]\nmT5 mC4[12]\nQA:MLQA[260],TyDiQA-GoldP[261]\nPanGu-α 1.1TBChineseTextCorpus -\nCCPM[262],C3[263],Sogou-Log,\nCPM-2 WuDaoCorpus[91] WMT20[264],Math23k[241],LCSTS[265],", "metadata": {"id": "ec27a1a92acdbfb4c1ed6590ac5a1d897f0490fc", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 24, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "CCPM[262],C3[263],Sogou-Log,\nCPM-2 WuDaoCorpus[91] WMT20[264],Math23k[241],LCSTS[265],\nLCQMC[242],AdGen[266],CUGE[267]\n54millionpublicsoftwarerepositorieshostedonGitHub HumanEval[246],\nCodex\ncontainingpythonfilesunder1MB 64originalprogrammingproblemswithunittest\nNLU:NLPCC2014-SC,SE-ABSA16_PHNS,SE-ABSA16_CAME,\nBDCI2019,COTE-BD[268],COTE-DP[268],COTE-MFW[268],", "metadata": {"id": "b7f0e28958731e4890c80f852f06d62e75dfeca2", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 24, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "NLU:NLPCC2014-SC,SE-ABSA16_PHNS,SE-ABSA16_CAME,\nBDCI2019,COTE-BD[268],COTE-DP[268],COTE-MFW[268],\nXNLI[229],OCNLI[269],CMNLI[269],CLUEWSC2020[269],\nFinRE[270],SanWen[271],CCKS2020,AFQMC[269],\nLCQMC[242],CSL[269],PAWS-X[231],BQCorpus[272],\nTNEWS,IFLYTEK[273],THUCNEWS,CNSE[274],CNSS[274],\nChinesetextcorpora,BaiduSearch,Webtext, NLPCC-DBQA,CHIP2019,cMedQA[275],", "metadata": {"id": "b364d16704356e44640d66341c8d1e1c4fcdd0b0", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 24, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "Chinesetextcorpora,BaiduSearch,Webtext, NLPCC-DBQA,CHIP2019,cMedQA[275],\nQA-long,QA-short,Poetry&Couplet cMedQA2[276],CKBQA13[277],WebQA[247],\nERNIE3.0\nDomain-specificdatafrommedical,lawandfinancialarea CLUENER[269],Weibo[278],OntoNotes[279],CCKS2019,\nBaiduknowledgegraphwithmorethan50millionfacts CMRC2018[248],CMRC2019[280],DRCD[281],", "metadata": {"id": "fc65f55f26b785032d69d0a0944fad806b223cbc", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 24, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "Baiduknowledgegraphwithmorethan50millionfacts CMRC2018[248],CMRC2019[280],DRCD[281],\nDuReader[282],Dureaderrobust[283],Dureaderchecklist,Dureaderyesno,\nC3[263],CHID[284],CAIL2018-Task1&Task2[285],\nDogWhistleInsider&Outsider[286],Sogou-log[287];\nNLG:LCSTS[265],KBQG,DuReader-QG[282],\nDureaderrobust-QG[283],MATINF-QA[288],Math23KMath23k[241],\nAdGen[266],WMT20-enzh[264],KdConv[289]", "metadata": {"id": "106d5a059f6f0cc277fbf772ebe0a3211a7b07f5", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 24, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "Dureaderrobust-QG[283],MATINF-QA[288],Math23KMath23k[241],\nAdGen[266],WMT20-enzh[264],KdConv[289]\nARC-Challenge[228],ARC-Easy[228],BoolQ[224],\nWikipedia,OWT,Books,C4[11], HellaSwag[215],PIQA[216],\nJurassic-1\nPileCC[290],arXiv,GitHub RACE-high[225],RACE-middle[225],\nRTE[235],StoryCloze[223],WinoGrande[219]\nKoreanblogs,Communitysites,News,KiN NSMC:amoviereviewdatasetfromNAVERmovies;", "metadata": {"id": "d9d14715eec1d43f366479b996b248e4bda6c17c", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 24, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "Koreanblogs,Communitysites,News,KiN NSMC:amoviereviewdatasetfromNAVERmovies;\nKoreanWikipedia,Wikipedia(EnglishandJapanese); KorQuAD1.0[291],KoreanMLdataset\nHyperCLOVA\nModu-Corpus:Messenger,News, AIHubKorean-English,YNAT[292],\nSpokenandwrittenlanguagecorpus,Webcorpus KLUE-TC[292],KLUE-STS[292]\nCommonCrawl,SogouT,SogouNews, FewCLUE[293],ZeroCLUE[269],\nYuan1.0", "metadata": {"id": "6031120cab98b6f9cc41c292ae13dd23022317e5", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 24, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "CommonCrawl,SogouT,SogouNews, FewCLUE[293],ZeroCLUE[269],\nYuan1.0\nBaiduBaike,Wikipedia,Books CMRC2018[248],WebQA[247]\nLM:Pile[290],LAMBADA[218],\nWikitext103[249],PG-19[250],C4[11];\nLU:MMLU[221],BIG-bench[236];\nsubsetsofMassiveWeb[98] RC:RACE-middle[225],RACE-high[225]\nGopher Books,C4[11],News,GitHuband QA:TriviaQA[217],TruthfulQA[226],NaturalQuestions[294];", "metadata": {"id": "10df4ab6a3a16a3a69aa833a517515b2135dcf87", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 24, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "Gopher Books,C4[11],News,GitHuband QA:TriviaQA[217],TruthfulQA[226],NaturalQuestions[294];\nWikipediasamplesfromMassiveText[98] FactCheckingonFever[295],MultiFC[296];\nHellaSwag[215],PIQA[216],WinoGrande[219],SIQA[297];\nRealToxicityPrompts[298],TwitterDataset[299],\nCivilCommentstoxicityclassification[300]\nNLU:NLPCC2014-SC,SE-ABSA16_PHNS,SE-ABSA16_CAME,", "metadata": {"id": "6e03e675c0caa244e7b79a40540c37b1d02bc887", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 24, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "CivilCommentstoxicityclassification[300]\nNLU:NLPCC2014-SC,SE-ABSA16_PHNS,SE-ABSA16_CAME,\nBDCI2019,EPRSTMT[293],COTE-BD[268],COTE-MFW[268],\nOCNLI[269],CMNLI[269],OCNLI-FC[293],CLUEWSC[269]\nCLUEWSC-FC[293],FinRE[270],SanWen[271],AFQMC[269],\nLCQMC[242],PAWS-X[231],BQCorpus[272],CSL[269]\nChinesetextcorpora,BaiduSearch,Webtext,\nCSL-FC[293],BUSTM,TNEWS,TNEWS-FC[293],IFLYTEK[273],IFLYTEK-FC", "metadata": {"id": "9488344c65f28eff38533be95120d8f74ed67aab", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 24, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "CSL-FC[293],BUSTM,TNEWS,TNEWS-FC[293],IFLYTEK[273],IFLYTEK-FC\nQA-long,QA-short,Poetry&Couplet\nTHUCNEWS,CNSE[274],CNSS[274],CSLDCP\nERNIE3.0TITAN Domain-specificdatafrommedical,lawandfinancialarea\nNLPCC-DBQA,CHIP2019,cMedQA[275],\nBaiduknowledgegraphwithmorethan50millionfacts\ncMedQA2[276],CKBQA13[277],WebQA[247],\nERNIE3.0adversarialdataset,ERNIE3.0controllabledataset", "metadata": {"id": "ac6faaea01e3f59fcf6bd7db262d65ebef57d749", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 24, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "cMedQA2[276],CKBQA13[277],WebQA[247],\nERNIE3.0adversarialdataset,ERNIE3.0controllabledataset\nPD&CFT,CMRC2017[301],CMRC2019[280]\nCHID[284],CHID-FC[293],WPLC,DRCD[281],\nDuReader[282],Dureaderrobust[283],Dureaderchecklist,Dureaderyesno,\nC3[263],CMRC2018[248],CAIL2018-Task1&Task2[285]\nDogWhistleInsider&Outsider[286]\nANLI[227],ARC[228],HeadQA[302],HellaSwag[215],", "metadata": {"id": "aed6997eacc7d42b6cee0040ac553ebacf2e3cec", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 24, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "DogWhistleInsider&Outsider[286]\nANLI[227],ARC[228],HeadQA[302],HellaSwag[215],\nLAMBADA[218],LogiQA[303],OpenBookQA[304],PIQA[216],\nGPT-NeoX-20B Pile[290] PROST[305],QA4MRE[306],SciQ[307],TriviaQA[217],\nWinoGrande[219],SuperGLUE[3],MATH[243],\nAdvancedKnowledge-BasedTasks\nHellaSwag[215],StoryCloze[223],PIQA[216],\nARC-Easy[228],ARC-Challenge[228],OpenBookQA[304],", "metadata": {"id": "900ac3fb6cb882b29eced84cb85e702e3c60c3d9", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 24, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "HellaSwag[215],StoryCloze[223],PIQA[216],\nARC-Easy[228],ARC-Challenge[228],OpenBookQA[304],\nWinoGrad[220],WinoGrande[219],SuperGLUE[3],\nRoBERTa[308],Pile[290],\nOPT WizardofWikipedia[310],EmpatheticDialogues[311],\nPushShift.ioReddit[309]\nConvAI2[312],BlendedSkillTalk[313],WizardofInternet[314]\nETHOS[244],CrowS-Pairs[315],StereoSet[245],\nRealToxicPrompts[298],DialogueResponsibleAIevaluations", "metadata": {"id": "bba3efc1dfc02c3f40c57601cd0f2410ff031df0", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 24, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "ETHOS[244],CrowS-Pairs[315],StereoSet[245],\nRealToxicPrompts[298],DialogueResponsibleAIevaluations\nTableContinuedonNextPage", "metadata": {"id": "98b36af4d6b6ec1df0570c1cb2fd560744995aa9", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 24, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "JOURNALOFLATEX 24\nModels TrainingDataset EvaluationDataset\nBLOOM ROOTS[316] -\narXiv,PMC,SemanticScholar\nWikipedia,StackExchange,LibreText,OpenTextbooks\nKnowledgeprobes,Latexequations,\nRefSeqGenome,OEIS,LIPIDMAPS,NASAExoplanet\nGalactica AminoProbe[125],BioLAMA[125],ChemicalReactions[125],\nCommonCrawl,ScientificCC,AcademicCC\nGalaxyClusters[125],MineralGroups[125]\nGitHubrepositories", "metadata": {"id": "15a4592d6f8e5527b64be052e7ae4c7e5f25445d", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 25, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "CommonCrawl,ScientificCC,AcademicCC\nGalaxyClusters[125],MineralGroups[125]\nGitHubrepositories\nKhanProblems[317],GSM8K[239],OneSmallStep\nNLG:TriviaQA[217],NQS,WebQS,SQuADv2[237],\nLAMBADA[218],DROP[234],QuAC[251],CoQA[233];\nNLU:HellaSwag[215],StoryCloze[223],WinoGrad[220],\nFilteredWebpages,Socialmediaconversations WinoGrande[219],RACE-middle[225],RACE-high[225],PIQA[216],\nGLaM", "metadata": {"id": "efcc9510cecb5f7c67d9e9aeaf4406bcdb391c94", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 25, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "GLaM\nWikipedia,Forums,Books,News ARC-Challenge[228],ARC-Easy[228],OpenbookQA[304],\nBoolQ[224],COPA[318],RTE[235],WiC[240],\nMultiRC[319],WSC[220],ReCoRD[320],CB[257],\nANLIR1[227],ANLIR2[227],ANLIR3[227]\nMini-TuringBenchmark(MTB)[4];\nSelf-collecteddialogswithturnsbyasking\nLaMDA Infiniset[127]:Publicdocuments,Dialogs,Utterances\ncrowdworkerstointeractwithLaMDA;\nWizardofWikipedia[310]", "metadata": {"id": "79b2370e5985c70bd31f7b405263c4b5cffe9e50", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 25, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "crowdworkerstointeractwithLaMDA;\nWizardofWikipedia[310]\nTwosnapshotsofCommonCrawlandBooks3, Completionprediction:LAMBADA[218]\nOpenWebText2,StackExchange,PubMedAbstracts, RC:RACE[225],BoolQ[224]\nMT-NLG\nWikipedia,PG-19[250],BookCorpus2,NIHExPorter, CR:PiQA[216]\nPileCC[290],CC-Stories[321],RealNews[322] NaturalLanguageInterface:ANLI[227],HANS[323]\nSelectedGitHubrepositories", "metadata": {"id": "8848762deaf9a7aa5412cb95e4658786fa97c216", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 25, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "SelectedGitHubrepositories\nAlphaCode CodeContests[118]:Codeforces[324], Codeforcescompetitions,CodeContests[118],APPS[243]\nDescription2Code[325],CodeNet[326]\nLM:Pile[290],LAMBADA[218],\nWikitext103[249],PG-19[250],C4[11];\nLU:57MMLU[221]tasks,62BIG-bench[236]tasks;\nMassiveWeb[98],MassiveText[98]\nChinchilla QA:TriviaQA[217],NaturalQuestions[294];\nBooks,C4[11],News,GitHub,Wikipedia", "metadata": {"id": "4e2861469cc286ed5616be60fd0a0f2c010963c2", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 25, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "Chinchilla QA:TriviaQA[217],NaturalQuestions[294];\nBooks,C4[11],News,GitHub,Wikipedia\nRC:RACE-middle[225],RACE-high[225];\nHellaSwag[215],PIQA[216],WinoGrande[219],\nSIQA[297],BoolQ[224],TruthfulQA[226]\nQA:TriviaQA[217],NaturalQuestions[294],WebQuestions[327],\nTyDiQA-GoldP[261];CR:PIQA[216],ARC[228],OpenBookQA[304];\nIRC:DROP[234],CoQA[233],QuAC[251],SQuADv2[237],RACE[225];", "metadata": {"id": "ac82c3d2fd2db7a753412bdc6bf953275a75e945", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 25, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "IRC:DROP[234],CoQA[233],QuAC[251],SQuADv2[237],RACE[225];\nwebpages,books,wikipedia,news,articles, NLI:ANLI[227];WT:WinoGrad[220],WinoGrande[219];\nPaLM\nsourcecode,socialmediaconversations CoT:GSM8K[239],StrategyQA[253],CSQA[254],\nSVAMP[328],MAWPS[329],AQuA[330];\nLU:MMLU[221]SuperGLUE[3],LAMBADA[218],\nHellaSwag[215],StoryCloze[223],BIG-bench[236],WMTlanguagepairs", "metadata": {"id": "89af9b540552d2ad4bf9bce516a884067507df70", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 25, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "HellaSwag[215],StoryCloze[223],BIG-bench[236],WMTlanguagepairs\nNLG:MLSum[331],XSum[332],E2E[333],WebNLG[334];\nMachineTranslation:Flores-101[335],English-GermanWMT’16,\nAlexaTM Wikipedia,mC4[12] English-FrenchWMT’14,German-FrenchWMT’19[336];\nNLP:XNLI[229],XCOPA[252],PAWS-X[231],XWinograd[337],\nSuperGLUE[3],SQUADv2[237],MultiArith[338]\nPer-turnresponsepreferenceandadversarialprobing,", "metadata": {"id": "5b596f0c737dff29edd355b7c67ab6cd300a7c32", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 25, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "SuperGLUE[3],SQUADv2[237],MultiArith[338]\nPer-turnresponsepreferenceandadversarialprobing,\nMulti-turndialogues,Information-seekingdialogues,\nHumandataforruleviolations\nChinchilla-generated[106]conversationalquestions,\nandper-turnresponsepreferences,\nSparrow GopherCitehumanevaluationinterface,\nSelf-playdataaccumulatedthroughtraining,\nFilteredELI5“Free”dialogues,DPC-generated[106]dialogues", "metadata": {"id": "4aad68c0508a2fd8f76d59821c47b30e3dc7dea6", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 25, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "Self-playdataaccumulatedthroughtraining,\nFilteredELI5“Free”dialogues,DPC-generated[106]dialogues\nGopherCiteFilteredELI5\nWinoGender[339],Winobias[340],BBQ[341],\nNaturalQuestions[294],QuizBowl[342],TriviaQA[217]\nMMLU[221],\nQA:TriviaQA[217],NaturalQuestions[294],TydiQA[261];\nRC:LAMBADA[218];\nU-PaLM SameasPaLM\nCR:BoolQ[224],PIQA[216],HellaSwag[215],WinoGrande[219];", "metadata": {"id": "4aa62929151b994b9b4e5d455f37fc93839b50fd", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 25, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "RC:LAMBADA[218];\nU-PaLM SameasPaLM\nCR:BoolQ[224],PIQA[216],HellaSwag[215],WinoGrande[219];\nCoT:GSM8K[239],BBH[236],StrategyQA[253],CSQA[254];\nLU:MMLU[221]SuperGLUE[3],MGSM[343]\nSuperGLUE[3],GSM8K[239],SVAMP[328],ASDiv[344],\nUL2 -\nMAWPS[329],AQuA[330]\nLAMBADA[218],Pile[290],MMLU[221],\nGLM-130B - CLUE[269],CrowS-Pairs[315],StereoSet[245],\nETHOS[244],RealToxicPrompts[298]", "metadata": {"id": "a73b0ff70e332982fbe290a30d4e3f1e0f042650", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 25, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "GLM-130B - CLUE[269],CrowS-Pairs[315],StereoSet[245],\nETHOS[244],RealToxicPrompts[298]\nCodeGen Pile[290],BigQuery,BigPython[116] MostlyBasicPythonProblems\nCR:BoolQ[224],PIQA[216],SIQA[297],HellaSwag[215],\nWinoGrande[219],ARC-Challenge[228],OpenBookQA[304];\nQA:TriviaQA[217],NaturalQuestions[294];\nCommonCrawl,C4[11],Github, RC:RACE-middle[225],RACE-high[225];\nLLaMA", "metadata": {"id": "bdad91e6d9bfd24c0113250dbb63b82db75b0550", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 25, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "CommonCrawl,C4[11],Github, RC:RACE-middle[225],RACE-high[225];\nLLaMA\nWikipedia,Books,arXiv,StackExchange MathematicalReasoning:MATH[243],GSM8K[239];\nCodeGeneration:HumanEval[246],MBPP[345];\nMMLU[221],RealToxicityPrompts[298],\nCrowS-Pairs[315],WinoGender[339],TruthfulQA[226]\nPanGUΣ WuDaoCorpora[91],CLUE[269],Pile[290],C4[11],andPythoncode -\nFinancialData,BIG-bench[236],MMLU[221],ARC,PiQA[216],", "metadata": {"id": "d47ced53ea6237d2c0f2dc48bddba36b0a26e036", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 25, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "FinancialData,BIG-bench[236],MMLU[221],ARC,PiQA[216],\nCommonsenseQA[254],BoolQ[224],OpenBookQA[304],RACE[225],MultiRC[319],\nBloombergGPT FinPile[128],ThePile[290],C4[11],Wikipedia[17]\nReCoRD[320],ANLI[227],RTE[235],COPA[252],WIC[240],WinoGrad[220],\nWinoGrande[219],HellaSWAG[215],StoryCloze[223]\nXuanYuan2.0 Internet -\nCodeT5+ CodeSearchNet[346],GithubCode HumanEval[246],MathQA[347],GSM8K[239]", "metadata": {"id": "ea72de265ac84bd9c8cd566608d6faee699e56a0", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 25, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "XuanYuan2.0 Internet -\nCodeT5+ CodeSearchNet[346],GithubCode HumanEval[246],MathQA[347],GSM8K[239]\nHumanEval[246],MBPP[345],DS-1000[349],HELM[350],\nStarCoder TheStackv1.2[348]\nMulti-LanguageEvaluation,GSM8K[239],MMLU[221],CoQA[233]\nCR:PIQA[216],SIQA[297],HellaSwag[215],\nWinoGrande[219],ARC-Easy[228],ARC-Challenge[228],OpenBookQA[304],\nCSQA[254];QA:TriviaQA[217],NaturalQuestions[294];", "metadata": {"id": "bb7a8af76b7a5edf693e8e3c3774f5091f71f112", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 25, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "CSQA[254];QA:TriviaQA[217],NaturalQuestions[294];\nLLAMA-2 - RC:BoolQ[224],QuAC[251],SQuADv2[237]\nMathematicalReasoning:MATH[243],GSM8K[239];\nCodeGeneration:HumanEval[246],MBPP[345];\nMMLU[221],BigBenchHard[236],AGIEval\nQA:TriviaQA[217],NaturalQuestions[294],WebQuestions\nCloze:StoryCloze[223],HellaSwag[215],BIG-Bench[236],SuperGLUE[3]\nPaLM-2 Webdocuments,Code,Books,Maths,Conversation", "metadata": {"id": "e5e093ab12074cd4ea6c3fe4da307fe7d8e56fd6", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 25, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "PaLM-2 Webdocuments,Code,Books,Maths,Conversation\nWT:Winograd[220],WinoGrande[219],RC:SQuADv2[237],RACE[225]\nCR:PIQA[216],ARC[228],OpenBookQA[304],NLI:ANLI[227]", "metadata": {"id": "4b8b8cb87e005b0c6953e132034454640702c730", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 25, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "JOURNALOFLATEX 25\nTABLE X: Training and evaluation datasets for instruction-tuned LLMs. All the abbreviations are the same as Table IX\nModels TrainingDatasets EvaluationDatasets\nNLI:ANLI[227],CB[257],RTE[235];\nT0 - SC:COPA[318],HellaSwag[215]StoryCloze[223];\nWSD:WiC[240];CorefR:WSC[220],Wino(XL)[219]\nELI5[351],ELI5fact-check[133],TriviaQA[217],\nARC-Challenge[228],ARC-Easy[228],", "metadata": {"id": "22edd9715d9483417d830d28d23ab384944efdc9", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 26, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "ELI5[351],ELI5fact-check[133],TriviaQA[217],\nARC-Challenge[228],ARC-Easy[228],\nWebGPT ELI5[351],TruthfulQA[226],TriviaQA[217]\nHand-writtendata,Demonstrationsofhumans,\nComparisonsbetweenmodel-generatedanswers\nTk-INSTRUCT SUP-NATINST[26] SUP-NATINST[26]\nmT0 xP3[134] -\nPromptSource[22],FLAN[25],\nPromptSource[22],FLAN[25],Super-NaturalInstructions[352],\nSuper-NaturalInstructions[352],", "metadata": {"id": "818719acc2a56fe0d2a2bcc759bbf5f7c3b0a5f2", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 26, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "PromptSource[22],FLAN[25],Super-NaturalInstructions[352],\nSuper-NaturalInstructions[352],\nOPT-IML UnifiedSKG[353],CrossFit[354],ExMix[355],T5[11],\nUnifiedSKG[353],CrossFit[354],\nReasoning,MMLU[221],BBH[236],RAFT[356]\nExMix[355],T5[11],Reasoning\nFlan Muffin,T0-SF,NIv2,CoT MMLU[221],BBH[236],TyDiQA[261],MGSM[343]\nWizardCoder CodeAlpaca HumanEval[246],MBPP[345],DS-1000[349]", "metadata": {"id": "bd9f715dacafc2d5397dbb6077ec6461f3991394", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 26, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "WizardCoder CodeAlpaca HumanEval[246],MBPP[345],DS-1000[349]\nB. Training Strategies range 1e−4 to 8e−4. Moreover, MT-NLG (530B) [21] and\nTraining models at a huge scale require some tricks to GPT-NeoX (20B) [100] suggest interpolating learning rates\nreducetrainingcosts,avoidlossdivergenceandachievebetter based on the model size using the GPT-3 [8] models ranging", "metadata": {"id": "bf438b770246058c1948a1af344b52fe12cf95ab", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 26, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "performance. We summarize and discuss some of these key between 13B and 175B. This avoids tuning the learning rate\ntricks used in different LLMs. hyperparameter.\nMixed Precision is a famous method for LLMs to reduce Training Parallelism 3D parallelism, a combination of data,\nmemory usage and improve training efficiency. In mixed pipeline and tensor parallelism, is the most utilized training", "metadata": {"id": "6bc53a7b06627f6f678cbc1ec8a771d19b8ae6ff", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 26, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "precision,forwardandbackwardpassesareperformedinFP16 parallelismapproachinLLMs[109],[14],[10],[9],[21],[97],\nformat whereas optimizer states and master weights are kept [94]. In addition to the 3D parallelism, BLOOM [9] uses zero\ninFP32format[357].Adrawbackassociatedwiththisformat optimizer [61] to shard optimizer states. PanGu-α [90] and", "metadata": {"id": "5eac983a18d33c4080259d75898aff6492ab9a72", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 26, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "change is training instability due to a smaller value range PanGu-Σ [115] go beyond the 3D parallelism and apply 5D\nresulting in loss spikes [109]. An alternative to FP16 is BF16 parallelism which additionally contains optimizer parallelism\nwhich has a comparatively larger range and performs some and rematerialization.", "metadata": {"id": "9e81b4ae1d90ef1afb92c67f53e950579645d0b2", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 26, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "which has a comparatively larger range and performs some and rematerialization.\nprecision-sensitive operations like gradient accumulation and Mode Switching adds task-related tokens at the beginning\nsoftmaxinFP32[9].BF16hasbetterperformanceandtraining of the text during training. These tokens refer to the natural", "metadata": {"id": "6d9751f6679b408f5cd96212987cdafabd44c6e2", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 26, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "stability but uses more memory and is supported on specific language understanding and natural language generation tasks\nhardware, for example, A100 GPUs. Therefore, its adoption whichareshowntoimprovethedownstreamtaskperformance\nin LLMs is limited. in [15], [20], [107]. During fine-tuning and inference, tokens", "metadata": {"id": "9766a07591a0ace7fbf4da989492901b24e6c0f0", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 26, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "in LLMs is limited. in [15], [20], [107]. During fine-tuning and inference, tokens\nTraining Instability is a common issue in LLMs where loss are appended based on the downstream tasks.\ndivergence or spiking is observed multiple times during train- Controllable Text Generation Generating credible and con-", "metadata": {"id": "665046fec9349a0e7f0b2f87fbe8b8135807a52f", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 26, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "ing. This happens in the presence of gradient clipping [14]. trolled text from a pre-trained model is challenging. GPT-\nTo mitigate this problem, many approaches suggest restarting 3 [8] and other LLMs use in-context learning to control\ntrainingfromanearliercheckpoint[14],[109],[103],skipping generated text. While in-context learning helps in controlling", "metadata": {"id": "73825d107489c7155079b6dc9f324e551ffa5320", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 26, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "200-500 earlier data batches at the point of divergence in [14] the generated text, ERNIE 3.0 Titan [99] suggests using\nand re-shuffling batches in [103]. The embedding layer gradi- adversarial loss to rank its generated text for credibility and\nentshrinkprovestofurtherstabilizethetrainingasitsgradient soft prompts such as genre, topic, keywords, sentiment, and", "metadata": {"id": "954ff74439575c210bf0100c47b9bd35766478ad", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 26, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "normissignificantlylargerthantheotherlayers[109].Another length for better control on generated text.\nsuggestiontoimprovetrainingstabilityforlargermodelsisnot\nto use biases in dense and norm layers as in [14]. C. Pre-Training vs Instruction Tuning\nWeight Initialization plays a significant role in model con-\nWhile pre-training is important for the generalization of", "metadata": {"id": "c99c0f257622b882c3c9cc7107468d2200b3f998", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 26, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "While pre-training is important for the generalization of\nvergence and training stability. GPT-NeoX [100] initializes\nLLMs, instruction-tuning improves the performance of LLMs\nfeed-forwardlayersbeforeresidualswith √2 asin[131]and\nL d further and makes them useable. Therefore, it is suggested\notherlayerswithsmallinitializationscheme[358].Thisavoids", "metadata": {"id": "3a43d37524bd768b9ae973d50fd343573231e42e", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 26, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "otherlayerswithsmallinitializationscheme[358].Thisavoids\nto perform instruction fine-tuning of pre-trained LLMs to use\nactivations growing exponentially with the increasing depth.\nthem effectively [25], [26], [137], [24], [133].\nMT-NLG [21] found higher variance for weight initialization\nleads to unstable training, hence validating small initialization\nD. Supervised Models vs Generalized Models", "metadata": {"id": "a4ae0429f4eb69b376e33418e1bc82302c728bc1", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 26, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "D. Supervised Models vs Generalized Models\nscheme [358]. Various models perform random weight ini-\ntialization which can cause bad initialization, Galactica [125] Although generalized models are capable of performing\nsuggests a longer warmup to negate the effect. diversetaskswithgoodperformancetheyhavenotyetoutper-", "metadata": {"id": "fd507cf30b210ab3736fc65177380b10754d187c", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 26, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "suggests a longer warmup to negate the effect. diversetaskswithgoodperformancetheyhavenotyetoutper-\nLearning Rate is important for stable training. It is suggested formed models trained in supervised settings. The supervised\nto use a lower value [9], [14], [20] with warmup and decay trained models are still state-of-the-art in various NLP tasks", "metadata": {"id": "b2c9548c0f211e82508f6cec2e7d4bf2e1b3ba28", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 26, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "(cosine or linear). Usually, the learning rate is within the by a large margin as shown in [8], [14], [26].", "metadata": {"id": "1eabb9851babc5abbde059f99bc881305fee78dd", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 26, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "JOURNALOFLATEX 26\nE. Zero-Shot vs Few-Shot IX. VERSIONING\nLLMs perform well in zero-shot and few-shot settings. But We keep track of the versions of this paper we release as\nthe performance difference between zero-shot and few-shot the content updates.\nis large for pre-trained models [8], [14], naming LLMs as Version 1.0: We covered 30 pre-trained models and 6", "metadata": {"id": "df59608f9ae7f7f3a99a39a3c63949139541091d", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 27, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "meta-learners [8]. LLMs zero-shot evaluations underperform instruction-tuned models, including their overview, findings,\nunsupervised methods in neural machine translation [8]. The training, and evaluation datasets, and discussed important\nliterature shows pre-training is not enough for good zero- architectural and training tricks by various LLMs.", "metadata": {"id": "31ed265907bdd663560245304eac303fe1910539", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 27, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "shot performance [14], [25]. To improve the zero-shot per- Version 1.1: Further pre-trained LLMs added along with\nformance the literature suggests using instruction fine-tuning discussion on on self-instruct LLMs. Categorized LLMs ac-\nthat improves the zero-shot performance significantly and cording to the application, provided descriptions of widely", "metadata": {"id": "ed01a3b35c697247ac5f7afd252e332210eed1dc", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 27, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "outperforms baselines. Instruction fine-tuning has also been used evaluation datasets, added a section on robotics, and\nshown to improve zero-shot generalization to unseen tasks. extended discussion in section VII. Tables have been updated.\nAnother model Flan-PaLM [25] unlocks zero-shot reasoning Version 1.2: Added sections on Alignment tuning and mul-", "metadata": {"id": "d42cdc745f6b0e3ef75b192ed2927a88a05c9da3", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 27, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "with CoT training. timodal LLMs. A performance comparison table on various\nbenchmarks and datasets. Added LLaMA-2 and PaLM-2.\nF. Encoder vs Decoder vs Encoder-Decoder Note: If you find any mistakes, or have issues and conflicts\nwith the writing in this paper, please email us. We welcome\nTraditionally, these architectures perform well for different\nsuggestions to improve this paper.", "metadata": {"id": "f5bb0b2f4a0292fa9ed78eb89bb647813f33924a", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 27, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "Traditionally, these architectures perform well for different\nsuggestions to improve this paper.\ntasks, for example, encoder-only for NLU tasks, decoder-\nonly for NLG, and encoder-decoder for sequence2sequence\nmodeling.Encoder-onlymodelsarefamousforsmallermodels\nsuch as Bert [5], RoBERTa [359], etc, whereas LLMs are\neither decoder-only [8], [100], [9] or encoder-decoder [11],", "metadata": {"id": "4b40dac13e24d44a941dd789bbe72e7fdc245b71", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 27, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "either decoder-only [8], [100], [9] or encoder-decoder [11],\n[12], [107]. While decoder-only models are good at NLG\ntasks, various LLMs, PaLM [14], OPT [10], GPT-3 [8],\nBLOOM [9], LLaMA [140], are decoder-only models with\nsignificantperformancegainsonbothNLUandNLGtasks.In\ncontradiction to this, T5 [11] and UL2 [15] identify encoder-\ndecoder models out-performing decoder-only models. In an-", "metadata": {"id": "bee3d94fe311d482697f67aa336077ee09545ab5", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 27, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "decoder models out-performing decoder-only models. In an-\nother study, PaLM [14] finds increasing the size of decoder-\nonlymodelscanreducetheperformancegapbetweendecoder-\nonly and encoder-decoder architectures.\nAlthough decoder-only architectures have become a trend for\nLLMs, many recently proposed approaches [15], [107] use\nmode-switching tokens in text with encoder-decoder architec-", "metadata": {"id": "16a2bb675858aad3533bbff30a57658effc58d0d", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 27, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "mode-switching tokens in text with encoder-decoder architec-\ntures to enable task-specific modes. Similarly, CodeT5+ [122]\nuses an encoder-decoder architecture with multiple training\nobjectives for different tasks, activating the encoder, decoder,\norbothaccordingtothetasks.Thesevariationsinarchitecture\nandtrainingobjectivesallowamodeltoperformwellindiffer-", "metadata": {"id": "81f0b368eaa71260d3ff3e886a68c316c0962233", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 27, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "andtrainingobjectivesallowamodeltoperformwellindiffer-\nent settings. Because of this dynamic configuration, the future\nof LLMs can be attributed to encoder-decoder architectures.\nVIII. CONCLUSION\nThis paper has reviewed various LLMs, discussing the pros\nandconsofmultiplemodels.Ourreviewconcludedsignificant\nfindingsandprovidedadetailedanalysisofthedesignaspects", "metadata": {"id": "4f5341dfd705eda49ae66b1f66fd2e046723f21d", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 27, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "findingsandprovidedadetailedanalysisofthedesignaspects\nof each LLM, including architecture, datasets, and training\npipelines. We have identified crucial architectural compo-\nnents and training strategies employed by different LLMs\nand presented a summary and discussion. Moreover, we have\ncomparedtheperformanceofLLMsinzero-shotandfew-shot\nsettings, explored the impact of fine-tuning, and compared", "metadata": {"id": "57db1a48347c82c7e0b91afe3ed78f1bd3dcfc37", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 27, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "settings, explored the impact of fine-tuning, and compared\nsupervised vs generalized models, and encoder vs decoder\nvs encoder-decoder architectures. This paper will serve as a\nvaluable resource for researchers, offering insights into the\nrecent advancements in LLMs and providing fundamental\nconcepts and details to develop improved LLMs.", "metadata": {"id": "1c00a9c47d201de25e13e3d0195b93700daf651c", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 27, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "JOURNALOFLATEX 27\nREFERENCES [22] V.Sanh,A.Webson,C.Raffel,S.H.Bach,L.Sutawika,Z.Alyafeai,\nA. Chaffin, A. Stiegler, T. L. Scao, A. Raja et al., “Multitask\n[1] B.A.yArcas,“Dolargelanguagemodelsunderstandus?”Daedalus, promptedtrainingenableszero-shottaskgeneralization,”arXivpreprint\nvol.151,no.2,pp.183–197,2022. 1 arXiv:2110.08207,2021. 2,11,13,15,25", "metadata": {"id": "034066da09cec354264136327f04be2e55e8db0c", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 28, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "vol.151,no.2,pp.183–197,2022. 1 arXiv:2110.08207,2021. 2,11,13,15,25\n[2] A. Chernyavskiy, D. Ilvovsky, and P. Nakov, “Transformers:“the end [23] N. Muennighoff, T. Wang, L. Sutawika, A. Roberts, S. Biderman,\nof history” for natural language processing?” in Machine Learning T. L. Scao, M. S. Bari, S. Shen, Z.-X. Yong, H. Schoelkopf", "metadata": {"id": "17649ea8d2f396d1e75ba41f3756a14278ee64f4", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 28, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "and Knowledge Discovery in Databases. Research Track: European etal.,“Crosslingualgeneralizationthroughmultitaskfinetuning,”arXiv\nConference, ECML PKDD 2021, Bilbao, Spain, September 13–17, preprintarXiv:2211.01786,2022. 2\n2021,Proceedings,PartIII21. Springer,2021,pp.677–693. 1\n[24] S.Iyer,X.V.Lin,R.Pasunuru,T.Mihaylov,D.Simig,P.Yu,K.Shus-", "metadata": {"id": "1e21add5025b7c5f58df3e40a184b6c41a5a86cb", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 28, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "[24] S.Iyer,X.V.Lin,R.Pasunuru,T.Mihaylov,D.Simig,P.Yu,K.Shus-\n[3] A.Wang,Y.Pruksachatkun,N.Nangia,A.Singh,J.Michael,F.Hill,\nter, T. Wang, Q. Liu, P. S. Koura et al., “Opt-iml: Scaling language\nO. Levy, and S. Bowman, “Superglue: A stickier benchmark for\nmodel instruction meta learning through the lens of generalization,”\ngeneral-purposelanguageunderstandingsystems,”Advancesinneural", "metadata": {"id": "bb5ad7971ae65396520e67b1f8e3ff9b3631dae2", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 28, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "general-purposelanguageunderstandingsystems,”Advancesinneural\narXivpreprintarXiv:2212.12017,2022. 2,11,13,15,16,17,25\ninformationprocessingsystems,vol.32,2019. 1,18,23,24\n[25] H.W.Chung,L.Hou,S.Longpre,B.Zoph,Y.Tay,W.Fedus,E.Li,\n[4] D. Adiwardana, M.-T. Luong, D. R. So, J. Hall, N. Fiedel, R. Thop-\nX.Wang,M.Dehghani,S.Brahmaetal.,“Scalinginstruction-finetuned", "metadata": {"id": "50bccc96cfd6ba859abafeae0f84ecb73982d06f", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 28, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "X.Wang,M.Dehghani,S.Brahmaetal.,“Scalinginstruction-finetuned\npilan, Z. Yang, A. Kulshreshtha, G. Nemade, Y. Lu et al., “Towards\nlanguage models,” arXiv preprint arXiv:2210.11416, 2022. 2, 8, 11,\nahuman-likeopen-domainchatbot,”arXivpreprintarXiv:2001.09977,\n13,14,15,16,17,25,26\n2020. 1,24\n[26] Y. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei,", "metadata": {"id": "af584ce2277874c407c2f74b8c3db732b5290e91", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 28, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "13,14,15,16,17,25,26\n2020. 1,24\n[26] Y. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei,\n[5] J.Devlin,M.-W.Chang,K.Lee,andK.Toutanova,“Bert:Pre-training\nA.Naik,A.Ashok,A.S.Dhanasekaran,A.Arunkumar,D.Stapetal.,\nofdeepbidirectionaltransformersforlanguageunderstanding,”arXiv\n“Super-naturalinstructions: Generalization via declarative instructions\npreprintarXiv:1810.04805,2018. 1,26", "metadata": {"id": "76cc4d83f72625531f64b03c7ed42095e01dfa4b", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 28, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "preprintarXiv:1810.04805,2018. 1,26\non 1600+ nlp tasks,” in Proceedings of the 2022 Conference on\n[6] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,\nEmpiricalMethodsinNaturalLanguageProcessing,2022,pp.5085–\nand L. Zettlemoyer, “Deep contextualized word representations,” in\n5109. 2,8,11,13,15,16,25\nNAACL-HLT. Association for Computational Linguistics, 2018, pp.", "metadata": {"id": "7fd732e086a10833cd8b91ffe1dbea7eaaf4e538", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 28, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "5109. 2,8,11,13,15,16,25\nNAACL-HLT. Association for Computational Linguistics, 2018, pp.\n2227–2237. 1 [27] B. Lester, R. Al-Rfou, and N. Constant, “The power of scale for\n[7] M.Lewis,Y.Liu,N.Goyal,M.Ghazvininejad,A.Mohamed,O.Levy, parameter-efficient prompt tuning,” arXiv preprint arXiv:2104.08691,\nV. Stoyanov, and L. Zettlemoyer, “Bart: Denoising sequence-to- 2021. 2,8", "metadata": {"id": "ed2c22de704c8df68ae201d0666a749a68416851", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 28, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "V. Stoyanov, and L. Zettlemoyer, “Bart: Denoising sequence-to- 2021. 2,8\nsequencepre-trainingfornaturallanguagegeneration,translation,and [28] J.He,C.Zhou,X.Ma,T.Berg-Kirkpatrick,andG.Neubig,“Towards\ncomprehension,”arXivpreprintarXiv:1910.13461,2019. 1 aunifiedviewofparameter-efficienttransferlearning,”arXivpreprint", "metadata": {"id": "2b41299427610c29372c8b386a74b1a393151709", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 28, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "[8] T.Brown,B.Mann,N.Ryder,M.Subbiah,J.D.Kaplan,P.Dhariwal, arXiv:2110.04366,2021. 2,7\nA.Neelakantan,P.Shyam,G.Sastry,A.Askelletal.,“Languagemod- [29] Z.Hu,Y.Lan,L.Wang,W.Xu,E.-P.Lim,R.K.-W.Lee,L.Bing,and\nelsarefew-shotlearners,”Advancesinneuralinformationprocessing S.Poria,“Llm-adapters:Anadapterfamilyforparameter-efficientfine-", "metadata": {"id": "59e1ab90bf552b8e3563f308f44ae91b6688a590", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 28, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "systems,vol.33,pp.1877–1901,2020. 1,2,8,9,10,14,15,22,25, tuning of large language models,” arXiv preprint arXiv:2304.01933,\n26 2023. 2,7\n[9] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilic´, D. Hesslow, [30] B. Lester, R. Al-Rfou, and N. Constant, “The power of scale for", "metadata": {"id": "d75ac12b15a5bab2e4b23cdfdfc22c08858caeec", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 28, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "R.Castagné,A.S.Luccioni,F.Yvon,M.Galléetal.,“Bloom:A176b- parameter-efficient prompt tuning,” arXiv preprint arXiv:2104.08691,\nparameter open-access multilingual language model,” arXiv preprint 2021. 2,7\narXiv:2211.05100,2022. 1,2,5,9,10,11,15,22,25,26 [31] X.L.LiandP.Liang,“Prefix-tuning:Optimizingcontinuousprompts", "metadata": {"id": "165e14979ce5b8a1861018f5306411c68eab9917", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 28, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "[10] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, forgeneration,”arXivpreprintarXiv:2101.00190,2021. 2,7\nC. Dewan, M. Diab, X. Li, X. V. Lin et al., “Opt: Open pre-trained [32] C.Zhou,Q.Li,C.Li,J.Yu,Y.Liu,G.Wang,K.Zhang,C.Ji,Q.Yan,\ntransformerlanguagemodels,”arXivpreprintarXiv:2205.01068,2022. L.Heetal.,“Acomprehensivesurveyonpretrainedfoundationmodels:", "metadata": {"id": "ae5c2732af1fe376939de5bbcf772feb2b3605bb", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 28, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "1,2,9,11,15,25,26 Ahistoryfromberttochatgpt,”arXivpreprintarXiv:2302.09419,2023.\n[11] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, 2,3\nY.Zhou,W.Li,andP.J.Liu,“Exploringthelimitsoftransferlearn- [33] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min,", "metadata": {"id": "4593ecf777dee756b763af1b730d4c3869218f4e", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 28, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "ing with a unified text-to-text transformer,” The Journal of Machine B. Zhang, J. Zhang, Z. Dong et al., “A survey of large language\nLearning Research, vol. 21, no. 1, pp. 5485–5551, 2020. 1, 2, 6, 8, models,”arXivpreprintarXiv:2303.18223,2023. 2,3,7,8,16\n15,22,23,24,25,26\n[34] G. Mialon, R. Dessi, M. Lomeli, C. Nalmpantis, R. Pasunuru,", "metadata": {"id": "26d1f49c9310b038683d5c12732621bdb6ddfece", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 28, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "15,22,23,24,25,26\n[34] G. Mialon, R. Dessi, M. Lomeli, C. Nalmpantis, R. Pasunuru,\n[12] L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant,\nR. Raileanu, B. Roziere, T. Schick, J. Dwivedi-Yu, A. Celikyil-\nA. Barua, and C. Raffel, “mt5: A massively multilingual pre-trained\nmaz et al., “Augmented language models: a survey,” arXiv preprint", "metadata": {"id": "79c9b58a945c3286c6c4780609f4759e2d955918", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 28, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "maz et al., “Augmented language models: a survey,” arXiv preprint\ntext-to-texttransformer,”arXivpreprintarXiv:2010.11934,2020. 1,2,\narXiv:2302.07842,2023. 2\n6,8,15,23,24,26\n[35] U.Naseem,I.Razzak,S.K.Khan,andM.Prasad,“Acomprehensive\n[13] Z. Zhang, Y. Gu, X. Han, S. Chen, C. Xiao, Z. Sun, Y. Yao, F. Qi,\nsurveyonwordrepresentationmodels:Fromclassicaltostate-of-the-", "metadata": {"id": "9ab13beb92f9213f04f450c0817c6b335ec0fb32", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 28, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "surveyonwordrepresentationmodels:Fromclassicaltostate-of-the-\nJ. Guan, P. Ke et al., “Cpm-2: Large-scale cost-effective pre-trained\nartwordrepresentationlanguagemodels,”TransactionsonAsianand\nlanguagemodels,”AIOpen,vol.2,pp.216–224,2021. 1,8,15\nLow-Resource Language Information Processing, vol. 20, no. 5, pp.\n[14] A.Chowdhery,S.Narang,J.Devlin,M.Bosma,G.Mishra,A.Roberts,\n1–35,2021. 3", "metadata": {"id": "3192c2d47903b74eb1ef0a18c4a038a9651f7ba7", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 28, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "[14] A.Chowdhery,S.Narang,J.Devlin,M.Bosma,G.Mishra,A.Roberts,\n1–35,2021. 3\nP.Barham,H.W.Chung,C.Sutton,S.Gehrmannetal.,“Palm:Scaling\n[36] B.Min,H.Ross,E.Sulem,A.P.B.Veyseh,T.H.Nguyen,O.Sainz,\nlanguagemodelingwithpathways,”arXivpreprintarXiv:2204.02311,\nE.Agirre,I.Heinz,andD.Roth,“Recentadvancesinnaturallanguage\n2022. 1,2,10,15,22,25,26", "metadata": {"id": "0f2cc65ae451d2ae69fa1e4fbc5b5fdc5749349f", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 28, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "E.Agirre,I.Heinz,andD.Roth,“Recentadvancesinnaturallanguage\n2022. 1,2,10,15,22,25,26\nprocessing via large pre-trained language models: A survey,” arXiv\n[15] Y.Tay,M.Dehghani,V.Q.Tran,X.Garcia,J.Wei,X.Wang,H.W.\npreprintarXiv:2111.01243,2021. 3\nChung,D.Bahri,T.Schuster,S.Zhengetal.,“Ul2:Unifyinglanguage", "metadata": {"id": "6632729344657f090b829c11cbf0cfefa72442a8", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 28, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "preprintarXiv:2111.01243,2021. 3\nChung,D.Bahri,T.Schuster,S.Zhengetal.,“Ul2:Unifyinglanguage\nlearning paradigms,” in The Eleventh International Conference on [37] J. J. Webster and C. Kit, “Tokenization as the initial phase in nlp,”\nLearningRepresentations,2022. 2,6,10,15,25,26 in COLING 1992 volume 4: The 14th international conference on\ncomputationallinguistics,1992. 4", "metadata": {"id": "1b79c0b7da416fec82ce052af0dc56e049cb8570", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 28, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "computationallinguistics,1992. 4\n[16] “Commoncrawl.”[Online].Available:https://commoncrawl.org/ 2\n[17] “Wikipedia.” [Online]. Available: https://en.wikipedia.org/wiki/Main_ [38] T.Kudo,“Subwordregularization:Improvingneuralnetworktransla-\nPage 2,24 tionmodelswithmultiplesubwordcandidates,”inProceedingsofthe", "metadata": {"id": "5d0c254b126439d52164f5476a617a4dc4e2ada5", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 28, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "Page 2,24 tionmodelswithmultiplesubwordcandidates,”inProceedingsofthe\n[18] “Openwebtext corpus.” [Online]. Available: http://Skylion007.github. 56thAnnualMeetingoftheAssociationforComputationalLinguistics\nio/OpenWebTextCorpus 2 (Volume1:LongPapers),2018,pp.66–75. 4", "metadata": {"id": "ad18178a724b2bea8aab3ce7755fa7e9679c8fd2", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 28, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "io/OpenWebTextCorpus 2 (Volume1:LongPapers),2018,pp.66–75. 4\n[19] “Bigquery dataset.” [Online]. Available: https://cloud.google.com/ [39] R. Sennrich, B. Haddow, and A. Birch, “Neural machine translation\nbigquery?hl=zh-cn 2 ofrarewordswithsubwordunits,”inProceedingsofthe54thAnnual", "metadata": {"id": "88f389cdd47905a51cd7ba4acaa76f2754010442", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 28, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "bigquery?hl=zh-cn 2 ofrarewordswithsubwordunits,”inProceedingsofthe54thAnnual\n[20] Y.Tay,J.Wei,H.W.Chung,V.Q.Tran,D.R.So,S.Shakeri,X.Gar- Meeting of the Association for Computational Linguistics (Volume 1:\ncia,H.S.Zheng,J.Rao,A.Chowdheryetal.,“Transcendingscaling LongPapers),2016,pp.1715–1725. 4", "metadata": {"id": "2ab53e28e5f10e7f0068ccc53455a2bbf8ad4a1e", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 28, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "cia,H.S.Zheng,J.Rao,A.Chowdheryetal.,“Transcendingscaling LongPapers),2016,pp.1715–1725. 4\nlaws with 0.1% extra compute,” arXiv preprint arXiv:2210.11399, [40] S. J. Mielke, Z. Alyafeai, E. Salesky, C. Raffel, M. Dey, M. Gallé,\n2022. 2,6,10,15,25 A.Raja,C.Si,W.Y.Lee,B.Sagotetal.,“Betweenwordsandchar-", "metadata": {"id": "32bf684720eb72f6d630899be33302183700ddab", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 28, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "2022. 2,6,10,15,25 A.Raja,C.Si,W.Y.Lee,B.Sagotetal.,“Betweenwordsandchar-\n[21] S. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari, acters: A brief history of open-vocabulary modeling and tokenization\nJ.Casper,Z.Liu,S.Prabhumoye,G.Zerveas,V.Korthikantietal.,“Us- innlp,”arXivpreprintarXiv:2112.10508,2021. 4", "metadata": {"id": "137acf807c88fb358eaec67b7bb221517546f039", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 28, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "ingdeepspeedandmegatrontotrainmegatron-turingnlg530b,alarge- [41] M.SchusterandK.Nakajima,“Japaneseandkoreanvoicesearch,”in\nscale generative language model,” arXiv preprint arXiv:2201.11990, 2012 IEEE international conference on acoustics, speech and signal\n2022. 2,9,15,25 processing(ICASSP). IEEE,2012,pp.5149–5152. 4", "metadata": {"id": "d34a69c20514b70c48193e4fabf299dde7b77ea7", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 28, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "JOURNALOFLATEX 28\n[42] C. W. Eriksen and J. E. Hoffman, “Some characteristics of selective [66] J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary,\nattention in visual perception determined by vocal reaction time,” D. Maclaurin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-", "metadata": {"id": "ce79489c61ef6a200d586fb1ae28fd9596258c59", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 29, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "Perception&Psychophysics,vol.11,no.2,pp.169–171,1972. 4 Milne et al., “Jax: composable transformations of python+ numpy\n[43] D.Bahdanau,K.Cho,andY.Bengio,“Neuralmachinetranslationby programs,”2018. 5\njointlylearningtoalignandtranslate,”arXivpreprintarXiv:1409.0473, [67] S. Li, J. Fang, Z. Bian, H. Liu, Y. Liu, H. Huang, B. Wang, and", "metadata": {"id": "29aa35c6ed4d2cfce7af476546d24054a825165c", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 29, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "2014. 4 Y. You, “Colossal-ai: A unified deep learning system for large-scale\n[44] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. paralleltraining,”arXivpreprintarXiv:2110.14883,2021. 5\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” [68] J.He,J.Qiu,A.Zeng,Z.Yang,J.Zhai,andJ.Tang,“Fastmoe:Afast", "metadata": {"id": "f053cf4f043f66b63303d9ecabd84fc0479079e9", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 29, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "Advancesinneuralinformationprocessingsystems,vol.30,2017. 4, mixture-of-expert training system,” arXiv preprint arXiv:2103.13262,\n5,8 2021. 5\n[45] R. Child, S. Gray, A. Radford, and I. Sutskever, “Generating long [69] L. Huawei Technologies Co., “Huawei mindspore ai development", "metadata": {"id": "2a6e911e16ffcc9f609eb1afef3f4d9b43f3aad9", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 29, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "sequenceswithsparsetransformers,”arXivpreprintarXiv:1904.10509, framework,”inArtificialIntelligenceTechnology. Springer,2022,pp.\n2019. 4,8,22 137–162. 5\n[46] T. Dao, D.Fu, S. Ermon, A.Rudra, and C. Ré,“Flashattention: Fast [70] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,", "metadata": {"id": "ebeea9dc051579fdc876c71b45e90769a00786dd", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 29, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "andmemory-efficientexactattentionwithio-awareness,”Advancesin T. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., “Pytorch: An\nNeural Information Processing Systems, vol. 35, pp. 16344–16359, imperative style, high-performance deep learning library,” Advances\n2022. 4 inneuralinformationprocessingsystems,vol.32,2019. 5", "metadata": {"id": "44f3ac92fb99e122602eb951f54d0d71ddf763bc", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 29, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "2022. 4 inneuralinformationprocessingsystems,vol.32,2019. 5\n[47] O. Press, N. Smith, and M. Lewis, “Train short, test long: Attention [71] M.Abadi,P.Barham,J.Chen,Z.Chen,A.Davis,J.Dean,M.Devin,\nwithlinearbiasesenablesinputlengthextrapolation,”inInternational S. Ghemawat, G. Irving, M. Isard et al., “Tensorflow: a system for", "metadata": {"id": "44c1a3b50879a7522c18e386b225183953b6c1b8", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 29, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "Conference on Learning Representations, 2022. [Online]. Available: large-scalemachinelearning.”inOsdi,vol.16,no.2016. Savannah,\nhttps://openreview.net/forum?id=R8sQPpGCv0 4 GA,USA,2016,pp.265–283. 5\n[48] J. Su, Y. Lu, S. Pan, A. Murtadha, B. Wen, and Y. Liu, “Roformer: [72] T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang, T. Xiao, B. Xu,", "metadata": {"id": "b11a9dc42632191dd39081e1e24e3674b6d7e7f5", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 29, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "Enhancedtransformerwithrotarypositionembedding,”arXivpreprint C. Zhang, and Z. Zhang, “Mxnet: A flexible and efficient machine\narXiv:2104.09864,2021. 4,9 learninglibraryforheterogeneousdistributedsystems,”arXivpreprint\n[49] A. Kazemnejad, I. Padhi, K. N. Ramamurthy, P. Das, and S. Reddy, arXiv:1512.01274,2015. 5", "metadata": {"id": "a181e9f85b1ba3906ecf4022cbb865299cb4377a", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 29, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "[49] A. Kazemnejad, I. Padhi, K. N. Ramamurthy, P. Das, and S. Reddy, arXiv:1512.01274,2015. 5\n“Theimpactofpositionalencodingonlengthgeneralizationintrans- [73] P.J.Liu*,M.Saleh*,E.Pot,B.Goodrich,R.Sepassi,L.Kaiser,and\nformers,”arXivpreprintarXiv:2305.19466,2023. 4 N. Shazeer, “Generating wikipedia by summarizing long sequences,”", "metadata": {"id": "48b61cdae130d4e53b17be7781731c81a1e3cec1", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 29, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "[50] K. Hornik, M. Stinchcombe, and H. White, “Multilayer feedforward in International Conference on Learning Representations, 2018.\nnetworksareuniversalapproximators,”Neuralnetworks,vol.2,no.5,\n[Online].Available:https://openreview.net/forum?id=Hyg0vbWC- 6\npp.359–366,1989. 5\n[74] T.Wang,A.Roberts,D.Hesslow,T.LeScao,H.W.Chung,I.Beltagy,", "metadata": {"id": "b54ec4c70830db00bacb45a36dadc25a57f58e36", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 29, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "pp.359–366,1989. 5\n[74] T.Wang,A.Roberts,D.Hesslow,T.LeScao,H.W.Chung,I.Beltagy,\n[51] V. Nair and G. E. Hinton, “Rectified linear units improve restricted\nJ. Launay, and C. Raffel, “What language model architecture and\nboltzmannmachines,”inProceedingsofthe27thinternationalconfer-\npretraining objective works best for zero-shot generalization?” in\nenceonmachinelearning(ICML-10),2010,pp.807–814. 5", "metadata": {"id": "6ec4ba7d6e114f1273d9dd8890aa4bcf7c2ece3f", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 29, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "enceonmachinelearning(ICML-10),2010,pp.807–814. 5\nInternational Conference on Machine Learning. PMLR, 2022, pp.\n[52] D. Hendrycks and K. Gimpel, “Gaussian error linear units (gelus),”\n22964–22984. 6\narXivpreprintarXiv:1606.08415,2016. 5\n[75] L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y. Wang, J. Gao,\n[53] N.Srivastava,G.Hinton,A.Krizhevsky,I.Sutskever,andR.Salakhut-", "metadata": {"id": "161c26eb19bf498206d73ab1cfb899199b4b5913", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 29, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "[53] N.Srivastava,G.Hinton,A.Krizhevsky,I.Sutskever,andR.Salakhut-\nM. Zhou, and H.-W. Hon, “Unified language model pre-training for\ndinov, “Dropout: a simple way to prevent neural networks from\nnatural language understanding and generation,” Advances in neural\noverfitting,”Thejournalofmachinelearningresearch,vol.15,no.1,\ninformationprocessingsystems,vol.32,2019. 6\npp.1929–1958,2014. 5", "metadata": {"id": "8ee6e65bdbdec0701d70d10bf59b311bdc8a9e1b", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 29, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "informationprocessingsystems,vol.32,2019. 6\npp.1929–1958,2014. 5\n[76] X.Liu,Y.Zheng,Z.Du,M.Ding,Y.Qian,Z.Yang,andJ.Tang,“Gpt\n[54] D.Krueger,T.Maharaj,J.Kramár,M.Pezeshki,N.Ballas,N.R.Ke,\nunderstands,too,”arXivpreprintarXiv:2103.10385,2021. 7\nA. Goyal, Y. Bengio, A. Courville, and C. Pal, “Zoneout: Regulariz-\n[77] N.Houlsby,A.Giurgiu,S.Jastrzebski,B.Morrone,Q.DeLaroussilhe,", "metadata": {"id": "4dacde7bb4f8719beae88a6377e350ed78cd6aa2", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 29, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "[77] N.Houlsby,A.Giurgiu,S.Jastrzebski,B.Morrone,Q.DeLaroussilhe,\ning rnns by randomly preserving hidden activations,” arXiv preprint\nA.Gesmundo,M.Attariyan,andS.Gelly,“Parameter-efficienttransfer\narXiv:1606.01305,2016. 5\nlearning for nlp,” in International Conference on Machine Learning.\n[55] N. Shazeer, “Glu variants improve transformer,” arXiv preprint\nPMLR,2019,pp.2790–2799. 7,8", "metadata": {"id": "6d6b6fa1cd29e88b1fe8e1b49998b70d8856e193", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 29, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "[55] N. Shazeer, “Glu variants improve transformer,” arXiv preprint\nPMLR,2019,pp.2790–2799. 7,8\narXiv:2002.05202,2020. 5\n[78] A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan,\n[56] Y.N.Dauphin,A.Fan,M.Auli,andD.Grangier,“Languagemodeling\nA. Jones, N. Joseph, B. Mann, N. DasSarma et al., “A general\nwith gated convolutional networks,” in International conference on", "metadata": {"id": "3fa3dbf29047a64f32efd589207e8f2e797f9218", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 29, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "with gated convolutional networks,” in International conference on\nlanguage assistant as a laboratory for alignment,” arXiv preprint\nmachinelearning. PMLR,2017,pp.933–941. 5\narXiv:2112.00861,2021. 7\n[57] B. Zhang and R. Sennrich, “Root mean square layer normalization,”\n[79] D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford,\nAdvancesinNeuralInformationProcessingSystems,vol.32,2019. 5", "metadata": {"id": "ae0af851520dba09e84cac21a72472509916d65b", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 29, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "AdvancesinNeuralInformationProcessingSystems,vol.32,2019. 5\nD.Amodei,P.Christiano,andG.Irving,“Fine-tuninglanguagemodels\n[58] A. Baevski and M. Auli, “Adaptive input representations for neural\nfromhumanpreferences,”arXivpreprintarXiv:1909.08593,2019. 7\nlanguagemodeling,”arXivpreprintarXiv:1809.10853,2018. 5", "metadata": {"id": "7d21e6117097a1f20cbc9a4184059a3e5cca8939", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 29, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "languagemodeling,”arXivpreprintarXiv:1809.10853,2018. 5\n[59] S. Shleifer, J. Weston, and M. Ott, “Normformer: Improved [80] S. Kim, S. J. Joo, D. Kim, J. Jang, S. Ye, J. Shin, and M. Seo,\ntransformer pretraining with extra normalization,” arXiv preprint “The cot collection: Improving zero-shot and few-shot learning of", "metadata": {"id": "1f95ea2c05b3674339519aabde04ce82f8a6cc28", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 29, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "arXiv:2110.09456,2021. 5 language models via chain-of-thought fine-tuning,” arXiv preprint\n[60] H. Wang, S. Ma, L. Dong, S. Huang, D. Zhang, and F. Wei, arXiv:2305.14045,2023. 8,13\n“Deepnet: Scaling transformers to 1,000 layers,” arXiv preprint [81] Q. Liu, F. Zhou, Z. Jiang, L. Dou, and M. Lin, “From zero to hero:", "metadata": {"id": "c9853ca8ad9996b51ad94b91fdb6315f7abfdf05", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 29, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "arXiv:2203.00555,2022. 5 Examining the power of symbolic tasks in instruction tuning,” arXiv\n[61] S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He, “Zero: Memory preprintarXiv:2304.07995,2023. 8,13\noptimizationstowardtrainingtrillionparametermodels,”inSC20:In- [82] E. Saravia, “Prompt Engineering Guide,” https://github.com/dair-", "metadata": {"id": "ab3f1410e3f3603e24a535b7477e49d6197da5fd", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 29, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "ternationalConferenceforHighPerformanceComputing,Networking, ai/Prompt-Engineering-Guide,122022. 8\nStorageandAnalysis. IEEE,2020,pp.1–16. 5,25 [83] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun,\n[62] M.Shoeybi,M.Patwary,R.Puri,P.LeGresley,J.Casper,andB.Catan- J. Xu, and Z. Sui, “A survey for in-context learning,” arXiv preprint", "metadata": {"id": "7c7557f32338560bed9a09d78e71b6d0aacd5e2e", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 29, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "zaro,“Megatron-lm:Trainingmulti-billionparameterlanguagemodels arXiv:2301.00234,2022. 8\nusingmodelparallelism,”arXivpreprintarXiv:1909.08053,2019. 5 [84] J. Huang and K. C.-C. Chang, “Towards reasoning in large language\n[63] “\"bmtrain: Efficient training for big models.\".” [Online]. Available: models:Asurvey,”arXivpreprintarXiv:2212.10403,2022. 8", "metadata": {"id": "68824a01d48631cd0028fe0b16704ac2447d43a9", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 29, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "https://github.com/OpenBMB/BMTrain 5 [85] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V.\n[64] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, Le, D. Zhou et al., “Chain-of-thought prompting elicits reasoning in\nP. Cistac, T. Rault, R. Louf, M. Funtowicz et al., “Transformers: large language models,” Advances in Neural Information Processing", "metadata": {"id": "14aba7ee93bd5c664128bf300a16aece86cc1e5a", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 29, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "State-of-the-art natural language processing,” in Proceedings of the Systems,vol.35,pp.24824–24837,2022. 8,17\n2020conferenceonempiricalmethodsinnaturallanguageprocessing: [86] X.Wang,J.Wei,D.Schuurmans,Q.Le,E.Chi,S.Narang,A.Chowd-\nsystemdemonstrations,2020,pp.38–45. 5 hery, and D. Zhou, “Self-consistency improves chain of thought rea-", "metadata": {"id": "4b615194c36b8105c9f859e59e876117d1f1d07e", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 29, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "[65] J. Rasley, S. Rajbhandari, O. Ruwase, and Y. He, “Deepspeed: Sys- soning in language models,” arXiv preprint arXiv:2203.11171, 2022.\ntem optimizations enable training deep learning models with over 8\n100 billion parameters,” in Proceedings of the 26th ACM SIGKDD [87] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and", "metadata": {"id": "a0773b893c9dafa3aa0f14c598241065140de810", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 29, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "International Conference on Knowledge Discovery & Data Mining, K. Narasimhan, “Tree of thoughts: Deliberate problem solving with\n2020,pp.3505–3506. 5 largelanguagemodels,”arXivpreprintarXiv:2305.10601,2023. 8", "metadata": {"id": "7b072f4193ed66e023ff0cecf7d47a841a854b3d", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 29, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "JOURNALOFLATEX 29\n[88] A.Radford,J.Wu,R.Child,D.Luan,D.Amodei,I.Sutskeveretal., [109] A.Zeng,X.Liu,Z.Du,Z.Wang,H.Lai,M.Ding,Z.Yang,Y.Xu,\n“Languagemodelsareunsupervisedmultitasklearners,”OpenAIblog, W. Zheng, X. Xia et al., “Glm-130b: An open bilingual pre-trained\nvol.1,no.8,p.9,2019. 8 model,”arXivpreprintarXiv:2210.02414,2022. 10,15,22,25", "metadata": {"id": "ac2ee3dfb1de47b04eb23b90f04e39d6b72313f7", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 30, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "vol.1,no.8,p.9,2019. 8 model,”arXivpreprintarXiv:2210.02414,2022. 10,15,22,25\n[89] S.McCandlish,J.Kaplan,D.Amodei,andO.D.Team,“Anempirical [110] Z. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, and J. Tang,\nmodeloflarge-batchtraining,”arXivpreprintarXiv:1812.06162,2018. “Glm: General language model pretraining with autoregressive blank", "metadata": {"id": "811beabe606c29ab74cea71cd81dd0c2416afeb0", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 30, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "8 infilling,”inProceedingsofthe60thAnnualMeetingoftheAssociation\n[90] W.Zeng,X.Ren,T.Su,H.Wang,Y.Liao,Z.Wang,X.Jiang,Z.Yang, forComputationalLinguistics(Volume1:LongPapers),2022,pp.320–\nK. Wang, X. Zhang et al., “Pangu-α : Large-scale autoregressive 335. 10\npretrained chinese language models with auto-parallel computation,” [111] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,", "metadata": {"id": "3f0442613994fe307945921e1abaaef1de175c02", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 30, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "arXivpreprintarXiv:2104.12369,2021. 8,15,22,25 T.Lacroix,B.Rozière,N.Goyal,E.Hambro,F.Azharetal.,“Llama:\n[91] S.Yuan,H.Zhao,Z.Du,M.Ding,X.Liu,Y.Cen,X.Zou,Z.Yang, Open and efficient foundation language models,” arXiv preprint\nandJ.Tang,“Wudaocorpora:Asuperlarge-scalechinesecorporafor arXiv:2302.13971,2023. 10,15,22", "metadata": {"id": "cf8c6a945a9e499f43568064605cb4da7dea4237", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 30, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "andJ.Tang,“Wudaocorpora:Asuperlarge-scalechinesecorporafor arXiv:2302.13971,2023. 10,15,22\npre-training language models,” AI Open, vol. 2, pp. 65–68, 2021. 8, [112] H. Touvron,L. Martin, K.Stone, P. Albert,A. Almahairi,Y. Babaei,\n23,24 N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., “Llama", "metadata": {"id": "650a9254fe20a73920cae033186973b00daf9252", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 30, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "23,24 N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., “Llama\n[92] Y.Sun,S.Wang,S.Feng,S.Ding,C.Pang,J.Shang,J.Liu,X.Chen, 2: Open foundation and fine-tuned chat models,” arXiv preprint\nY. Zhao, Y. Lu et al., “Ernie 3.0: Large-scale knowledge enhanced arXiv:2307.09288,2023. 10,15", "metadata": {"id": "ef07db7aec98f3f3c8973df6f8b6230869568aeb", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 30, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "Y. Zhao, Y. Lu et al., “Ernie 3.0: Large-scale knowledge enhanced arXiv:2307.09288,2023. 10,15\npre-trainingforlanguageunderstandingandgeneration,”arXivpreprint [113] M.N.RabeandC.Staats,“Self-attentiondoesnotneedo(n2)memory,”\narXiv:2107.02137,2021. 9,15 arXivpreprintarXiv:2112.05682,2021. 10", "metadata": {"id": "6fecb87a6e8d837063021ee714609896d203dde9", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 30, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "arXiv:2107.02137,2021. 9,15 arXivpreprintarXiv:2112.05682,2021. 10\n[93] Z.Dai,Z.Yang,Y.Yang,J.Carbonell,Q.V.Le,andR.Salakhutdinov, [114] V. A. Korthikanti, J. Casper, S. Lym, L. McAfee, M. Andersch,\n“Transformer-xl: Attentive language models beyond a fixed-length M. Shoeybi, and B. Catanzaro, “Reducing activation recomputation", "metadata": {"id": "86bb9b54748b14e7fea71f4982f71614b8d4e466", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 30, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "context,”arXivpreprintarXiv:1901.02860,2019. 9 in large transformer models,” Proceedings of Machine Learning and\n[94] O.Lieber,O.Sharir,B.Lenz,andY.Shoham,“Jurassic-1:Technical Systems,vol.5,2023. 10\ndetails and evaluation,” White Paper. AI21 Labs, vol. 1, 2021. 9, 15, [115] X. Ren, P. Zhou, X. Meng, X. Huang, Y. Wang, W. Wang, P. Li,", "metadata": {"id": "1e94e7b5eae346ed37ed64cddd24e9af5a182693", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 30, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "25 X.Zhang,A.Podolskiy,G.Arshinovetal.,“Pangu-(cid:80):Towardstrillion\n[95] Y. Levine, N. Wies, O. Sharir, H. Bata, and A. Shashua, “Limits to parameter language model with sparse heterogeneous computing,”\ndepth efficiencies of self-attention,” Advances in Neural Information arXivpreprintarXiv:2303.10845,2023. 10,11,15,22,25", "metadata": {"id": "b43722de5df81652dede39fd81a270c9cd55846e", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 30, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "ProcessingSystems,vol.33,pp.22640–22651,2020. 9 [116] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou,\n[96] B. Kim, H. Kim, S.-W. Lee, G. Lee, D. Kwak, D. H. Jeon, S. Park, S. Savarese, and C. Xiong, “Codegen: An open large language\nS.Kim,S.Kim,D.Seoetal.,“Whatchangescanlarge-scalelanguage model for code with multi-turn program synthesis,” arXiv preprint", "metadata": {"id": "0f458c2902842ca66579cba92919545b39c22756", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 30, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "models bring? intensive study on hyperclova: Billions-scale korean arXiv:2203.13474,2022. 10,15,22,24\ngenerativepretrainedtransformers,”arXivpreprintarXiv:2109.04650, [117] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan,\n2021. 9,15 H.Edwards,Y.Burda,N.Joseph,G.Brockmanetal.,“Evaluatinglarge", "metadata": {"id": "5ed694ba59f25bef663bb861bfa97eb63260512f", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 30, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "2021. 9,15 H.Edwards,Y.Burda,N.Joseph,G.Brockmanetal.,“Evaluatinglarge\n[97] S. Wu, X. Zhao, T. Yu, R. Zhang, C. Shen, H. Liu, F. Li, H. Zhu, language models trained on code,” arXiv preprint arXiv:2107.03374,\nJ.Luo,L.Xuetal.,“Yuan1.0:Large-scalepre-trainedlanguagemodel 2021. 10,15", "metadata": {"id": "6cff2fc5fb7fe949c78d532aa5beddbb764930f8", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 30, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "J.Luo,L.Xuetal.,“Yuan1.0:Large-scalepre-trainedlanguagemodel 2021. 10,15\ninzero-shotandfew-shotlearning,”arXivpreprintarXiv:2110.04725, [118] Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond,\n2021. 9,15,25 T. Eccles, J. Keeling, F. Gimeno, A. Dal Lago et al., “Competition-", "metadata": {"id": "e4306165e454d0adea5f27b1a49063f83bbce72a", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 30, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "2021. 9,15,25 T. Eccles, J. Keeling, F. Gimeno, A. Dal Lago et al., “Competition-\n[98] J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song, levelcodegenerationwithalphacode,”Science,vol.378,no.6624,pp.\nJ.Aslanides,S.Henderson,R.Ring,S.Youngetal.,“Scalinglanguage 1092–1097,2022. 11,15,22,24", "metadata": {"id": "a764a64ace8434a92e2c30f54042bdf837ad3201", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 30, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "J.Aslanides,S.Henderson,R.Ring,S.Youngetal.,“Scalinglanguage 1092–1097,2022. 11,15,22,24\nmodels: Methods, analysis & insights from training gopher,” arXiv [119] N. Shazeer, “Fast transformer decoding: One write-head is all you\npreprintarXiv:2112.11446,2021. 9,10,15,23,24 need,”arXivpreprintarXiv:1911.02150,2019. 11", "metadata": {"id": "3d75102da65a76db26afe6b4dfe25f129428fbed", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 30, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "preprintarXiv:2112.11446,2021. 9,10,15,23,24 need,”arXivpreprintarXiv:1911.02150,2019. 11\n[99] S. Wang, Y. Sun, Y. Xiang, Z. Wu, S. Ding, W. Gong, S. Feng, [120] R.Y.PangandH.He,“Textgenerationbylearningfromdemonstra-\nJ. Shang, Y. Zhao, C. Pang et al., “Ernie 3.0 titan: Exploring larger- tions,”arXivpreprintarXiv:2009.07839,2020. 11", "metadata": {"id": "4b464e5c34f542fe9149446d4e115b03d6cbf0bc", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 30, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "scaleknowledgeenhancedpre-trainingforlanguageunderstandingand [121] R. Dabre and A. Fujita, “Softmax tempering for training neural\ngeneration,”arXivpreprintarXiv:2112.12731,2021. 9,15,25 machine translation models,” arXiv preprint arXiv:2009.09372, 2020.\n[100] S. Black, S. Biderman, E. Hallahan, Q. Anthony, L. Gao, L. Gold- 11", "metadata": {"id": "1aa3029bc9df09f81b53f20aca920a655571869f", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 30, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "[100] S. Black, S. Biderman, E. Hallahan, Q. Anthony, L. Gao, L. Gold- 11\ning, H. He, C. Leahy, K. McDonell, J. Phang et al., “Gpt-neox- [122] Y. Wang, H. Le, A. D. Gotmare, N. D. Bui, J. Li, and S. C. Hoi,\n20b: An open-source autoregressive language model,” arXiv preprint “Codet5+: Open code large language models for code understanding", "metadata": {"id": "913328ce9063daefc3dba35b5f13c161a8d9c0d2", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 30, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "arXiv:2204.06745,2022. 9,22,25,26 andgeneration,”arXivpreprintarXiv:2305.07922,2023. 11,15,26\n[101] W.BenandK.Aran,“Gpt-j-6b:A6billionparameterautoregressive [123] Y.Wang,W.Wang,S.Joty,andS.C.Hoi,“Codet5:Identifier-aware\nlanguagemodel,”2021. 9 unifiedpre-trainedencoder-decodermodelsforcodeunderstandingand", "metadata": {"id": "56f98a2f46d7e38b4245934774ad3be8ad2299cb", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 30, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "languagemodel,”2021. 9 unifiedpre-trainedencoder-decodermodelsforcodeunderstandingand\n[102] P.Micikevicius,S.Narang,J.Alben,G.Diamos,E.Elsen,D.Garcia, generation,”arXivpreprintarXiv:2109.00859,2021. 11\nB. Ginsburg, M. Houston, O. Kuchaiev, G. Venkatesh et al., “Mixed [124] R. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou,", "metadata": {"id": "df1336f3345b58a0344e886a07b14be88124b1bd", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 30, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "precisiontraining,”arXivpreprintarXiv:1710.03740,2017. 9 M.Marone,C.Akiki,J.Li,J.Chimetal.,“Starcoder:maythesource\n[103] N.Du,Y.Huang,A.M.Dai,S.Tong,D.Lepikhin,Y.Xu,M.Krikun, bewithyou!”arXivpreprintarXiv:2305.06161,2023. 11,15\nY. Zhou, A. W. Yu, O. Firat et al., “Glam: Efficient scaling of [125] R.Taylor,M.Kardas,G.Cucurull,T.Scialom,A.Hartshorn,E.Sar-", "metadata": {"id": "d8f61535d55675a6a77ee896929b7b34d14548e9", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 30, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "languagemodelswithmixture-of-experts,”inInternationalConference avia, A. Poulton, V. Kerkez, and R. Stojnic, “Galactica: A large\nonMachineLearning. PMLR,2022,pp.5547–5569. 9,15,22,25 languagemodelforscience,”arXivpreprintarXiv:2211.09085,2022.\n[104] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, 11,15,24,25", "metadata": {"id": "5f07c4af6fe8856610cf3ef6a61643552444f29f", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 30, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "[104] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, 11,15,24,25\nandJ.Dean,“Outrageouslylargeneuralnetworks:Thesparsely-gated [126] FairScale authors, “Fairscale: A general purpose modular pytorch\nmixture-of-experts layer,” arXiv preprint arXiv:1701.06538, 2017. 9, library for high performance and large scale training,” https://github.", "metadata": {"id": "7fe6cd98a1897d39c9be66c7ef3a501ad0bfd988", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 30, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "22 com/facebookresearch/fairscale,2021. 11\n[105] W. Fedus, B. Zoph, and N. Shazeer, “Switch transformers: Scaling [127] R.Thoppilan,D.DeFreitas,J.Hall,N.Shazeer,A.Kulshreshtha,H.-T.\nto trillion parameter models with simple and efficient sparsity,” The Cheng,A.Jin,T.Bos,L.Baker,Y.Duetal.,“Lamda:Languagemodels", "metadata": {"id": "b8e7b00d4fbe8115fb110616c236c9c203b707dc", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 30, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "JournalofMachineLearningResearch,vol.23,no.1,pp.5232–5270, for dialog applications,” arXiv preprint arXiv:2201.08239, 2022. 11,\n2022. 9 15,24\n[106] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, [128] S. Wu, O. Irsoy, S. Lu, V. Dabravolski, M. Dredze, S. Gehrmann,", "metadata": {"id": "8a476fa561da51e804cea7c835e14375a51bfbbf", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 30, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "E.Rutherford,D.d.L.Casas,L.A.Hendricks,J.Welbl,A.Clarketal., P. Kambadur, D. Rosenberg, and G. Mann, “Bloomberggpt: A large\n“Training compute-optimal large language models,” arXiv preprint language model for finance,” arXiv preprint arXiv:2303.17564, 2023.\narXiv:2203.15556,2022. 9,15,24 11,15,24", "metadata": {"id": "e238eeca79842066f01d07040b5c895e85e39661", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 30, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "arXiv:2203.15556,2022. 9,15,24 11,15,24\n[107] S. Soltan, S. Ananthakrishnan, J. FitzGerald, R. Gupta, W. Hamza, [129] Y. Levine, N. Wies, O. Sharir, H. Bata, and A. Shashua, “Limits to\nH. Khan, C. Peris, S. Rawls, A. Rosenbaum, A. Rumshisky et al., depth efficiencies of self-attention,” Advances in Neural Information", "metadata": {"id": "6031e0b595fe5e62e59447b642032b8c53f7f5ea", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 30, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "“Alexatm 20b: Few-shot learning using a large-scale multilingual ProcessingSystems,vol.33,pp.22640–22651,2020. 11\nseq2seq model,” arXiv preprint arXiv:2208.01448, 2022. 10, 15, 22, [130] X. Zhang, Q. Yang, and D. Xu, “Xuanyuan 2.0: A large chinese\n25,26 financial chat model with hundreds of billions parameters,” arXiv", "metadata": {"id": "8ef90990e72e72c54ee907d49d85571914810519", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 30, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "25,26 financial chat model with hundreds of billions parameters,” arXiv\n[108] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, preprintarXiv:2305.12002,2023. 11,15,16\nS. Shakeri, E. Taropa, P. Bailey, Z. Chen et al., “Palm 2 technical [131] W. Ben, “Mesh-transformer-jax: Model-parallel implementation of", "metadata": {"id": "0f407b3c3491124eaf3f5b16ddc5f83bd94b3329", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 30, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "report,”arXivpreprintarXiv:2305.10403,2023. 10,15 transformerlanguagemodelwithjax,”2021. 12,25", "metadata": {"id": "1ff13f33ee3b7e4b70d6cb0b8f8fcafa7bf7642f", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 30, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "JOURNALOFLATEX 30\n[132] S. Black, S. Biderman, E. Hallahan, Q. Anthony, L. Gao, L. Gold- [155] D. Ganguli, A. Askell, N. Schiefer, T. Liao, K. Lukošiu¯te˙, A. Chen,\ning, H. He, C. Leahy, K. McDonell, J. Phang et al., “Gpt-neox- A.Goldie,A.Mirhoseini,C.Olsson,D.Hernandezetal.,“Thecapacity", "metadata": {"id": "32b8085111f83b808c15cf407dbe00024316c2f2", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 31, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "20b: An open-source autoregressive language model,” arXiv preprint for moral self-correction in large language models,” arXiv preprint\narXiv:2204.06745,2022. 15 arXiv:2302.07459,2023. 16\n[133] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, [156] A. Wei, N. Haghtalab, and J. Steinhardt, “Jailbroken: How does llm", "metadata": {"id": "b2bffd15729995a76aa68ce57bd11f4894b2251e", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 31, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "C.Hesse,S.Jain,V.Kosaraju,W.Saundersetal.,“Webgpt:Browser- safetytrainingfail?”arXivpreprintarXiv:2307.02483,2023. 16\nassisted question-answering with human feedback,” arXiv preprint [157] D. Ganguli, L. Lovitt, J. Kernion, A. Askell, Y. Bai, S. Kadavath,\narXiv:2112.09332,2021. 15,25 B. Mann, E. Perez, N. Schiefer, K. Ndousse et al., “Red teaming", "metadata": {"id": "4231764b7730b83200ac7dcbd5d39cd46433a72e", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 31, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "arXiv:2112.09332,2021. 15,25 B. Mann, E. Perez, N. Schiefer, K. Ndousse et al., “Red teaming\n[134] N. Muennighoff, T. Wang, L. Sutawika, A. Roberts, S. Biderman, language models to reduce harms: Methods, scaling behaviors, and\nT. L. Scao, M. S. Bari, S. Shen, Z.-X. Yong, H. Schoelkopf lessonslearned,”arXivpreprintarXiv:2209.07858,2022. 16", "metadata": {"id": "72e030b7c1b2bbae8c0291498cbc02f6ddea7dff", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 31, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "etal.,“Crosslingualgeneralizationthroughmultitaskfinetuning,”arXiv [158] S.Casper,J.Lin,J.Kwon,G.Culp,andD.Hadfield-Menell,“Explore,\npreprintarXiv:2211.01786,2022. 13,15,25 establish,exploit:Redteaminglanguagemodelsfromscratch,”arXiv\n[135] A.Glaese,N.McAleese,M.Tre˛bacz,J.Aslanides,V.Firoiu,T.Ewalds, preprintarXiv:2306.09442,2023. 16", "metadata": {"id": "b5219cea9197eabc9c26cecd66276add50fb7d17", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 31, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "M. Rauh, L. Weidinger, M. Chadwick, P. Thacker et al., “Improving [159] E.Perez,S.Huang,F.Song,T.Cai,R.Ring,J.Aslanides,A.Glaese,\nalignment of dialogue agents via targeted human judgements,” arXiv N. McAleese, and G. Irving, “Red teaming language models with\npreprintarXiv:2209.14375,2022. 15 languagemodels,”arXivpreprintarXiv:2202.03286,2022. 16", "metadata": {"id": "e18537e12f256f6f5986ded4d44352d66477908f", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 31, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "preprintarXiv:2209.14375,2022. 15 languagemodels,”arXivpreprintarXiv:2202.03286,2022. 16\n[136] Z.Luo,C.Xu,P.Zhao,Q.Sun,X.Geng,W.Hu,C.Tao,J.Ma,Q.Lin, [160] T. Scialom, T. Chakrabarty, and S. Muresan, “Fine-tuned language\nandD.Jiang,“Wizardcoder:Empoweringcodelargelanguagemodels modelsarecontinuallearners,”inProceedingsofthe2022Conference", "metadata": {"id": "e46aabe27a3140286f70f8b5763b583ccc84297a", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 31, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "withevol-instruct,”arXivpreprintarXiv:2306.08568,2023. 14,15 on Empirical Methods in Natural Language Processing, 2022, pp.\n[137] L.Ouyang,J.Wu,X.Jiang,D.Almeida,C.Wainwright,P.Mishkin, 6107–6122. 16\nC. Zhang, S. Agarwal, K. Slama, A. Ray et al., “Training language [161] Z. Shi and A. Lipani, “Don’t stop pretraining? make prompt-based", "metadata": {"id": "b5e0bd8258a60dc16d7ee258d47a2941ff335ca3", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 31, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "models to follow instructions with human feedback,” Advances in fine-tuningpowerfullearner,”arXivpreprintarXiv:2305.01711,2023.\nNeural Information Processing Systems, vol. 35, pp. 27730–27744, 16\n2022. 11,15,17,25 [162] H. Gupta, S. A. Sawant, S. Mishra, M. Nakamura, A. Mitra,", "metadata": {"id": "beae5b1832a100c1cb087a3eda8294a0880314f9", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 31, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "2022. 11,15,17,25 [162] H. Gupta, S. A. Sawant, S. Mishra, M. Nakamura, A. Mitra,\n[138] Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, S.Mashetty,andC.Baral,“Instructiontunedmodelsarequicklearn-\nand H. Hajishirzi, “Self-instruct: Aligning language model with self ers,”arXivpreprintarXiv:2306.05539,2023. 16", "metadata": {"id": "6f7eba17dbe33c3f5f2d7eee7b4f4d5949f33d51", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 31, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "generated instructions,” arXiv preprint arXiv:2212.10560, 2022. 13, [163] H.Chen,Y.Zhang,Q.Zhang,H.Yang,X.Hu,X.Ma,Y.Yanggong,\n17 and J. Zhao, “Maybe only 0.5% data is needed: A preliminary\n[139] D.Yin,X.Liu,F.Yin,M.Zhong,H.Bansal,J.Han,andK.-W.Chang, exploration of low training data instruction tuning,” arXiv preprint", "metadata": {"id": "ee92c42b8c764e897630f9e630434c5f67138f03", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 31, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "“Dynosaur: A dynamic growth paradigm for instruction-tuning data arXiv:2305.09246,2023. 16\ncuration,”arXivpreprintarXiv:2305.14327,2023. 14 [164] C. Zhou, P. Liu, P. Xu, S. Iyer, J. Sun, Y. Mao, X. Ma, A. Efrat,\n[140] P.Gao,J.Han,R.Zhang,Z.Lin,S.Geng,A.Zhou,W.Zhang,P.Lu, P.Yu,L.Yuetal.,“Lima:Lessismoreforalignment,”arXivpreprint", "metadata": {"id": "3efa80d2f385e9ca276aa00f09f4b3b8738215eb", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 31, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "C. He, X. Yue et al., “Llama-adapter v2: Parameter-efficient visual arXiv:2305.11206,2023. 16\ninstructionmodel,”arXivpreprintarXiv:2304.15010,2023. 14,26 [165] B. Zhang and H. Soh, “Large language models as zero-shot human\n[141] “Openai.gpt-4technicalreport,”2023. 14 modelsforhuman-robotinteraction,”arXivpreprintarXiv:2303.03548,", "metadata": {"id": "937a0013345e23d0661aaec26222921c14a375a5", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 31, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "[142] T.LiuandB.K.H.Low,“Goat:Fine-tunedllamaoutperformsgpt-4 2023. 16\nonarithmetictasks,”arXivpreprintarXiv:2305.14201,2023. 14 [166] A.LykovandD.Tsetserukou,“Llm-brain:Ai-drivenfastgenerationof\n[143] H.Wang,C.Liu,N.Xi,Z.Qiang,S.Zhao,B.Qin,andT.Liu,“Huatuo: robot behaviour tree based on large language model,” arXiv preprint", "metadata": {"id": "28730b391b00cd3afbf0834e3eaf3527d5323794", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 31, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "Tuningllamamodelwithchinesemedicalknowledge,”arXivpreprint arXiv:2305.19352,2023. 16\narXiv:2304.06975,2023. 14 [167] E.Billing,J.Rosén,andM.Lamb,“Languagemodelsforhuman-robot\n[144] C. Xu, Q. Sun, K. Zheng, X. Geng, P. Zhao, J. Feng, C. Tao, and interaction,”inACM/IEEEInternationalConferenceonHuman-Robot", "metadata": {"id": "b957dd175ebfdc62b02b4abdba3f4cd8087974d1", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 31, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "D. Jiang, “Wizardlm: Empowering large language models to follow Interaction, March 13–16, 2023, Stockholm, Sweden. ACM Digital\ncomplexinstructions,”arXivpreprintarXiv:2304.12244,2023. 14 Library,2023,pp.905–906. 16\n[145] J.Menick,M.Trebacz,V.Mikulik,J.Aslanides,F.Song,M.Chadwick, [168] Y.Ye,H.You,andJ.Du,“Improvedtrustinhuman-robotcollaboration", "metadata": {"id": "f99ff4276adfa8ba51b0c86b97885471b2241e06", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 31, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "M. Glaese, S. Young, L. Campbell-Gillingham, G. Irving et al., withchatgpt,”IEEEAccess,2023. 16\n“Teaching language models to support answers with verified quotes,” [169] I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay,\narXivpreprintarXiv:2203.11147,2022. 15 D.Fox,J.Thomason,andA.Garg,“Progprompt:Generatingsituated", "metadata": {"id": "ed8206ea6c292c63233e221420ef023b4cc7c37f", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 31, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "arXivpreprintarXiv:2203.11147,2022. 15 D.Fox,J.Thomason,andA.Garg,“Progprompt:Generatingsituated\n[146] R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and robottaskplansusinglargelanguagemodels,”in2023IEEEInterna-\nC. Finn, “Direct preference optimization: Your language model is tionalConferenceonRoboticsandAutomation(ICRA). IEEE,2023,", "metadata": {"id": "209b54f5014c97b214dc06185b6e62c4d1d689bd", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 31, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "secretlyarewardmodel,”arXivpreprintarXiv:2305.18290,2023. 16 pp.11523–11530. 16\n[147] H.Dong,W.Xiong,D.Goyal,R.Pan,S.Diao,J.Zhang,K.Shum,and [170] Y. Zhen, S. Bi, L. Xing-tong, P. Wei-qin, S. Hai-peng, C. Zi-rui,\nT. Zhang, “Raft: Reward ranked finetuning for generative foundation and F. Yi-shu, “Robot task planning based on large language model", "metadata": {"id": "7db0c143efd8a5d7f7dec1ad9553908d246ea95d", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 31, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "modelalignment,”arXivpreprintarXiv:2304.06767,2023. 16 representingknowledgewithdirectedgraphstructures,”arXivpreprint\n[148] Z.Yuan,H.Yuan,C.Tan,W.Wang,S.Huang,andF.Huang,“Rrhf: arXiv:2306.05171,2023. 16\nRankresponsestoalignlanguagemodelswithhumanfeedbackwithout [171] W.Huang,P.Abbeel,D.Pathak,andI.Mordatch,“Languagemodels", "metadata": {"id": "2f99a4c65b064d1c729e577388b14397465f96b6", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 31, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "tears,”arXivpreprintarXiv:2304.05302,2023. 16 as zero-shot planners: Extracting actionable knowledge for embodied\n[149] F. Song, B. Yu, M. Li, H. Yu, F. Huang, Y. Li, and H. Wang, agents,” in International Conference on Machine Learning. PMLR,\n“Preferencerankingoptimizationforhumanalignment,”arXivpreprint 2022,pp.9118–9147. 16", "metadata": {"id": "1ea6ef3be2e37f057f865de577ca9620e079cc6f", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 31, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "“Preferencerankingoptimizationforhumanalignment,”arXivpreprint 2022,pp.9118–9147. 16\narXiv:2306.17492,2023. 16 [172] Y.Ding,X.Zhang,C.Paxton,andS.Zhang,“Taskandmotionplanning\n[150] H.Liu,C.Sferrazza,andP.Abbeel,“Languagesarerewards:Hindsight withlargelanguagemodelsforobjectrearrangement,”arXivpreprint\nfinetuning using human feedback,” arXiv preprint arXiv:2302.02676, arXiv:2303.06247,2023. 16", "metadata": {"id": "ecb063dd470249d85f75d043044ecca4b7ea7134", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 31, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "finetuning using human feedback,” arXiv preprint arXiv:2302.02676, arXiv:2303.06247,2023. 16\n2023. 16 [173] ——,“Leveragingcommonsenseknowledgefromlargelanguagemod-\n[151] Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, elsfortaskandmotionplanning,”inRSS2023WorkshoponLearning\nA.Chen,A.Goldie,A.Mirhoseini,C.McKinnonetal.,“Constitutional forTaskandMotionPlanning,2023. 16", "metadata": {"id": "4cf64fbb902fae66d284759b5695ff4021aa4e6a", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 31, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "A.Chen,A.Goldie,A.Mirhoseini,C.McKinnonetal.,“Constitutional forTaskandMotionPlanning,2023. 16\nai:Harmlessnessfromaifeedback,”arXivpreprintarXiv:2212.08073, [174] Y.Ge,W.Hua,J.Ji,J.Tan,S.Xu,andY.Zhang,“Openagi:Whenllm\n2022. 16 meetsdomainexperts,”arXivpreprintarXiv:2304.04370,2023. 16", "metadata": {"id": "e462f3de9dbb09e7cdfc9fe25dff3f29b52689d5", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 31, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "2022. 16 meetsdomainexperts,”arXivpreprintarXiv:2304.04370,2023. 16\n[152] Y.Dubois,X.Li,R.Taori,T.Zhang,I.Gulrajani,J.Ba,C.Guestrin, [175] T. Zhong, Y. Wei, L. Yang, Z. Wu, Z. Liu, X. Wei, W. Li, J. Yao,\nP. Liang, and T. B. Hashimoto, “Alpacafarm: A simulation frame- C.Ma,X.Lietal.,“Chatabl:Abductivelearningvianaturallanguage", "metadata": {"id": "7e2a1c1bd9abb7566264aeb7ed45295e2184ab8a", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 31, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "work for methods that learn from human feedback,” arXiv preprint interactionwithchatgpt,”arXivpreprintarXiv:2304.11107,2023. 16\narXiv:2305.14387,2023. 16 [176] J. Wu, R. Antonova, A. Kan, M. Lepert, A. Zeng, S. Song, J. Bohg,\n[153] Z. Sun, Y. Shen, Q. Zhou, H. Zhang, Z. Chen, D. Cox, Y. Yang, S.Rusinkiewicz,andT.Funkhouser,“Tidybot:Personalizedrobotas-", "metadata": {"id": "35e83020c973f2339670ea7e5458af9682d0f2dd", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 31, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "and C. Gan, “Principle-driven self-alignment of language mod- sistancewithlargelanguagemodels,”arXivpreprintarXiv:2305.05658,\nels from scratch with minimal human supervision,” arXiv preprint 2023. 16\narXiv:2305.03047,2023. 16 [177] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter,", "metadata": {"id": "c96d7a3061c483ba4f95d784271286d6682ece2d", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 31, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "[154] C. Si, Z. Gan, Z. Yang, S. Wang, J. Wang, J. Boyd-Graber, A.Wahid,J.Tompson,Q.Vuong,T.Yuetal.,“Palm-e:Anembodied\nand L. Wang, “Prompting gpt-3 to be reliable,” arXiv preprint multimodallanguagemodel,”arXivpreprintarXiv:2303.03378,2023.\narXiv:2210.09150,2022. 16 16,17", "metadata": {"id": "6469d2d2f5bb5517aa93c8d678ef28c64a9f145b", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 31, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "JOURNALOFLATEX 31\n[178] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, [199] G. Luo, Y. Zhou, T. Ren, S. Chen, X. Sun, and R. Ji, “Cheap and\nJ. Tompson, I. Mordatch, Y. Chebotar, P. Sermanet, T. Jackson, quick: Efficient vision-language instruction tuning for large language", "metadata": {"id": "7d530f5f2fb4c1d7d8267884aacc0454d1870461", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 32, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "N. Brown, L. Luu, S. Levine, K. Hausman, and brian ichter, “Inner models,”arXivpreprintarXiv:2305.15023,2023. 17\nmonologue: Embodied reasoning through planning with language [200] R.Zhang,J.Han,A.Zhou,X.Hu,S.Yan,P.Lu,H.Li,P.Gao,and\nmodels,” in 6th Annual Conference on Robot Learning, 2022. Y.Qiao,“Llama-adapter:Efficientfine-tuningoflanguagemodelswith", "metadata": {"id": "98889ec3d92b47995bc9389cd141c310b29f8e4e", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 32, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "[Online].Available:https://openreview.net/forum?id=3R3Pz5i0tye 16 zero-initattention,”arXivpreprintarXiv:2303.16199,2023. 17\n[179] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, [201] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and\nK. Lenc, A. Mensch, K. Millican, M. Reynolds et al., “Flamingo: I. Sutskever, “Robust speech recognition via large-scale weak super-", "metadata": {"id": "b08b903752489d533fa8b0f1e7723758cb53aee0", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 32, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "a visual language model for few-shot learning,” Advances in Neural vision,” in International Conference on Machine Learning. PMLR,\nInformationProcessingSystems,vol.35,pp.23716–23736,2022. 17 2023,pp.28492–28518. 17\n[180] J.Li,D.Li,S.Savarese,andS.Hoi,“Blip-2:Bootstrappinglanguage- [202] Z. Zhang, A. Zhang, M. Li, H. Zhao, G. Karypis, and A. Smola,", "metadata": {"id": "d9984d97c61a5b70ad2a47a391ba099bfb81e92d", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 32, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "image pre-training with frozen image encoders and large language “Multimodal chain-of-thought reasoning in language models,” arXiv\nmodels,”arXivpreprintarXiv:2301.12597,2023. 17 preprintarXiv:2302.00923,2023. 17\n[181] H.Liu,C.Li,Q.Wu,andY.J.Lee,“Visualinstructiontuning,”arXiv [203] J. Ge, H. Luo, S. Qian, Y. Gan, J. Fu, and S. Zhan, “Chain of", "metadata": {"id": "ddd24e3e4bdad30be09078889207f0ceb364a85a", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 32, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "preprintarXiv:2304.08485,2023. 17 thought prompt tuning in vision language models,” arXiv preprint\n[182] K.Li,Y.He,Y.Wang,Y.Li,W.Wang,P.Luo,Y.Wang,L.Wang,and arXiv:2304.07919,2023. 17\nY.Qiao,“Videochat:Chat-centricvideounderstanding,”arXivpreprint [204] C.Wu,S.Yin,W.Qi,X.Wang,Z.Tang,andN.Duan,“Visualchatgpt:", "metadata": {"id": "dfeeef586d7fc352207fa0e8f9693a8c0d038d74", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 32, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "arXiv:2305.06355,2023. 17 Talking, drawing and editing with visual foundation models,” arXiv\n[183] M. Maaz, H. Rasheed, S. Khan, and F. S. Khan, “Video-chatgpt: preprintarXiv:2303.04671,2023. 17\nTowards detailed video understanding via large vision and language [205] Z. Yang, L. Li, J. Wang, K. Lin, E. Azarnasab, F. Ahmed, Z. Liu,", "metadata": {"id": "c0ad62085fd170ad6852f418286152aa90c9926e", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 32, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "models,”arXivpreprintarXiv:2306.05424,2023. 17 C. Liu, M. Zeng, and L. Wang, “Mm-react: Prompting chatgpt for\n[184] H. Zhang, X. Li, and L. Bing, “Video-llama: An instruction-tuned multimodal reasoning and action,” arXiv preprint arXiv:2303.11381,\naudio-visuallanguagemodelforvideounderstanding,”arXivpreprint 2023. 17", "metadata": {"id": "14ba7a449e6f74bdc6638098b89f580b9c4bc55a", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 32, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "audio-visuallanguagemodelforvideounderstanding,”arXivpreprint 2023. 17\narXiv:2306.02858,2023. 17 [206] T. Wang, J. Zhang, J. Fei, Y. Ge, H. Zheng, Y. Tang, Z. Li,\n[185] X.Mei,C.Meng,H.Liu,Q.Kong,T.Ko,C.Zhao,M.D.Plumbley, M. Gao, S. Zhao, Y. Shan et al., “Caption anything: Interactive", "metadata": {"id": "1b72418229b820c851deea9be2002fdc9c8da56b", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 32, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "Y. Zou, and W. Wang, “Wavcaps: A chatgpt-assisted weakly-labelled image description with diverse multimodal controls,” arXiv preprint\naudio captioning dataset for audio-language multimodal research,” arXiv:2305.02677,2023. 17\narXivpreprintarXiv:2303.17395,2023. 17 [207] X. Zhu, R. Zhang, B. He, Z. Zeng, S. Zhang, and P. Gao, “Pointclip", "metadata": {"id": "efbab4953c787a4af002cd822879c59142019c88", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 32, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "[186] C. Lyu, M. Wu, L. Wang, X. Huang, B. Liu, Z. Du, S. Shi, and v2:Adaptingclipforpowerful3dopen-worldlearning,”arXivpreprint\nZ. Tu, “Macaw-llm: Multi-modal language modeling with image, arXiv:2211.11682,2022. 17\naudio, video, and text integration,” arXiv preprint arXiv:2306.09093, [208] P.Lu, B. Peng,H. Cheng,M. Galley,K.-W. Chang,Y. N.Wu, S.-C.", "metadata": {"id": "254db1eb3023a5534a8166330030f06c397143a5", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 32, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "2023. 17 Zhu,andJ.Gao,“Chameleon:Plug-and-playcompositionalreasoning\n[187] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny, “Minigpt-4: En- withlargelanguagemodels,”arXivpreprintarXiv:2304.09842,2023.\nhancing vision-language understanding with advanced large language 17\nmodels,”arXivpreprintarXiv:2304.10592,2023. 17 [209] T. Gupta and A. Kembhavi, “Visual programming: Compositional", "metadata": {"id": "c4d833d8b1bc11de5bc0f0f5195588578aaec953", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 32, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "[188] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, visual reasoning without training,” in Proceedings of the IEEE/CVF\nT.Unterthiner,M.Dehghani,M.Minderer,G.Heigold,S.Gellyetal., Conference on Computer Vision and Pattern Recognition, 2023, pp.\n“Animageisworth16x16words:Transformersforimagerecognition 14953–14962. 17", "metadata": {"id": "67698056343c89a29d74aa634a7b5a89ab88cf61", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 32, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "“Animageisworth16x16words:Transformersforimagerecognition 14953–14962. 17\natscale,”arXivpreprintarXiv:2010.11929,2020. 17 [210] P. Gao, Z. Jiang, H. You, P. Lu, S. C. Hoi, X. Wang, and H. Li,\n[189] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, “Dynamicfusionwithintra-andinter-modalityattentionflowforvisual", "metadata": {"id": "49a24dd028d5b678323fbd1dd915464db5f928bf", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 32, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez, I. Stoica, and question answering,” in Proceedings of the IEEE/CVF conference on\nE. P. Xing, “Vicuna: An open-source chatbot impressing gpt-4 computervisionandpatternrecognition,2019,pp.6639–6648. 17\nwith 90%* chatgpt quality,” March 2023. [Online]. Available: [211] Z. Yu, J. Yu, Y. Cui, D. Tao, and Q. Tian, “Deep modular co-", "metadata": {"id": "2bb292d2301310c3f69925ee435b41a9264442d1", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 32, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "https://lmsys.org/blog/2023-03-30-vicuna/ 17 attention networks for visual question answering,” in Proceedings of\n[190] Q.Ye,H.Xu,G.Xu,J.Ye,M.Yan,Y.Zhou,J.Wang,A.Hu,P.Shi, theIEEE/CVFconferenceoncomputervisionandpatternrecognition,\nY. Shi et al., “mplug-owl: Modularization empowers large language 2019,pp.6281–6290. 17", "metadata": {"id": "6b6fdc78040e4ea48c1fc121c7ffd1cf236a5312", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 32, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "Y. Shi et al., “mplug-owl: Modularization empowers large language 2019,pp.6281–6290. 17\nmodels with multimodality,” arXiv preprint arXiv:2304.14178, 2023. [212] E.J.Hu,Y.Shen,P.Wallis,Z.Allen-Zhu,Y.Li,S.Wang,L.Wang,\n17 andW.Chen,“Lora:Low-rankadaptationoflargelanguagemodels,”\n[191] W.Dai,J.Li,D.Li,A.M.H.Tiong,J.Zhao,W.Wang,B.Li,P.Fung, arXivpreprintarXiv:2106.09685,2021. 17", "metadata": {"id": "0b4cf2552cb0ecb3d38ef1093f5325294cf18c70", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 32, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "[191] W.Dai,J.Li,D.Li,A.M.H.Tiong,J.Zhao,W.Wang,B.Li,P.Fung, arXivpreprintarXiv:2106.09685,2021. 17\nand S. Hoi, “Instructblip: Towards general-purpose vision-language [213] H. You, R. Sun, Z. Wang, L. Chen, G. Wang, H. A. Ayyubi, K.-\nmodels with instruction tuning,” arXiv preprint arXiv:2305.06500, W.Chang,andS.-F.Chang,“Idealgpt:Iterativelydecomposingvision", "metadata": {"id": "37a8db8aad4da6421a1d0b0d37447f6519073b41", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 32, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "2023. 17 and language reasoning via large language models,” arXiv preprint\n[192] W. Wang, Z. Chen, X. Chen, J. Wu, X. Zhu, G. Zeng, P. Luo, arXiv:2305.14985,2023. 17\nT. Lu, J. Zhou, Y. Qiao et al., “Visionllm: Large language model is [214] R.Zhang,X.Hu,B.Li,S.Huang,H.Deng,Y.Qiao,P.Gao,andH.Li,", "metadata": {"id": "3d48bc054b011d2a3574f957953d61abddf79cbf", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 32, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "also an open-ended decoder for vision-centric tasks,” arXiv preprint “Prompt, generate, then cache: Cascade of foundation models makes\narXiv:2305.11175,2023. 17 strongfew-shotlearners,”inProceedingsoftheIEEE/CVFConference\n[193] Z. Xu, Y. Shen, and L. Huang, “Multiinstruct: Improving multi- onComputerVisionandPatternRecognition,2023,pp.15211–15222.", "metadata": {"id": "b3f437fd2026c682d12bcb25ab5b2d063d0cf8b8", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 32, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "modal zero-shot learning via instruction tuning,” arXiv preprint 17\narXiv:2212.10773,2022. 17 [215] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi, “Hel-\n[194] S.Yin,C.Fu,S.Zhao,K.Li,X.Sun,T.Xu,andE.Chen,“Asurveyon laswag: Can a machine really finish your sentence?” arXiv preprint\nmultimodallargelanguagemodels,”arXivpreprintarXiv:2306.13549, arXiv:1905.07830,2019. 18,23,24,25", "metadata": {"id": "3188db3ce997705444baeef7ccc6689f600209f0", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 32, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "multimodallargelanguagemodels,”arXivpreprintarXiv:2306.13549, arXiv:1905.07830,2019. 18,23,24,25\n2023. 17 [216] Y. Bisk, R. Zellers, J. Gao, Y. Choi et al., “Piqa: Reasoning about\n[195] Z. Zhao, L. Guo, T. Yue, S. Chen, S. Shao, X. Zhu, Z. Yuan, and physical commonsense in natural language,” in Proceedings of the", "metadata": {"id": "52eb10e898a5207bb2395e97af01f7bad1e79014", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 32, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "J.Liu,“Chatbridge:Bridgingmodalitieswithlargelanguagemodelas AAAI conference on artificial intelligence, vol. 34, no. 05, 2020, pp.\nalanguagecatalyst,”arXivpreprintarXiv:2305.16103,2023. 17 7432–7439. 18,23,24\n[196] L.Li,Y.Yin,S.Li,L.Chen,P.Wang,S.Ren,M.Li,Y.Yang,J.Xu, [217] M.Joshi,E.Choi,D.S.Weld,andL.Zettlemoyer,“Triviaqa:Alarge", "metadata": {"id": "a44ffd0115bd9d86e3155f8991648f1086c8d0fe", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 32, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "X.Sunetal.,“M3it:Alarge-scaledatasettowardsmulti-modalmul- scale distantly supervised challenge dataset for reading comprehen-\ntilingual instruction tuning,” arXiv preprint arXiv:2306.04387, 2023. sion,”arXivpreprintarXiv:1705.03551,2017. 18,23,24,25\n17 [218] D. Paperno, G. Kruszewski, A. Lazaridou, Q. N. Pham, R. Bernardi,", "metadata": {"id": "f260688f775dc499d07197725113438b2b4bb12f", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 32, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "17 [218] D. Paperno, G. Kruszewski, A. Lazaridou, Q. N. Pham, R. Bernardi,\n[197] R.Yang,L.Song,Y.Li,S.Zhao,Y.Ge,X.Li,andY.Shan,“Gpt4tools: S. Pezzelle, M. Baroni, G. Boleda, and R. Fernández, “The lambada\nTeachinglargelanguagemodeltousetoolsviaself-instruction,”arXiv dataset: Word prediction requiring a broad discourse context,” arXiv", "metadata": {"id": "64e1dc2d1f01dd3fe953d9e34eca033ce97464bc", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 32, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "preprintarXiv:2305.18752,2023. 17 preprintarXiv:1606.06031,2016. 18,23,24\n[198] R.Pi,J.Gao,S.Diao,R.Pan,H.Dong,J.Zhang,L.Yao,J.Han,H.Xu, [219] K.Sakaguchi,R.L.Bras,C.Bhagavatula,andY.Choi,“Winogrande:\nand L. K. T. Zhang, “Detgpt: Detect what you need via reasoning,” Anadversarialwinogradschemachallengeatscale,”Communications", "metadata": {"id": "ea99c763433620d8a0c86338358d1e9be9be08a0", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 32, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "arXivpreprintarXiv:2305.14167,2023. 17 oftheACM,vol.64,no.9,pp.99–106,2021. 18,23,24,25", "metadata": {"id": "ac4640dee5ff9559c298e30c4443d4d9db26597f", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 32, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "JOURNALOFLATEX 32\n[220] H. Levesque, E. Davis, and L. Morgenstern, “The winograd schema [240] M. T. Pilehvar and J. Camacho-Collados, “Wic: 10,000 example\nchallenge,”inThirteenthinternationalconferenceontheprinciplesof pairs for evaluating context-sensitive representations,” arXiv preprint\nknowledgerepresentationandreasoning,2012. 18,22,23,24,25 arXiv:1808.09121,vol.6,2018. 21,23,24,25", "metadata": {"id": "62b027996769c4483c634fa146357475d0e09e18", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 33, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "knowledgerepresentationandreasoning,2012. 18,22,23,24,25 arXiv:1808.09121,vol.6,2018. 21,23,24,25\n[221] D.Hendrycks,C.Burns,S.Basart,A.Zou,M.Mazeika,D.Song,and [241] Y. Wang, X. Liu, and S. Shi, “Deep neural solver for math word\nJ.Steinhardt,“Measuringmassivemultitasklanguageunderstanding,” problems,”inProceedingsofthe2017conferenceonempiricalmethods", "metadata": {"id": "de0162b10f8377db57609dd22c6e89da1c702616", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 33, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "arXivpreprintarXiv:2009.03300,2020. 18,23,24,25 innaturallanguageprocessing,2017,pp.845–854. 21,23\n[222] A.Wang,A.Singh,J.Michael,F.Hill,O.Levy,andS.R.Bowman, [242] X. Liu, Q. Chen, C. Deng, H. Zeng, J. Chen, D. Li, and B. Tang,\n“Glue: A multi-task benchmark and analysis platform for natural “Lcqmc:Alarge-scalechinesequestionmatchingcorpus,”inProceed-", "metadata": {"id": "a6b0c4acb63bf1f2bf841df90186d071df58f840", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 33, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "languageunderstanding,”arXivpreprintarXiv:1804.07461,2018. 18, ingsofthe27thinternationalconferenceoncomputationallinguistics,\n22,23 2018,pp.1952–1962. 21,23\n[223] N. Mostafazadeh, N. Chambers, X. He, D. Parikh, D. Batra, L. Van- [243] D.Hendrycks,S.Basart,S.Kadavath,M.Mazeika,A.Arora,E.Guo,", "metadata": {"id": "12b671a37c0a1ec90f3a0ad3a46c8dcbc3ae2d3a", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 33, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "derwende,P.Kohli,andJ.Allen,“Acorpusandevaluationframework C. Burns, S. Puranik, H. He, D. Song et al., “Measuring coding\nfor deeper understanding of commonsense stories,” arXiv preprint challenge competence with apps,” arXiv preprint arXiv:2105.09938,\narXiv:1604.01696,2016. 18,23,24,25 2021. 21,23,24", "metadata": {"id": "1b028f464756cf965b20ca619f7d54adba287aad", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 33, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "arXiv:1604.01696,2016. 18,23,24,25 2021. 21,23,24\n[224] C. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins, and [244] I.Mollas,Z.Chrysopoulou,S.Karlos,andG.Tsoumakas,“Ethos:an\nK. Toutanova, “Boolq: Exploring the surprising difficulty of natural onlinehatespeechdetectiondataset,”arXivpreprintarXiv:2006.08328,\nyes/noquestions,”arXivpreprintarXiv:1905.10044,2019. 18,23,24 2020. 21,23,24", "metadata": {"id": "09c06c2058b2883f66f553f74a09130840ee3bc7", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 33, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "yes/noquestions,”arXivpreprintarXiv:1905.10044,2019. 18,23,24 2020. 21,23,24\n[225] G. Lai, Q. Xie, H. Liu, Y. Yang, and E. Hovy, “Race: Large-scale [245] M. Nadeem, A. Bethke, and S. Reddy, “Stereoset: Measuring\nreading comprehension dataset from examinations,” arXiv preprint stereotypical bias in pretrained language models,” arXiv preprint", "metadata": {"id": "702388bbf799640f9040f536270e36f1432c9728", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 33, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "arXiv:1704.04683,2017. 21,22,23,24 arXiv:2004.09456,2020. 21,23,24\n[246] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan,\n[226] S. Lin, J. Hilton, and O. Evans, “Truthfulqa: Measuring how models\nmimichumanfalsehoods,”arXivpreprintarXiv:2109.07958,2021.21,\nH.Edwards,Y.Burda,N.Joseph,G.Brockmanetal.,“Evaluatinglarge", "metadata": {"id": "f3e46b6cbbf96a9c3c28fb59f819493e1751b19f", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 33, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "H.Edwards,Y.Burda,N.Joseph,G.Brockmanetal.,“Evaluatinglarge\nlanguage models trained on code,” arXiv preprint arXiv:2107.03374,\n23,24,25\n2021. 21,23,24,25\n[227] Y. Nie, A. Williams, E. Dinan, M. Bansal, J. Weston, and D. Kiela,\n[247] Y.Chang,M.Narang,H.Suzuki,G.Cao,J.Gao,andY.Bisk,“We-\n“Adversarial nli: A new benchmark for natural language understand-", "metadata": {"id": "877f656ff56c568d0a43bd3d5c747e2df86fcaba", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 33, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "“Adversarial nli: A new benchmark for natural language understand-\nbqa: Multihop and multimodal qa,” in Proceedings of the IEEE/CVF\ning,”arXivpreprintarXiv:1910.14599,2019. 21,23,24,25\nConference on Computer Vision and Pattern Recognition, 2022, pp.\n[228] P.Clark,I.Cowhey,O.Etzioni,T.Khot,A.Sabharwal,C.Schoenick,\n16495–16504. 21,23", "metadata": {"id": "be8ac6fe1e3a7b2f93e2911aa777873d500566a7", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 33, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "[228] P.Clark,I.Cowhey,O.Etzioni,T.Khot,A.Sabharwal,C.Schoenick,\n16495–16504. 21,23\nand O. Tafjord, “Think you have solved question answering? try arc,\n[248] Y.Cui,T.Liu,W.Che,L.Xiao,Z.Chen,W.Ma,S.Wang,andG.Hu,\nthe ai2 reasoning challenge,” arXiv preprint arXiv:1803.05457, 2018.\n“A span-extraction dataset for chinese machine reading comprehen-\n21,23,24,25", "metadata": {"id": "27f8426670c82bc02dc1376e9488ded4a0c36c4e", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 33, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "“A span-extraction dataset for chinese machine reading comprehen-\n21,23,24,25\nsion,”arXivpreprintarXiv:1810.07366,2018. 22,23\n[229] A. Conneau, G. Lample, R. Rinott, A. Williams, S. R. Bowman,\n[249] S. Merity, C. Xiong, J. Bradbury, and R. Socher, “Pointer sentinel\nH.Schwenk,andV.Stoyanov,“Xnli:Evaluatingcross-lingualsentence\nmixturemodels,”arXivpreprintarXiv:1609.07843,2016. 22,23,24", "metadata": {"id": "9485baad8ae9613a77839b728f3c16fedd5cbf19", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 33, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "mixturemodels,”arXivpreprintarXiv:1609.07843,2016. 22,23,24\nrepresentations,”arXivpreprintarXiv:1809.05053,2018. 21,23,24\n[250] J. W. Rae, A. Potapenko, S. M. Jayakumar, and T. P. Lillicrap,\n[230] A. Williams, N. Nangia, and S. Bowman, “A broad-coverage\n“Compressivetransformersforlong-rangesequencemodelling,”arXiv\nchallenge corpus for sentence understanding through inference,” in", "metadata": {"id": "57d8015b16f05b34baf0704448b87a89f1d19a87", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 33, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "challenge corpus for sentence understanding through inference,” in\npreprintarXiv:1911.05507,2019. 22,23,24\nProceedings of the 2018 Conference of the North American Chapter\n[251] E.Choi,H.He,M.Iyyer,M.Yatskar,W.-t.Yih,Y.Choi,P.Liang,and\nof the Association for Computational Linguistics: Human Language\nL.Zettlemoyer,“Quac:Questionansweringincontext,”arXivpreprint", "metadata": {"id": "4fb929fdbad8f2ba8dd160fafac8d85924dbe1ab", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 33, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "L.Zettlemoyer,“Quac:Questionansweringincontext,”arXivpreprint\nTechnologies, Volume 1 (Long Papers). New Orleans, Louisiana:\narXiv:1808.07036,2018. 22,24\nAssociationforComputationalLinguistics,Jun.2018,pp.1112–1122.\n[252] E.M.Ponti,G.Glavaš,O.Majewska,Q.Liu,I.Vulic´,andA.Korho-\n[Online].Available:https://aclanthology.org/N18-1101 21", "metadata": {"id": "85b51ba59f8db8a1483732544c523df4f3af140d", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 33, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "[Online].Available:https://aclanthology.org/N18-1101 21\nnen, “Xcopa: A multilingual dataset for causal commonsense reason-\n[231] Y. Yang, Y. Zhang, C. Tar, and J. Baldridge, “Paws-x: A cross-\ning,”arXivpreprintarXiv:2005.00333,2020. 22,24\nlingualadversarialdatasetforparaphraseidentification,”arXivpreprint\n[253] M. Geva, D. Khashabi, E. Segal, T. Khot, D. Roth, and J. Berant,", "metadata": {"id": "baad4669e1fec0c7b0c086de6ec6c8c36d19c2f0", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 33, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "[253] M. Geva, D. Khashabi, E. Segal, T. Khot, D. Roth, and J. Berant,\narXiv:1908.11828,2019. 21,23,24\n“Did aristotle use a laptop? a question answering benchmark with\n[232] Y. Zhang, J. Baldridge, and L. He, “PAWS: Paraphrase adversaries\nimplicit reasoning strategies,” Transactions of the Association for\nfrom word scrambling,” in Proceedings of the 2019 Conference of", "metadata": {"id": "1d03b9f57642e1eebb7c2482f3df5a3433051137", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 33, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "from word scrambling,” in Proceedings of the 2019 Conference of\nComputationalLinguistics,vol.9,pp.346–361,2021. 22,24\nthe North American Chapter of the Association for Computational\n[254] A. Talmor, J. Herzig, N. Lourie, and J. Berant, “Commonsenseqa:\nLinguistics:HumanLanguageTechnologies,Volume1(LongandShort\nA question answering challenge targeting commonsense knowledge,”", "metadata": {"id": "98860db742a7a282436557c6b9b7e2bbe9686189", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 33, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "A question answering challenge targeting commonsense knowledge,”\nPapers). Minneapolis, Minnesota: Association for Computational\narXivpreprintarXiv:1811.00937,2018. 22,24\nLinguistics, Jun. 2019, pp. 1298–1308. [Online]. Available: https:\n[255] S. Iyer, N. Dandekar, and K. Csernai, “First quora\n//aclanthology.org/N19-1131 21\ndataset release: Question pairs,” https://quoradata.quora.com/", "metadata": {"id": "8843fb81aa1f069ebeef0c53493b79439ec69472", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 33, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "//aclanthology.org/N19-1131 21\ndataset release: Question pairs,” https://quoradata.quora.com/\n[233] S. Reddy, D. Chen, and C. D. Manning, “Coqa: A conversational\nFirst-Quora-Dataset-Release-Question-Pairs. 23\nquestion answering challenge,” Transactions of the Association for\n[256] A. Williams, N. Nangia, and S. R. Bowman, “A broad-coverage\nComputationalLinguistics,vol.7,pp.249–266,2019. 21,24", "metadata": {"id": "6e2f847dff6c6ef4e2a85397539991be2052d7f3", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 33, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "ComputationalLinguistics,vol.7,pp.249–266,2019. 21,24\nchallengecorpusforsentenceunderstandingthroughinference,”arXiv\n[234] D.Dua,Y.Wang,P.Dasigi,G.Stanovsky,S.Singh,andM.Gardner, preprintarXiv:1704.05426,2017. 23\n“Drop: A reading comprehension benchmark requiring discrete rea-\n[257] M.-C. De Marneffe, M. Simons, and J. Tonhauser, “The commit-", "metadata": {"id": "87061fe5910994b7aaedf1275ddfdd23f1cc8dc5", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 33, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "[257] M.-C. De Marneffe, M. Simons, and J. Tonhauser, “The commit-\nsoningoverparagraphs,”arXivpreprintarXiv:1903.00161,2019. 21,\nmentbank: Investigating projection in naturally occurring discourse,”\n24 inproceedingsofSinnundBedeutung,vol.23,no.2,2019,pp.107–\n[235] I.Dagan,O.Glickman,andB.Magnini,“Thepascalrecognisingtex- 124. 23,24,25", "metadata": {"id": "1dcbe3fe1a00c1aa37f93a3996fd5946e51ef098", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 33, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "[235] I.Dagan,O.Glickman,andB.Magnini,“Thepascalrecognisingtex- 124. 23,24,25\ntualentailmentchallenge,”inMachinelearningchallengesworkshop. [258] O. Bojar, R. Chatterjee, C. Federmann, Y. Graham, B. Haddow,\nSpringer,2005,pp.177–190. 21,23,24,25 M.Huck,A.J.Yepes,P.Koehn,V.Logacheva,C.Monzetal.,“Find-", "metadata": {"id": "decc812e1e6452f21ed472caefd8cb801444425f", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 33, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "Springer,2005,pp.177–190. 21,23,24,25 M.Huck,A.J.Yepes,P.Koehn,V.Logacheva,C.Monzetal.,“Find-\n[236] A.Srivastava,A.Rastogi,A.Rao,A.A.M.Shoeb,A.Abid,A.Fisch, ingsofthe2016conferenceonmachinetranslation,”inProceedingsof\nA.R.Brown,A.Santoro,A.Gupta,A.Garriga-Alonsoetal.,“Beyond theFirstConferenceonMachineTranslation:Volume2,SharedTask", "metadata": {"id": "3740c18ba60233b9c3999c1c929394c47855d330", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 33, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "the imitation game: Quantifying and extrapolating the capabilities of Papers,2016,pp.131–198. 23\nlanguagemodels,”arXivpreprintarXiv:2206.04615,2022. 21,23,24, [259] X.Pan,B.Zhang,J.May,J.Nothman,K.Knight,andH.Ji,“Cross-\n25 lingual name tagging and linking for 282 languages,” in Proceedings", "metadata": {"id": "1e5b524113c2cde738497304688610d81bd91d2d", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 33, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "25 lingual name tagging and linking for 282 languages,” in Proceedings\n[237] P. Rajpurkar, R. Jia, and P. Liang, “Know what you don’t know: of the 55th Annual Meeting of the Association for Computational\nUnanswerablequestionsforsquad,”arXivpreprintarXiv:1806.03822, Linguistics(Volume1:LongPapers),2017,pp.1946–1958. 23", "metadata": {"id": "fd48509086d8c22b7291c03b3ae77f0a3575a9bd", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 33, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "2018. 21,24 [260] P. Lewis, B. Og˘uz, R. Rinott, S. Riedel, and H. Schwenk, “Mlqa:\n[238] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, “Squad: 100,000+ Evaluatingcross-lingualextractivequestionanswering,”arXivpreprint\nquestions for machine comprehension of text,” arXiv preprint arXiv:1910.07475,2019. 23", "metadata": {"id": "6b77e77b81c14f38d0add2a089c378cd7faaa8e7", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 33, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "questions for machine comprehension of text,” arXiv preprint arXiv:1910.07475,2019. 23\narXiv:1606.05250,2016. 21,23 [261] J.H.Clark,E.Choi,M.Collins,D.Garrette,T.Kwiatkowski,V.Niko-\n[239] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, laev,andJ.Palomaki,“Tydiqa:Abenchmarkforinformation-seeking", "metadata": {"id": "4b2c78337e4e4c43d695e45f5d59f957454dc011", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 33, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "M.Plappert,J.Tworek,J.Hilton,R.Nakanoetal.,“Trainingverifiers question answering in typologically diverse languages,” Transactions\ntosolvemathwordproblems,”arXivpreprintarXiv:2110.14168,2021. oftheAssociationforComputationalLinguistics,vol.8,pp.454–470,\n21,24 2020. 23,24,25", "metadata": {"id": "b4ed8c815b105352b8fefab653ad34423f5534ae", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 33, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "JOURNALOFLATEX 33\n[262] W.Li,F.Qi,M.Sun,X.Yi,andJ.Zhang,“Ccpm:Achineseclassical [285] C.Xiao,H.Zhong,Z.Guo,C.Tu,Z.Liu,M.Sun,Y.Feng,X.Han,\npoetrymatchingdataset,”arXivpreprintarXiv:2106.01979,2021. 23 Z. Hu, H. Wang et al., “Cail2018: A large-scale legal dataset for\n[263] K.Sun,D.Yu,D.Yu,andC.Cardie,“Investigatingpriorknowledge judgmentprediction,”arXivpreprintarXiv:1807.02478,2018. 23", "metadata": {"id": "d40a951197d03e06734a1947cd17734257374aa5", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 34, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "forchallengingchinesemachinereadingcomprehension,”Transactions [286] C.Xu,W.Zhou,T.Ge,K.Xu,J.McAuley,andF.Wei,“Blowthedog\noftheAssociationforComputationalLinguistics,vol.8,pp.141–155, whistle:Achinesedatasetforcantunderstandingwithcommonsense\n2020. 23 andworldknowledge,”arXivpreprintarXiv:2104.02704,2021. 23", "metadata": {"id": "5f96120bf701ac9e0c77a2ce9e5067bedc1b7f53", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 34, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "2020. 23 andworldknowledge,”arXivpreprintarXiv:2104.02704,2021. 23\n[264] B.Loïc,B.Magdalena,B.Ondˇrej,F.Christian,G.Yvette,G.Roman, [287] C. Xiong, Z. Dai, J. Callan, Z. Liu, and R. Power, “End-to-end\nH. Barry, H. Matthias, J. Eric, K. Tom et al., “Findings of the neuralad-hocrankingwithkernelpooling,”inProceedingsofthe40th", "metadata": {"id": "15f3896bead3bf101408d68d33bc7542b6ab26a5", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 34, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "2020 conference on machine translation (wmt20),” in Proceedings InternationalACMSIGIRconferenceonresearchanddevelopmentin\nof the Fifth Conference on Machine Translation. Association for informationretrieval,2017,pp.55–64. 23\nComputationalLinguistics„2020,pp.1–55. 23 [288] C.Xu,J.Pei,H.Wu,Y.Liu,andC.Li,“Matinf:Ajointlylabeledlarge-", "metadata": {"id": "993ed19879a5e6b285a4cd72988ea7d25495210f", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 34, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "[265] B. Hu, Q. Chen, and F. Zhu, “Lcsts: A large scale chinese short text scaledatasetforclassification,questionansweringandsummarization,”\nsummarizationdataset,”arXivpreprintarXiv:1506.05865,2015. 23 arXivpreprintarXiv:2004.12302,2020. 23\n[266] Z.Shao,M.Huang,J.Wen,W.Xu,andX.Zhu,“Longanddiversetext [289] H. Zhou, C. Zheng, K. Huang, M. Huang, and X. Zhu, “Kdconv: A", "metadata": {"id": "f3f9e01e2116ae48ec495ad8f50b9a13ec9b4dc7", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 34, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "generation with planning-based hierarchical variational model,” arXiv chinesemulti-domaindialoguedatasettowardsmulti-turnknowledge-\npreprintarXiv:1908.06605,2019. 23 drivenconversation,”arXivpreprintarXiv:2004.04100,2020. 23\n[267] Y.Yao,Q.Dong,J.Guan,B.Cao,Z.Zhang,C.Xiao,X.Wang,F.Qi, [290] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster,", "metadata": {"id": "3dbe05e1b0f29aa934203a1fdf8d4b6d3211e310", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 34, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "J. Bao, J. Nie et al., “Cuge: A chinese language understanding and J. Phang, H. He, A. Thite, N. Nabeshima et al., “The pile: An\ngeneration evaluation benchmark,” arXiv preprint arXiv:2112.13610, 800gb dataset of diverse text for language modeling,” arXiv preprint\n2021. 23 arXiv:2101.00027,2020. 23,24", "metadata": {"id": "4e1593edfec47fb54b7de8369543472930f38b61", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 34, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "2021. 23 arXiv:2101.00027,2020. 23,24\n[268] Y. Li, T. Liu, D. Li, Q. Li, J. Shi, and Y. Wang, “Character-based [291] S. Lim, M. Kim, and J. Lee, “Korquad1. 0: Korean qa dataset for\nbilstm-crfincorporatingposanddictionariesforchineseopiniontarget machine reading comprehension,” arXiv preprint arXiv:1909.07005,\nextraction,”inAsianConferenceonMachineLearning. PMLR,2018, 2019. 23", "metadata": {"id": "2bfb330b4f15737c0dea3231c6d396040cb13ebd", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 34, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "extraction,”inAsianConferenceonMachineLearning. PMLR,2018, 2019. 23\npp.518–533. 23 [292] S. Park, J. Moon, S. Kim, W. I. Cho, J. Han, J. Park, C. Song,\n[269] L.Xu,H.Hu,X.Zhang,L.Li,C.Cao,Y.Li,Y.Xu,K.Sun,D.Yu, J.Kim,Y.Song,T.Ohetal.,“Klue:Koreanlanguageunderstanding\nC. Yu et al., “Clue: A chinese language understanding evaluation evaluation,”arXivpreprintarXiv:2105.09680,2021. 23", "metadata": {"id": "6c9fc1700466efa67baabdfdb233f8527a197811", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 34, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "benchmark,”arXivpreprintarXiv:2004.05986,2020. 23,24 [293] L. Xu, X. Lu, C. Yuan, X. Zhang, H. Xu, H. Yuan, G. Wei, X. Pan,\n[270] Z.Li,N.Ding,Z.Liu,H.Zheng,andY.Shen,“Chineserelationextrac- X.Tian,L.Qinetal.,“Fewclue:Achinesefew-shotlearningevaluation\ntionwithmulti-grainedinformationandexternallinguisticknowledge,” benchmark,”arXivpreprintarXiv:2107.07498,2021. 23", "metadata": {"id": "57b56512d4cbee1b8dea59bab9679a7d57b36188", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 34, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "in Proceedings of the 57th Annual Meeting of the Association for [294] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh,\nComputationalLinguistics,2019,pp.4377–4386. 23 C.Alberti,D.Epstein,I.Polosukhin,J.Devlin,K.Leeetal.,“Natural\n[271] J. Xu, J. Wen, X. Sun, and Q. Su, “A discourse-level named entity questions:abenchmarkforquestionansweringresearch,”Transactions", "metadata": {"id": "da5b09c06a951c624a384c4146476c3ca9e4b1a3", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 34, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "recognitionand relationextractiondatasetfor chineseliteraturetext,” oftheAssociationforComputationalLinguistics,vol.7,pp.453–466,\narXivpreprintarXiv:1711.07010,2017. 23 2019. 23,24\n[272] J.Chen,Q.Chen,X.Liu,H.Yang,D.Lu,andB.Tang,“Thebqcorpus: [295] J.Thorne,A.Vlachos,C.Christodoulopoulos,andA.Mittal,“Fever:a", "metadata": {"id": "7f776272f4e493dfb6a8bd2fe0a408d226752ceb", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 34, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "A large-scale domain-specific chinese corpus for sentence semantic large-scaledatasetforfactextractionandverification,”arXivpreprint\nequivalenceidentification,”inProceedingsofthe2018conferenceon arXiv:1803.05355,2018. 23\nempirical methods in natural language processing, 2018, pp. 4946– [296] I. Augenstein, C. Lioma, D. Wang, L. C. Lima, C. Hansen,", "metadata": {"id": "9000ad5e30bca172d8023f06618c041f47ef74a2", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 34, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "4951. 23 C.Hansen,andJ.G.Simonsen,“Multifc:Areal-worldmulti-domain\n[273] L.CO,“Iflytek:amultiplecategorieschinesetextclassifier.competi- dataset for evidence-based fact checking of claims,” arXiv preprint\ntionofficialwebsite,”2019. 23 arXiv:1909.03242,2019. 23\n[274] B.Liu,D.Niu,H.Wei,J.Lin,Y.He,K.Lai,andY.Xu,“Matching [297] M. Sap, H. Rashkin, D. Chen, R. LeBras, and Y. Choi, “Socialiqa:", "metadata": {"id": "52161be9dd462bb63b19eb1d2c042ddfb611ad7c", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 34, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "article pairs with graphical decomposition and convolutions,” arXiv Commonsense reasoning about social interactions,” arXiv preprint\npreprintarXiv:1802.07459,2018. 23 arXiv:1904.09728,2019. 23,24\n[275] S.Zhang,X.Zhang,H.Wang,J.Cheng,P.Li,andZ.Ding,“Chinese [298] S. Gehman, S. Gururangan, M. Sap, Y. Choi, and N. A. Smith,", "metadata": {"id": "91120bf63b4e4fbdb92f77a9aa601b46a456fccc", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 34, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "medical question answer matching using end-to-end character-level “Realtoxicityprompts:Evaluatingneuraltoxicdegenerationinlanguage\nmulti-scalecnns,”AppliedSciences,vol.7,no.8,p.767,2017. 23 models,”arXivpreprintarXiv:2009.11462,2020. 23,24\n[276] S. Zhang, X. Zhang, H. Wang, L. Guo, and S. Liu, “Multi-scale [299] S. L. Blodgett, L. Green, and B. O’Connor, “Demographic dialectal", "metadata": {"id": "37e5e62414ee14040f8d369c023d6bef9e009bed", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 34, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "attentive interaction networks for chinese medical question answer variation in social media: A case study of african-american english,”\nselection,”IEEEAccess,vol.6,pp.74061–74071,2018. 23 arXivpreprintarXiv:1608.08868,2016. 23\n[277] P. Li, W. Li, Z. He, X. Wang, Y. Cao, J. Zhou, and W. Xu, “Dataset [300] D. Borkan, L. Dixon, J. Sorensen, N. Thain, and L. Vasserman,", "metadata": {"id": "a6df759af2155f0d6a184e2c989d58426371fdb6", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 34, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "andneuralrecurrentsequencelabelingmodelforopen-domainfactoid “Nuanced metrics for measuring unintended bias with real data for\nquestionanswering,”arXivpreprintarXiv:1607.06275,2016. 23 textclassification,”inCompanionproceedingsofthe2019worldwide\n[278] N.PengandM.Dredze,“Namedentityrecognitionforchinesesocial webconference,2019,pp.491–500. 23", "metadata": {"id": "9518564eea89f215cec4de896c0fb945c826320e", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 34, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "[278] N.PengandM.Dredze,“Namedentityrecognitionforchinesesocial webconference,2019,pp.491–500. 23\nmedia with jointly trained embeddings,” in Proceedings of the 2015 [301] Y. Cui, T. Liu, Z. Chen, W. Ma, S. Wang, and G. Hu, “Dataset for\nconferenceonempiricalmethodsinnaturallanguageprocessing,2015, thefirstevaluationonchinesemachinereadingcomprehension,”arXiv", "metadata": {"id": "aae39089c248ef459fc880533ee2a8a89b5ddf80", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 34, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "pp.548–554. 23 preprintarXiv:1709.08299,2017. 23\n[279] R.Weischedel,S.Pradhan,L.Ramshaw,M.Palmer,N.Xue,M.Mar- [302] D. Vilares and C. Gómez-Rodríguez, “Head-qa: A healthcare dataset\ncus, A. Taylor, C. Greenberg, E. Hovy, R. Belvin et al., “Ontonotes forcomplexreasoning,”arXivpreprintarXiv:1906.04701,2019. 23", "metadata": {"id": "7cf6a16d87fe6e3f643cb7533baa84a20f52ff01", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 34, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "release4.0,”LDC2011T03,Philadelphia,Penn.:LinguisticDataCon- [303] J. Liu, L. Cui, H. Liu, D. Huang, Y. Wang, and Y. Zhang, “Logiqa:\nsortium,2011. 23 A challenge dataset for machine reading comprehension with logical\n[280] Y.Cui,T.Liu,Z.Yang,Z.Chen,W.Ma,W.Che,S.Wang,andG.Hu, reasoning,”arXivpreprintarXiv:2007.08124,2020. 23", "metadata": {"id": "f589a2ea127df6eaf9169b2188b0b2eda1678eba", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 34, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "“Asentenceclozedatasetforchinesemachinereadingcomprehension,” [304] T.Mihaylov,P.Clark,T.Khot,andA.Sabharwal,“Canasuitofarmor\narXivpreprintarXiv:2004.03116,2020. 23 conductelectricity?anewdatasetforopenbookquestionanswering,”\n[281] C. C. Shao, T. Liu, Y. Lai, Y. Tseng, and S. Tsai, “Drcd: A arXivpreprintarXiv:1809.02789,2018. 23,24", "metadata": {"id": "af3a64232b05f29caf170d23f3ea7c1df3d42c21", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 34, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "chinese machine reading comprehension dataset,” arXiv preprint [305] S.Aroca-Ouellette,C.Paik,A.Roncone,andK.Kann,“Prost:Phys-\narXiv:1806.00920,2018. 23 ical reasoning of objects through space and time,” arXiv preprint\n[282] W. He, K. Liu, J. Liu, Y. Lyu, S. Zhao, X. Xiao, Y. Liu, Y. Wang, arXiv:2106.03634,2021. 23", "metadata": {"id": "636cf2ac24fa198aadfd9b32eeb1010891f6811f", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 34, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "[282] W. He, K. Liu, J. Liu, Y. Lyu, S. Zhao, X. Xiao, Y. Liu, Y. Wang, arXiv:2106.03634,2021. 23\nH. Wu, Q. She et al., “Dureader: a chinese machine reading [306] A.Peñas,E.Hovy,P.Forner,Á.Rodrigo,R.Sutcliffe,andR.Morante,\ncomprehension dataset from real-world applications,” arXiv preprint “Qa4mre 2011-2013: Overview of question answering for machine", "metadata": {"id": "63f595b4f32d3ae71cafab61d95c5f519601584e", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 34, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "arXiv:1711.05073,2017. 23 readingevaluation,”inInformationAccessEvaluation.Multilinguality,\n[283] H.Tang,J.Liu,H.Li,Y.Hong,H.Wu,andH.Wang,“Dureaderrobust: Multimodality, and Visualization: 4th International Conference of the\nAchinesedatasettowardsevaluatingtherobustnessofmachinereading CLEFInitiative,CLEF2013,Valencia,Spain,September23-26,2013.", "metadata": {"id": "5d41f809a2fc9c50e558160e8dd0eb07a25ec367", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 34, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "comprehensionmodels,”arXivpreprintarXiv:2004.11142,2020. 23 Proceedings4. Springer,2013,pp.303–320. 23\n[284] C.Zheng,M.Huang,andA.Sun,“Chid:Alarge-scalechineseidiom [307] J.Welbl,N.F.Liu,andM.Gardner,“Crowdsourcingmultiplechoice\ndatasetforclozetest,”arXivpreprintarXiv:1906.01265,2019. 23 sciencequestions,”arXivpreprintarXiv:1707.06209,2017. 23", "metadata": {"id": "80df6f09c608ad0ea76a5a8076aaa51f30b5b0cb", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 34, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "JOURNALOFLATEX 34\n[308] Y.Liu,M.Ott,N.Goyal,J.Du,M.Joshi,D.Chen,O.Levy,M.Lewis, [330] W.Ling,D.Yogatama,C.Dyer,andP.Blunsom,“Programinduction\nL.Zettlemoyer,andV.Stoyanov,“Roberta:Arobustlyoptimizedbert byrationalegeneration:Learningtosolveandexplainalgebraicword\npretrainingapproach,”arXivpreprintarXiv:1907.11692,2019. 23 problems,”arXivpreprintarXiv:1705.04146,2017. 24", "metadata": {"id": "4f691832ea922a8412cb78a2bc2449825e062502", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 35, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "[309] J.Baumgartner,S.Zannettou,B.Keegan,M.Squire,andJ.Blackburn, [331] T. Scialom, P.-A. Dray, S. Lamprier, B. Piwowarski, and J. Sta-\n“Thepushshiftredditdataset,”inProceedingsoftheinternationalAAAI iano,“Mlsum:Themultilingualsummarizationcorpus,”arXivpreprint\nconferenceonwebandsocialmedia,vol.14,2020,pp.830–839. 23 arXiv:2004.14900,2020. 24", "metadata": {"id": "1470f736a107f5f9919dfdf3bef066579fa6e893", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 35, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "conferenceonwebandsocialmedia,vol.14,2020,pp.830–839. 23 arXiv:2004.14900,2020. 24\n[310] E. Dinan, S. Roller, K. Shuster, A. Fan, M. Auli, and J. Weston, [332] S.Narayan,S.B.Cohen,andM.Lapata,“Don’tgivemethedetails,\n“Wizard of wikipedia: Knowledge-powered conversational agents,” just the summary!” Topic-Aware Convolutional Neural Networks for", "metadata": {"id": "4cf10a7bcf4286a6dc656745862f9fbbbf08a4e2", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 35, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "arXivpreprintarXiv:1811.01241,2018. 23,24 ExtremeSummarization.ArXiv,abs,1808. 24\n[311] H. Rashkin, E. M. Smith, M. Li, and Y.-L. Boureau, “Towards [333] J.Novikova,O.Dušek,andV.Rieser,“Thee2edataset:Newchallenges\nempatheticopen-domainconversationmodels:Anewbenchmarkand forend-to-endgeneration,”arXivpreprintarXiv:1706.09254,2017.24", "metadata": {"id": "50407f75fff22dca783e5f4cc3ceea7250bb3dbd", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 35, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "dataset,”arXivpreprintarXiv:1811.00207,2018. 23 [334] T. C. Ferreira, C. Gardent, N. Ilinykh, C. Van Der Lee, S. Mille,\nD.Moussallem,andA.Shimorina,“The2020bilingual,bi-directional\n[312] E. Dinan, V. Logacheva, V. Malykh, A. Miller, K. Shuster, J. Ur-\nwebnlg+sharedtaskoverviewandevaluationresults(webnlg+2020),”\nbanek, D. Kiela, A. Szlam, I. Serban, R. Lowe et al., “The second", "metadata": {"id": "9f20fbdab930e85250acf1c47d974d6e6c2111c7", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 35, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "banek, D. Kiela, A. Szlam, I. Serban, R. Lowe et al., “The second\ninProceedingsofthe3rdInternationalWorkshoponNaturalLanguage\nconversational intelligence challenge (convai2),” in The NeurIPS’18\nGenerationfromtheSemanticWeb(WebNLG+),2020. 24\nCompetition: From Machine Learning to Intelligent Conversations.\n[335] N.Goyal,C.Gao,V.Chaudhary,P.-J.Chen,G.Wenzek,D.Ju,S.Kr-\nSpringer,2020,pp.187–208. 23", "metadata": {"id": "07b17774f69bfd70dfb436db50332d61c0722bcc", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 35, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "[335] N.Goyal,C.Gao,V.Chaudhary,P.-J.Chen,G.Wenzek,D.Ju,S.Kr-\nSpringer,2020,pp.187–208. 23\nishnan,M.Ranzato,F.Guzmán,andA.Fan,“Theflores-101evaluation\n[313] E.M.Smith,M.Williamson,K.Shuster,J.Weston,andY.-L.Boureau,\nbenchmark for low-resource and multilingual machine translation,”\n“Canyouputitalltogether:Evaluatingconversationalagents’ability", "metadata": {"id": "3dfaa47a9cc686b6afe83642a457d7f1cb0e75ff", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 35, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "“Canyouputitalltogether:Evaluatingconversationalagents’ability\nTransactionsoftheAssociationforComputationalLinguistics,vol.10,\ntoblendskills,”arXivpreprintarXiv:2004.08449,2020. 23\npp.522–538,2022. 24\n[314] M.Komeili,K.Shuster,andJ.Weston,“Internet-augmenteddialogue\n[336] Y.Xia,X.Tan,F.Tian,F.Gao,W.Chen,Y.Fan,L.Gong,Y.Leng,\ngeneration,”arXivpreprintarXiv:2107.07566,2021. 23", "metadata": {"id": "2db210d91b0ba1438c5b3fb09ea9401f3af36c5f", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 35, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "generation,”arXivpreprintarXiv:2107.07566,2021. 23\nR.Luo,Y.Wangetal.,“Microsoftresearchasia’ssystemsforwmt19,”\n[315] N. Nangia, C. Vania, R. Bhalerao, and S. R. Bowman, “Crows-pairs: arXivpreprintarXiv:1911.06191,2019. 24\nA challenge dataset for measuring social biases in masked language [337] A. Tikhonov and M. Ryabinin, “It’s all in the heads: Using attention", "metadata": {"id": "9154a4c3fac6af8589f385196956faf820f1d655", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 35, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "models,”arXivpreprintarXiv:2010.00133,2020. 23,24 headsasabaselineforcross-lingualtransferincommonsensereason-\n[316] H.Laurençon,L.Saulnier,T.Wang,C.Akiki,A.VillanovadelMoral, ing,”arXivpreprintarXiv:2106.12066,2021. 24\nT.LeScao,L.VonWerra,C.Mou,E.GonzálezPonferrada,H.Nguyen [338] S.RoyandD.Roth,“Solvinggeneralarithmeticwordproblems,”arXiv", "metadata": {"id": "8a6543de6739306f2e760278423311e6fcb38322", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 35, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "et al., “The bigscience roots corpus: A 1.6 tb composite multilingual preprintarXiv:1608.01413,2016. 24\ndataset,”AdvancesinNeuralInformationProcessingSystems,vol.35, [339] R.Rudinger,J.Naradowsky,B.Leonard,andB.VanDurme,“Gender\npp.31809–31826,2022. 24 biasincoreferenceresolution,”arXivpreprintarXiv:1804.09301,2018.\n[317] D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, 24", "metadata": {"id": "82a95fc7e234f4529b33d33e92998bafda299f8b", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 35, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "[317] D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, 24\nD.Song,andJ.Steinhardt,“Measuringmathematicalproblemsolving [340] J.Zhao,T.Wang,M.Yatskar,V.Ordonez,andK.-W.Chang,“Gender\nwiththemathdataset,”arXivpreprintarXiv:2103.03874,2021. 24 bias in coreference resolution: Evaluation and debiasing methods,”", "metadata": {"id": "82940819e861e9500f3df1d41f4dcd62eb4617d4", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 35, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "[318] M. Roemmele, C. A. Bejan, and A. S. Gordon, “Choice of plausible arXivpreprintarXiv:1804.06876,2018. 24\nalternatives:Anevaluationofcommonsensecausalreasoning.”inAAAI [341] A.Parrish,A.Chen,N.Nangia,V.Padmakumar,J.Phang,J.Thomp-\nspring symposium: logical formalizations of commonsense reasoning, son,P.M.Htut,andS.R.Bowman,“Bbq:Ahand-builtbiasbenchmark", "metadata": {"id": "dd4b676f038a78c622a6d073eaf2f191bdbb446f", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 35, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "2011,pp.90–95. 24,25 forquestionanswering,”arXivpreprintarXiv:2110.08193,2021. 24\n[319] D. Khashabi, S. Chaturvedi, M. Roth, S. Upadhyay, and D. Roth, [342] J. Boyd-Graber, B. Satinoff, H. He, and H. Daumé III, “Besting\n“Lookingbeyondthesurface:Achallengesetforreadingcomprehen- the quiz master: Crowdsourcing incremental classification games,”", "metadata": {"id": "24210d34dfcc620deb9c33562bd82b1531032c1e", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 35, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "sionovermultiplesentences,”inProceedingsofthe2018Conference in Proceedings of the 2012 joint conference on empirical methods\noftheNorthAmericanChapteroftheAssociationforComputational in natural language processing and computational natural language\nLinguistics:HumanLanguageTechnologies,Volume1(LongPapers), learning,2012,pp.1290–1301. 24", "metadata": {"id": "2a89828a8153aca338afd8590b9687d6fc3769c4", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 35, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "Linguistics:HumanLanguageTechnologies,Volume1(LongPapers), learning,2012,pp.1290–1301. 24\n2018,pp.252–262. 24 [343] F.Shi,M.Suzgun,M.Freitag,X.Wang,S.Srivats,S.Vosoughi,H.W.\n[320] S.Zhang,X.Liu,J.Liu,J.Gao,K.Duh,andB.VanDurme,“Record: Chung,Y.Tay,S.Ruder,D.Zhouetal.,“Languagemodelsaremulti-", "metadata": {"id": "74a5a61f9e1fe3ce61e3c52e64ce4ae2aefe85b6", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 35, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "Bridgingthegapbetweenhumanandmachinecommonsensereading lingualchain-of-thoughtreasoners,”arXivpreprintarXiv:2210.03057,\ncomprehension,”arXivpreprintarXiv:1810.12885,2018. 24 2022. 24,25\n[321] T. H. Trinh and Q. V. Le, “A simple method for commonsense [344] S.-Y.Miao,C.-C.Liang,andK.-Y.Su,“Adiversecorpusforevaluating", "metadata": {"id": "5d060ac80ae7fc092a5e107394ac94a5ef6ae82b", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 35, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "reasoning,”arXivpreprintarXiv:1806.02847,2018. 24 and developing english math word problem solvers,” arXiv preprint\n[322] R.Zellers,A.Holtzman,H.Rashkin,Y.Bisk,A.Farhadi,F.Roesner, arXiv:2106.15772,2021. 24\nandY.Choi,“Defendingagainstneuralfakenews,”Advancesinneural [345] J.Austin,A.Odena,M.Nye,M.Bosma,H.Michalewski,D.Dohan,", "metadata": {"id": "13de78473e6fc07bbfbadfdfd80bc50cabe45181", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 35, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "informationprocessingsystems,vol.32,2019. 24 E.Jiang,C.Cai,M.Terry,Q.Leetal.,“Programsynthesiswithlarge\nlanguagemodels,”arXivpreprintarXiv:2108.07732,2021. 24,25\n[323] R.T.McCoy,E.Pavlick,andT.Linzen,“Rightforthewrongreasons:\n[346] H. Husain, H. Wu, T. Gazit, M. Allamanis, and M. Brockschmidt,\nDiagnosing syntactic heuristics in natural language inference,” arXiv", "metadata": {"id": "173c52f0d0cc7d752a26331327424b2204f36c8a", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 35, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "Diagnosing syntactic heuristics in natural language inference,” arXiv\n“Codesearchnet challenge: Evaluating the state of semantic code\npreprintarXiv:1902.01007,2019. 24\nsearch,”CoRR,vol.abs/1909.09436,2019. 24\n[324] M.Mirzayanov,“Codeforces:Resultsof2020,”https://codeforces.com/\n[347] J.Austin,A.Odena,M.I.Nye,M.Bosma,H.Michalewski,D.Dohan,\nblog/entry/89502. 24", "metadata": {"id": "3cdc7e65ce729379278bb959ceb21f8234810706", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 35, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "[347] J.Austin,A.Odena,M.I.Nye,M.Bosma,H.Michalewski,D.Dohan,\nblog/entry/89502. 24\nE. Jiang, C. J. Cai, M. Terry, Q. V. Le, and C. Sutton, “Program\n[325] E.Caballero,.OpenAI,andI.Sutskever,“Description2CodeDataset,”\nsynthesis with large language models,” CoRR, vol. abs/2108.07732,\n8 2016. [Online]. Available: https://github.com/ethancaballero/\n2021. 24\ndescription2code 24", "metadata": {"id": "c15d4a87e1df123e420fdbf56b53a138c92b2887", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 35, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "8 2016. [Online]. Available: https://github.com/ethancaballero/\n2021. 24\ndescription2code 24\n[348] D. Kocetkov, R. Li, L. B. Allal, J. Li, C. Mou, C. M. Ferrandis,\n[326] R.Puri,D.S.Kung,G.Janssen,W.Zhang,G.Domeniconi,V.Zolotov, Y.Jernite,M.Mitchell,S.Hughes,T.Wolfetal.,“Thestack:3tbof", "metadata": {"id": "2b506efd6a77fe0ea61df14f6444027dc016cdd8", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 35, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "J.Dolby,J.Chen,M.Choudhury,L.Deckeretal.,“Codenet:Alarge- permissivelylicensedsourcecode,”arXivpreprintarXiv:2211.15533,\nscaleaiforcodedatasetforlearningadiversityofcodingtasks,”arXiv 2022. 24\npreprintarXiv:2105.12655,2021. 24 [349] Y. Lai, C. Li, Y. Wang, T. Zhang, R. Zhong, L. Zettlemoyer, W.-", "metadata": {"id": "cf7da8547cd2ff32348b383d6868618af980c8fa", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 35, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "[327] J. Berant, A. Chou, R. Frostig, and P. Liang, “Semantic parsing on t. Yih, D. Fried, S. Wang, and T. Yu, “Ds-1000: A natural and\nfreebase from question-answer pairs,” in Proceedings of the 2013 reliablebenchmarkfordatasciencecodegeneration,”inInternational\nconferenceonempiricalmethodsinnaturallanguageprocessing,2013, ConferenceonMachineLearning. PMLR,2023,pp.18319–18345.", "metadata": {"id": "f481c7daee2285092d0a42acae9d7bfdd13d683e", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 35, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "pp.1533–1544. 24 24,25\n[328] A.Patel,S.Bhattamishra,andN.Goyal,“Arenlpmodelsreallyableto [350] P.Liang,R.Bommasani,T.Lee,D.Tsipras,D.Soylu,M.Yasunaga,\nsolvesimplemathwordproblems?”arXivpreprintarXiv:2103.07191, Y.Zhang,D.Narayanan,Y.Wu,A.Kumaretal.,“Holisticevaluation\n2021. 24 oflanguagemodels,”arXivpreprintarXiv:2211.09110,2022. 24", "metadata": {"id": "cdbc971149f3e6a9526b66ed8fa7b66b72ecfeda", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 35, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "2021. 24 oflanguagemodels,”arXivpreprintarXiv:2211.09110,2022. 24\n[329] R. Koncel-Kedziorski, S. Roy, A. Amini, N. Kushman, and H. Ha- [351] A.Fan,Y.Jernite,E.Perez,D.Grangier,J.Weston,andM.Auli,“Eli5:\njishirzi,“Mawps:Amathwordproblemrepository,”inProceedingsof Long form question answering,” arXiv preprint arXiv:1907.09190,\nthe2016conferenceofthenorthamericanchapteroftheassociation 2019. 25", "metadata": {"id": "00fb461fd622fac6d2f74b4e508ffb1b0b981c3d", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 35, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "the2016conferenceofthenorthamericanchapteroftheassociation 2019. 25\nforcomputationallinguistics:humanlanguagetechnologies,2016,pp. [352] Y. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei,\n1152–1157. 24 A.Arunkumar,A.Ashok,A.S.Dhanasekaran,A.Naik,D.Stapetal.,", "metadata": {"id": "23936d7d33ef805d85f270396c2525a9ff1d65a5", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 35, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "JOURNALOFLATEX 35\n“Benchmarking generalization via in-context instructions on 1,600+\nlanguagetasks,”arXivpreprintarXiv:2204.07705,2022. 25\n[353] T. Xie, C. H. Wu, P. Shi, R. Zhong, T. Scholak, M. Yasunaga, C.-\nS. Wu, M. Zhong, P. Yin, S. I. Wang et al., “Unifiedskg: Unifying\nand multi-tasking structured knowledge grounding with text-to-text\nlanguagemodels,”arXivpreprintarXiv:2201.05966,2022. 25", "metadata": {"id": "2a100a7e9f97eb4b0508a3596317a35735089c3f", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 36, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "languagemodels,”arXivpreprintarXiv:2201.05966,2022. 25\n[354] Q.Ye,B.Y.Lin,andX.Ren,“Crossfit:Afew-shotlearningchallenge\nforcross-taskgeneralizationinnlp,”arXivpreprintarXiv:2104.08835,\n2021. 25\n[355] V. Aribandi, Y. Tay, T. Schuster, J. Rao, H. S. Zheng, S. V.\nMehta, H. Zhuang, V. Q. Tran, D. Bahri, J. Ni et al., “Ext5: To-\nwardsextrememulti-taskscalingfortransferlearning,”arXivpreprint", "metadata": {"id": "b4eb63e6fc7075a55d9ae34e6db64e6ed9c98c4f", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 36, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "wardsextrememulti-taskscalingfortransferlearning,”arXivpreprint\narXiv:2111.10952,2021. 25\n[356] N. Alex, E. Lifland, L. Tunstall, A. Thakur, P. Maham, C. J.\nRiedel, E. Hine, C. Ashurst, P. Sedille, A. Carlier et al., “Raft:\nA real-world few-shot text classification benchmark,” arXiv preprint\narXiv:2109.14076,2021. 25\n[357] P.Micikevicius,S.Narang,J.Alben,G.Diamos,E.Elsen,D.Garcia,", "metadata": {"id": "55f356d9885968295aad3c703e6e87ad16e479eb", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 36, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "arXiv:2109.14076,2021. 25\n[357] P.Micikevicius,S.Narang,J.Alben,G.Diamos,E.Elsen,D.Garcia,\nB. Ginsburg, M. Houston, O. Kuchaiev, G. Venkatesh et al., “Mixed\nprecisiontraining,”arXivpreprintarXiv:1710.03740,2017. 25\n[358] T.Q.NguyenandJ.Salazar,“Transformerswithouttears:Improving\nthenormalizationofself-attention,”CoRR,vol.abs/1910.05895,2019.\n25", "metadata": {"id": "0f1c4f32ef72d5efa47e733d6c903b7f912b9c2b", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 36, "created_at": "2025-09-07T19:16:04Z"}}
{"content": "thenormalizationofself-attention,”CoRR,vol.abs/1910.05895,2019.\n25\n[359] Y.Liu,M.Ott,N.Goyal,J.Du,M.Joshi,D.Chen,O.Levy,M.Lewis,\nL.Zettlemoyer,andV.Stoyanov,“Roberta:Arobustlyoptimizedbert\npretrainingapproach,”arXivpreprintarXiv:1907.11692,2019. 26\nView publication stats", "metadata": {"id": "6ede49447a114aa243bd144ad1a7c704eac8b8b5", "source": "AComprehensiveOverviewofLargeLanguageModels.pdf", "page": 36, "created_at": "2025-09-07T19:16:04Z"}}
