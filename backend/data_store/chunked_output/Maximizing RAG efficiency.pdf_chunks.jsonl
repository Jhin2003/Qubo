{"content": "NaturalLanguageProcessing(2025),31,pp.1–25\ndoi:10.1017/nlp.2024.53\nARTICLE\nMaximizing RAG efficiency: A comparative analysis of\nRAG methods\nTolgaS¸akar andHakanEmekci\nAppliedDataScience,TEDÜniversitesi,Ankara,Turkey\nCorrespondingauthor:TolgaS¸akar;Email:tolga.sakar@tedu.edu.tr\n(Received29May2024;revised17September2024;accepted17September2024;firstpublishedonline30October2024)\nAbstract\nThispaperaddressestheoptimizationofretrieval-augmentedgeneration(RAG)processesbyexploring\nvariousmethodologies,includingadvancedRAGmethods.Theresearch,drivenbytheneedtoenhance", "metadata": {"id": "1c0534a468cfa887f760b1c74d8e617eebbd69bd", "source": "Maximizing RAG efficiency.pdf", "page": 1, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "variousmethodologies,includingadvancedRAGmethods.Theresearch,drivenbytheneedtoenhance\nRAGprocessesashighlightedbyrecentstudies,involvedagrid-searchoptimizationof23,625iterations.\nWe evaluated multiple RAG methods across different vectorstores, embedding models, and large lan-\nguagemodels,usingcross-domaindatasetsandcontextualcompressionfilters.Thefindingsemphasizethe\nimportanceofbalancingcontextqualitywithsimilarity-basedrankingmethods,aswellasunderstanding\ntradeoffs between similarity scores, token usage, runtime, and hardware utilization. Additionally, con-", "metadata": {"id": "7bc353cc69f39e2ce80816669bde2d3398012b57", "source": "Maximizing RAG efficiency.pdf", "page": 1, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "textualcompressionfilterswerefoundtobecrucialforefficienthardwareutilizationandreducedtoken\nconsumption,despitetheevidentimpactsonsimilarityscores,whichmaybeacceptabledependingon\nspecificusecasesandRAGmethods.\nKeywords:LargeLanguageModels;VectorDatabases;Retrieval-AugmentedGeneration;ContextualCompression;\nEmbeddingModels\n1. Introduction\nThe use of goal-oriented large language models (LLMs) (Devlin et al. 2019; Brown et al. 2020;\nChowdheryetal.2022),coupledwithdiverseLLM-orientedframeworks,iscontinuallybroaden-\ningthespectrumofAIapplications,enhancingtheproficiencyofLLMsacrosscomplextasks(Wei", "metadata": {"id": "a618d5b27fddf0e2c3fb36a952d1cded2cdc37b2", "source": "Maximizing RAG efficiency.pdf", "page": 1, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "ingthespectrumofAIapplications,enhancingtheproficiencyofLLMsacrosscomplextasks(Wei\netal.2022).ContemporaryLLMsdemonstrateremarkablecapabilities,fromansweringquestions\naboutlegaldocumentswithlatentprovenance(Jeong2023;Nigametal.2023;Cuietal.2023)to\nchatbotsadeptatgeneratingprogramingcode(Vaithilingametal.2022).However,thisincreased\ncapability also introduces additional complexities. Emerging LLMs, formidable in conventional\ntext-basedtasks,necessitateexternalresourcestoadapttoevolvingknowledge.\nTo address this challenge, non-parametric retrieval-based methodologies, exemplified by", "metadata": {"id": "936b9cb20bea4edfd557ad8b05ce4b94defbf064", "source": "Maximizing RAG efficiency.pdf", "page": 1, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "To address this challenge, non-parametric retrieval-based methodologies, exemplified by\nretrieval-augmented generation (RAG) (Lewis et al. 2020), are becoming integral to the latest\nLLM applications, especially for domain-specific tasks (Vaithilingam et al. 2022; Manathunga\nand Illangasekara 2023; Nigam et al. 2023; Pesaru et al. 2023; Peng et al. 2023; Gupta 2023).\nThe evolution of AI-stack applications underscores the critical role of fine-tuning RAG meth-\nods in updating the knowledge base of LLMs (Choi et al. 2021; Lin et al. 2023; Konstantinos", "metadata": {"id": "be0d731852af3f258d0b84545dd61877068aada1", "source": "Maximizing RAG efficiency.pdf", "page": 1, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "ods in updating the knowledge base of LLMs (Choi et al. 2021; Lin et al. 2023; Konstantinos\nandPouwelseAndriopoulosandJohan2023).Retrieval-basedapplicationsdemandoptimization\nwhensearchingthemostrelevantpassages,ortop-Kvectors,throughsemanticsimilaritysearch.\nQuerying multi-document vectors and augmenting LLMs with relevant context introduce\ndependenciesonbothtimeandtokenlimits.The‘bi-encoder’retrievalmodels(refertoFigure1)\nleverage cutting-edge approximate nearest-neighbor search algorithms (Jégou et al. 2011).", "metadata": {"id": "22486e1f1c9cfb92732b2d90e5d8bf13898781f1", "source": "Maximizing RAG efficiency.pdf", "page": 1, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "leverage cutting-edge approximate nearest-neighbor search algorithms (Jégou et al. 2011).\n(cid:2)C The Author(s), 2024. Published by Cambridge University Press. This is an Open Access article, distributed under the terms of the\nCreativeCommonsAttributionlicence(https://creativecommons.org/licenses/by/4.0/),whichpermitsunrestrictedre-use,distributionand\nreproduction,providedtheoriginalarticleisproperlycited.\nhttps://doi.org/10.1017/nlp.2024.53 Published online by Cambridge University Press", "metadata": {"id": "487e1a666fa672c1036f848e9ec0006d7b98c162", "source": "Maximizing RAG efficiency.pdf", "page": 1, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "2 T.S¸akarandH.Emekci\nHowever,diversedatastructures(e.g.,multimedia,tables,graphs,charts,andunstructuredtext)\npose additional challenges, including the potential generation of hallucinative responses (Sean\netal.2019;KonstantinosandPouwelseAndriopoulosandJohan2023)andtheriskofsurpassing\nLLMtokenlimits(Roychowdhuryetal.2023).Overcomingthehallucinationproblemandstaying\nwithinthetokengenerationlimitisadifficulttask.Thetradeoffisthatoncetheretrievalprocessis\ncompleteandtop-Kdocumentsareobtainedbythesearchalgorithm,thetotalnumberoftokens\nsentmayexceedthetokenlimitforthespecificLLMinuse.", "metadata": {"id": "bdc1ca632ff4a022c1759a3757f312eb036464ab", "source": "Maximizing RAG efficiency.pdf", "page": 2, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "sentmayexceedthetokenlimitforthespecificLLMinuse.\nOnthe otherhand, constrainingtheretrieval capabilities maylimit the LLM’s ability to suc-\ncessfullygeneratetherelevantresponsebasedontherestrainedcontext.Optimizingthetradeoff\nrequiresanin-depthanalysisinvolvingvariousprocesses,includingexperimentingwithdiverse\nandhardware-optimizedsearchalgorithms(Chenetal.2019;ZhangandHe,2019;Malkovand\nYashunin2020;Johnsonetal.2021;Linetal.2023),applyingembeddingfiltersbasedonsimilarity\nscorethreshold, androutingtheLLMinputsandoutputsalongwiththefilteredcontext.These", "metadata": {"id": "bbe8e18f5a58a7b41be45328560cf9fa3a84d856", "source": "Maximizing RAG efficiency.pdf", "page": 2, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "scorethreshold, androutingtheLLMinputsandoutputsalongwiththefilteredcontext.These\nprocessesplayacrucialroleinfine-tuningLLMresponsesbyoptimizingtheretrievalprocess.In\nthe pursuit of optimization, commonly known vector databases, such as Pinecone (Sage 2023),\nChromaDB,FAISS,Weaviate,andQdrant,findapplicationforvariousreasons.Thesedatabases\nare classified based on criteria like scalability, ease of use (versatile Application Programming\nInterface [API] support), filtering options,security, efficiency, andspeed (Hanet al.2023).The", "metadata": {"id": "cd06c6a881bebd083fe33856cf7c8a4d8c3dbaef", "source": "Maximizing RAG efficiency.pdf", "page": 2, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "Interface [API] support), filtering options,security, efficiency, andspeed (Hanet al.2023).The\nintegration of these databases into the LLM operational pipeline not only enhances the overall\nefficiencyandeffectivenessofretrieval-basedmethodologiesbutalsocontributessignificantlyto\ntheoptimizationofLLMperformance.\nOptimization initiates with document preprocessing. Chunking the multi-document into\nsmaller paragraphs and overlapping the chunks (sentence tokens) could potentially affect the\nretrieval process since the embedded documents in the vector space are selected as a candidate", "metadata": {"id": "89c4a314d0ca22fc97172e28d05e8a40f2d07cef", "source": "Maximizing RAG efficiency.pdf", "page": 2, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "retrieval process since the embedded documents in the vector space are selected as a candidate\ncontext by the search algorithm (Schwaber-Cohen 2023; Zhou et al. 2023). Chunks and over-\nlapping chunk hyper-parameters control the granularity of text splitting, making fine-tuning\ncrucial, especially when dealing with documents of a similar or reasonably uniform format\n(Schwaber-Cohen2023).Inadditiontooptimizingdatapreprocessing,therearevariousknown\nRAGmethodologies,suchas‘Stuff’,‘Refine’,‘MapReduce’,‘MapRe-rank’,‘QueryStep-Down’,", "metadata": {"id": "50d49aaa25e1ac4a246d7c64c38d4d88cd9d6bc6", "source": "Maximizing RAG efficiency.pdf", "page": 2, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "RAGmethodologies,suchas‘Stuff’,‘Refine’,‘MapReduce’,‘MapRe-rank’,‘QueryStep-Down’,\nand‘ReciprocalRAG’(refertoFigures2.4.1–2.5.3),whichsignificantlyimpactvectorstorescala-\nbility,semanticretrievalspeed,andtokenbudgeting.WhenmakinganLLMcall,RAGmethods\neither feed the retrieved documents as the unfiltered context to the call or apply re-ranking\nwithin the retrieved context based on the highest individual similarity for each document vec-\ntor.Moreimportantly,inmulti-documenttasks,fine-tuningthecontextroutingintheLLMcall", "metadata": {"id": "9e49c3629d22bd822c31788b47ad5bb31552f9d4", "source": "Maximizing RAG efficiency.pdf", "page": 2, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "tor.Moreimportantly,inmulti-documenttasks,fine-tuningthecontextroutingintheLLMcall\nsequence can dramatically affect the response generation time, hardware resources, and token\nbudgeting(Nairetal.2023).DespitetherecognitionoftheimportanceofoptimizingRAGpro-\ncessesinnumerouspapers(Vaithilingametal.2022;Nairetal.2023;TopsakalandAkinci2023;\nManathunga and Illangasekara 2023; Nigam et al. 2023; Pesaru et al. 2023; Peng et al. 2023;\nKonstantinos and Pouwelse, Andriopoulos and Johan 2023; Roychowdhury et al. 2023), there", "metadata": {"id": "93c513e9c0a7bcabb9da003298fd3e1c15f6e27b", "source": "Maximizing RAG efficiency.pdf", "page": 2, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "Konstantinos and Pouwelse, Andriopoulos and Johan 2023; Roychowdhury et al. 2023), there\nremains a notable gap in previous work on the methods of optimization. Therefore, this paper\naimstoilluminatetheprocessofoptimizingRAGprocessestoimproveLLMresponses.\n2. Materialsandmethodology\n2.1 Retrieval-augmentedgeneration\nRAG method enables continuous updates and data freshness for question-answering chatbots\nby retrieving latent documents as provenance, without the need of re-training or fine-tuning\nthe LLMs for domain-specific tasks. First-generation RAG models tackled challenges (Mialon", "metadata": {"id": "fd2bf83f584b24b7aa501723ab9579f957462b78", "source": "Maximizing RAG efficiency.pdf", "page": 2, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "the LLMs for domain-specific tasks. First-generation RAG models tackled challenges (Mialon\net al. 2023) in knowledge-intensive text generation tasks by combining a parametric pretrained\nsequence-to-sequence BART model (refer to Figure 1) (Sutskever et al. 2014; Lewis et al. 2019)\nhttps://doi.org/10.1017/nlp.2024.53 Published online by Cambridge University Press", "metadata": {"id": "71bcda9019f52dfcf096a365f3b86795a5cb7a10", "source": "Maximizing RAG efficiency.pdf", "page": 2, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "NaturalLanguageProcessing 3\nFigure 1. First-generation retrieval-augmented generation (RAG) methodology: Non-parametric RAG with a parametric\nsequence-to-sequencemodel.(RefertoRAGforknowledge-intensivenaturallanguageprocessingtasks.\nwithanon-parametricmemory.Thismemoryisavectorizeddenserepresentationoflatentdoc-\numentsfromWikipedia,accessedthroughapretrainedneuralretriever(Lewisetal.2020).This\nintegrationofasequence-to-sequenceencoderandatop-Kdocumentindexenablestokengen-\nerationbasedonlatentprovenanceaugmentedwithcontextfromthequery.The‘Retrievaland", "metadata": {"id": "09f3ac2e625fd67291f45dbb9751e14ee9c606d7", "source": "Maximizing RAG efficiency.pdf", "page": 3, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "erationbasedonlatentprovenanceaugmentedwithcontextfromthequery.The‘Retrievaland\nGeneration’sequenceefficientlyproducesoutputbyleveragingboththepretrainedsequence-to-\nsequencemodelandthenon-parametricmemorywithaprobabilisticmodel(refertoFigure1).\nThe non-parametric retriever conditions latent provenance on the input query (x), and sub-\nsequently, the parametric sequence-to-sequence model (P ) is then conditioned on this new\no\ninformation along with the input (x) to generate the output response Pθ(y i |x,z,y i−1 ). The", "metadata": {"id": "1a1feed35564bfb3be03870a34adcd9816800f11", "source": "Maximizing RAG efficiency.pdf", "page": 3, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "o\ninformation along with the input (x) to generate the output response Pθ(y i |x,z,y i−1 ). The\ncombined probabilistic model then marginalizes to approximate the top-K documents (latent\ndocuments).Thiscanoccureitheronaper-outputbasis,meaningthatasinglelatentdocument\nis responsible for all output generation, or different indexed documents are responsible for dif-\nferentoutputgenerations.Subsequently,theparameterizedθ generatesthey tokenbasedonthe\ni\ntop-K contextfromaprevioustoken,y i−1 ,x,z,wherexistheoriginalinput,andzisaretriever\ndocument(refertoFigure1).", "metadata": {"id": "40f4e91c3133f6e74e1b97da300d6de1ee9695b6", "source": "Maximizing RAG efficiency.pdf", "page": 3, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "document(refertoFigure1).\nRecent advancements in retrieval methods involve prominent embedding models, which are\nextensively trained on a large corpus of data in diverse languages. Notably, a significant shift\nhas occurred, with a preference for LLMs over traditional models like sequence-to-sequence\nfor token generation in the context of RAG. This transition builds on the foundations laid by\nfirst-generation RAG models, expanding the possibilities for more efficient and versatile text\ngeneration using latent documents as provenance. The latest RAG methods (refer to Figures", "metadata": {"id": "646b3c6985bc9a2086b9d6bc507ca6477c30f835", "source": "Maximizing RAG efficiency.pdf", "page": 3, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "generation using latent documents as provenance. The latest RAG methods (refer to Figures\n2.4.1–2.5.3)usedinLLM-poweredchatbotsinvolvemuchmoresophisticatedretrievalprocesses.\nUsing frameworks such as LangChain and LlAma-index, LLMs can now have direct access to\nSQL,vectorstores,Googlesearchresults,andvariousAPIs.Thisincreasedcapacityingenerating\nmore accurate results by having access to concurrent information allows LLMs to expand their\nknowledgebasefordomain-specifictasks.Moreover,usingembeddingfiltersprovidedbyframe-", "metadata": {"id": "6b12455e807608f4e2e7ca987d4a4e4dcce94018", "source": "Maximizing RAG efficiency.pdf", "page": 3, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "knowledgebasefordomain-specifictasks.Moreover,usingembeddingfiltersprovidedbyframe-\nworks,RAGprocessescanbecomeevenmoreefficientintermsofretrievalspeedwhenquerying\nfromvectordatabases.Embeddingfiltersapplythresholdsduringthesimilaritysearchprocessto\nselecttextvectors(embeddings) thathavesimilarityscoresabovethethresholdwhileremoving\ntheredundantvectors,achievingefficiencyinbothretrievalspeedandtokenbudgeting.\n2.2 Vectorstores\nVectorstores, playing a crucial role in RAG-based LLM applications, are distinguished by their", "metadata": {"id": "f333f7017323cf189704d6894ed078b68a466ed2", "source": "Maximizing RAG efficiency.pdf", "page": 3, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "Vectorstores, playing a crucial role in RAG-based LLM applications, are distinguished by their\nproficiency in executing fast and precise similarity searches and retrievals. Unlike traditional\ndatabases reliant on exact matches, vector databases assess similarity based on vector distance\nandembeddingfilters,enablingthecaptureofsemanticandcontextualmeaning(Hanetal.2023)\nhttps://doi.org/10.1017/nlp.2024.53 Published online by Cambridge University Press", "metadata": {"id": "1625bf32a2c906c4191f59239c6317da78e83fc8", "source": "Maximizing RAG efficiency.pdf", "page": 3, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "4 T.S¸akarandH.Emekci\nFigure2. Schemaforsearchingandmatchingsimilarcontextfromvectordatabasebasedonuserquery.\n(refertoFigure2).Thispositionsthemaspivotalcomponentsintheongoingefforttooptimize\ntheintricateinterplaybetweenlanguagemodelsandretrievalmethodologies.\nProminentvectordatabases,suchasFAISS,Pinecone,andChromaDB,findapplicationsinvar-\niousdomains,eachofferingkeydifferentiators.FAISS,acknowledgedasaleadingvectordatabase,\nexcels in high-speed vector indexing for similarity searches. Its memory-efficient search algo-", "metadata": {"id": "fe713247abb252320d68fd60c23cf93b33626e97", "source": "Maximizing RAG efficiency.pdf", "page": 4, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "excels in high-speed vector indexing for similarity searches. Its memory-efficient search algo-\nrithmenablesthehandlingofhigh-dimensionaldatawithoutcompromisingspeedandefficiency\n(Johnson et al. 2019). Adding to the strengths of FAISS, Pinecone emerges as a notable choice,\nproviding a high-level API service and delivering comparable performanceto FAISS in similar-\nity searches (Sage 2023). However, as a managed service, Pinecone raises scalability concerns.\nLimitationsonthenumberofqueriesposeabarriertolarge-scaledataprocessingneeds,especially\nforhigh-volumeapplications.", "metadata": {"id": "eefe3314c0d2be80ac25149a1bb1f128c756988d", "source": "Maximizing RAG efficiency.pdf", "page": 4, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "forhigh-volumeapplications.\nUnlikePinecone,ChromaDBisanopen-sourcevectordatabasethatoffersmoreflexibilityin\ntermsofscalabilityandusage.Thisopen-sourcenaturefacilitatesadaptabilitytodifferentneeds\nandusecases,makingitacompellingoptionforcustomizationandcontrolovervectordatabase\ninfrastructure.Semanticsimilaritysearchistheretrievalofinformationordatabasedonsemantic\nrelationshipsthatgobeyondexactkeywordortextmatching.Itfocusesonunderstandingthecon-\ntextualandconceptualrelationshipsbetweenwords,phrases,ordocuments.Aspecificapplication", "metadata": {"id": "1c775c219b4f0192acac3b78ff7572dc2b2ed9bb", "source": "Maximizing RAG efficiency.pdf", "page": 4, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "textualandconceptualrelationshipsbetweenwords,phrases,ordocuments.Aspecificapplication\nwithinsemanticsimilaritysearchistheexplorationofsemanticconnectionsbetweenembeddings.\nVectorrepresentations(embeddings)ofwords,phrases,ordocumentsareusedforsimilarity-\nbasedsearch.Theprocessbeginsbyconvertingdocumentsortextintovectorsusinganembed-\ndingmodel.Then,similaritysearchalgorithmsidentifythevectorsmostsimilartothequerybased\non various metrics, such as cosine similarity. In the final step, the output contains the response", "metadata": {"id": "a97ca6092c3b5e55573142fb49d65aca8a93c3da", "source": "Maximizing RAG efficiency.pdf", "page": 4, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "on various metrics, such as cosine similarity. In the final step, the output contains the response\ncorresponding to the most similar vectors within the vector space. When calculating the sim-\nilarity, cosine similarity is especially effective in this context as it measures the cosine of the\nangle between two vectors, providing a normalized measure that captures the directional simi-\nlaritybetweenvectors.Thispropertymakesitparticularlyusefulforevaluatingthesimilarityof\nembeddingsinsemanticsearchtasks.\nCosinesimilarity", "metadata": {"id": "c08d2c1df02096bfd527e4b2b22d30b3fe6e8926", "source": "Maximizing RAG efficiency.pdf", "page": 4, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "embeddingsinsemanticsearchtasks.\nCosinesimilarity\nCosinesimilaritymeasuresthecosineoftheanglebetweentwovectors.GiventwovectorsAand\nBinavectorspace:\n(cid:2)\ncos(A,B)= A·B = (cid:3)\n(cid:2)\nn i=1 A (cid:3)i ·\n(cid:2)\nB i\n(cid:3)A(cid:3)·(cid:3)B(cid:3)\nn i=1 (A i )2· n i=1 (B i )2\nhttps://doi.org/10.1017/nlp.2024.53 Published online by Cambridge University Press", "metadata": {"id": "573dfa313fb8a6b0fe73c7baa3233cf17f6897be", "source": "Maximizing RAG efficiency.pdf", "page": 4, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "NaturalLanguageProcessing 5\nFigure3. Semanticsimilaritysearchprocesswhenfindingtop-Kdocumentsinavectorspacebasedoninputquerytoassign\nscore-weightedranks.\n(cid:129) AandBarevectorsinavectorspace.\n(cid:129) A andB arethecomponentsofvectorsAandBatindexi,respectively.\ni i\n(cid:129) nisthedimensionality(numberofcomponents)ofthevectors.\n(cid:2)\n(cid:129) Thenumerator n i=1(cid:3) A i ·B i represen (cid:3) tsthedotproductofvectorsAandB.\n(cid:2) (cid:2)\n(cid:129) The denominators n i=1 (A i )2· n i=1 (B i )2 represent the Euclidean magnitudes\n(lengths)ofvectorsAandB,respectively.\n2.3 Searchalgorithms", "metadata": {"id": "388af44a0cef7275fd20c72b866197d77b8e66c5", "source": "Maximizing RAG efficiency.pdf", "page": 5, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "(lengths)ofvectorsAandB,respectively.\n2.3 Searchalgorithms\nAttheforefrontofenhancingretrievalefficiencyandspeed,searchalgorithmsplayapivotalrole.\nSpecifically, tree-based algorithms are widely employed in vector databases to measure the dis-\ntance between similar top-K vectors. Nearest neighbor search (NNS) (Chen et al. 2019; Zhang\nandHe2019;MalkovandYashunin2020;Hanetal.2023)identifiesthedatapointsinadataset\nthatarenearesttoaparticularquerypoint,oftenbasedonadistancemeasuresuchasEuclidean\ndistance or cosine similarity. Exact closest neighbor search uses methods like linear search or", "metadata": {"id": "7a15eccd320f64d5192d8bdaab666ef6b000755c", "source": "Maximizing RAG efficiency.pdf", "page": 5, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "distance or cosine similarity. Exact closest neighbor search uses methods like linear search or\ntree-based structures like kd-trees (Bentley 1975; Dolatshah, Hadian, and Minaei-Bidgoli 2015;\nGhojogh,Sharifian,andMohammadzade2018)toidentifythegenuinenearestneighborswithout\napproximations.However,thecomputationalcomplexityofaccuratesearchmightbeprohibitive\nforbigorhigh-dimensionaldatasets.\nApproximateNNS(ANNS)(ZhangandHe2019;Christiani2019;LiandHu2020;Singhetal.\n2021),ontheotherhand,achievesacompromisebetweenaccuracyandefficiency.Byadopting", "metadata": {"id": "a735554c1c3b9f50ab27c73f4437251b6fbda9fe", "source": "Maximizing RAG efficiency.pdf", "page": 5, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "2021),ontheotherhand,achievesacompromisebetweenaccuracyandefficiency.Byadopting\nindexstructuressuchaslocality-sensitivehashing(LSH)(Dasgupta,Kumar,andSarlos2011),or\ngraph-basedtechniques,ittradessomeprecisionforquickerretrieval.ANNSisespeciallybenefi-\ncialinsituationsinvolvinghigh-volumeorhigh-dimensionaldatasets,suchaspictureretrieval,\nrecommendation systems, and similarity search in massive text corpora. To meet the issues of\nboth precise and ANNS, several methods such as k-d trees, LSH, and tree-based structures are\nutilized,withtradeoffstofitspecificusecasesandcomputationalrestrictions.", "metadata": {"id": "3ed93ffb11540583bcf5548b06b7216a335439fc", "source": "Maximizing RAG efficiency.pdf", "page": 5, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "utilized,withtradeoffstofitspecificusecasesandcomputationalrestrictions.\nThe process of similarity search begins by transmitting the vector embeddings of the input\nquery to a preexisting vector store, which contains embeddings of various documents (Feature\nEmbeddings) (refer to Figure 3). The initial step involves identifying the most similar vector\nembeddingswithintheavailablevectorspace.Subsequently,uponlocatingthemostrelevantor\nhttps://doi.org/10.1017/nlp.2024.53 Published online by Cambridge University Press", "metadata": {"id": "b2f9796c83683f7a215f973a3bfaee3a02969072", "source": "Maximizing RAG efficiency.pdf", "page": 5, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "6 T.S¸akarandH.Emekci\nFigure4. Stuffretrieval-augmentedgenerationmethod.\ntop-Kdocuments,thesedocumentsareretrievedandrankedbasedontheirindividualcosinesim-\nilaritytotheinputquery(see:Figure3).ThisretrievalmechanismisfundamentaltoeveryRAG\nmethodology. Consequently, the similarity search and retrieval processes are indispensable and\nnecessitatethoroughevaluationandoptimization.\n2.4 RAGmethods\nVarious RAG methods offer distinct benefits when augmenting LLMs with latent information.\nSelecting the appropriate RAG method is crucial for optimizing retrieval speed, token budget-", "metadata": {"id": "24baee9dce698f83e008e476f6197839b75f2cdc", "source": "Maximizing RAG efficiency.pdf", "page": 6, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "Selecting the appropriate RAG method is crucial for optimizing retrieval speed, token budget-\ning,andresponseaccuracy.RAGmethods,suchasStuff,Refine,MapReduce,andMapRe-rank\ncan directly influence the number of top-K documents, retrieval speed, the number of input\ntokens used for generation, and response time. Therefore, optimizing the RAG methods to suit\nthespecifictaskisofutmostimportance.\n2.4.1Stuffmethod\nThestuffmethodisthemoststraightforwardRAGtechniqueforupdatingtheknowledgebaseof\nanLLMwithlatentinformation(refertoFigure4).Thequeryreceivedfromtheuserissenttothe", "metadata": {"id": "b83e9c1e6d21cc5a347b2f9c5bfe0f0141d72c1a", "source": "Maximizing RAG efficiency.pdf", "page": 6, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "anLLMwithlatentinformation(refertoFigure4).Thequeryreceivedfromtheuserissenttothe\nexistingvectorstoretosearchforsimilarcontentbasedonthespecifiednumberofdocuments.\nThese documents are then incorporated into the system prompt template as context, which the\nLLMusestogeneratearesponse.ThismethodaimstoenableLLMstogeneratewell-structured\nresponsesbasedonallrelevantinformationwithintheentirecontext.However,theStuffmethod\nalso presents cost-efficiency challenges. Providing an LLM with multiple contexts could poten-", "metadata": {"id": "136afbc02900c09c19286fcd250a8bcd02403695", "source": "Maximizing RAG efficiency.pdf", "page": 6, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "also presents cost-efficiency challenges. Providing an LLM with multiple contexts could poten-\ntially increase token usage, as both input and output tokens will be significantly higher unless\nlimitedbymaxtokenhyperparameter.Itisgenerallyanappropriateoptionforapplicationswhere\nRAGprocessesdonotinvolvelongdocuments(Nairetal.2023,LangChainn.d.).\nMoreover, the context window of the selected LLM plays a crucial role, as certain models\nhave quite a limited context window, such as GPT-3.5-Turbo with only a 4096 context token", "metadata": {"id": "6edaac295c2374acdecf7a15ccb8fa2b81ee1ffc", "source": "Maximizing RAG efficiency.pdf", "page": 6, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "have quite a limited context window, such as GPT-3.5-Turbo with only a 4096 context token\nlimit.Therefore,themoredocumentsretrievedfromthevectorstore,themorelikelythecontext\nwindowlimitwillbereached.\n2.4.2Refinemethod\nTherefinemethod,incontrasttothe’Stuff’method,createsananswerbyiterativelyloopingover\nthe input documents and refining its response (refer to Figure 5) (LangChain n.d.). Essentially,\nafter the first iteration, an additional context is generated from LLM call using document n−1 .\nThegeneratedcontextfromdocument n−1 istheninsertedintothesuccessorsystemprompttem-", "metadata": {"id": "14623d8fdda9fccf802d38fc05e4b35bb3a65ef3", "source": "Maximizing RAG efficiency.pdf", "page": 6, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "Thegeneratedcontextfromdocument n−1 istheninsertedintothesuccessorsystemprompttem-\nplateasadditionalcontext,alongwiththenextdocumentcontextintheiteration(document ),\nn\nand then with the combined context ( document n−1 +document n ) and the user query, another\nhttps://doi.org/10.1017/nlp.2024.53 Published online by Cambridge University Press", "metadata": {"id": "5881937e043a89562448220854f6a36ab56717c8", "source": "Maximizing RAG efficiency.pdf", "page": 6, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "NaturalLanguageProcessing 7\nFigure5. Refineretrieval-augmentedgenerationmethod.\nansweriscreated.Thisloopcontinuesuntilthespecifiednumberoftop-Kdocumentsisreached.\nBecausetherefinemethodonlysendsasingledocumenttotheLLMatatime,itisideallysuited\nforapplicationsthatrequiretheanalysisofmoredocumentsthanthemodelcanaccommodate,\nwhichaddressesthetokencontextwindowissue.Thecleardisadvantageisthatthismethodwill\nmake significantly more LLM calls, which is not ideal for token budgeting and response time.\nOntheotherhand,thefinalresponsewillbe‘refined’duetotheenrichedcontextsuppliedfrom", "metadata": {"id": "3b6a39cfcf89948db2cdf860c039415e461e4fbc", "source": "Maximizing RAG efficiency.pdf", "page": 7, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "Ontheotherhand,thefinalresponsewillbe‘refined’duetotheenrichedcontextsuppliedfrom\npredecessorLLMcallsusingnumerousdocuments.\n2.4.3Map-reducemethod\nInthe‘MapReduce’method,similarto’Refine’,eachdocumentisiterativelyusedtogeneratea\nresponse(refertoFigure6)(LangChainn.d.).However,onekeydifferenceinthismethodisthat,\nrather than combining predecessor context with the successor, the final responses are ‘mapped’\ntogether(refertoFigure6).Thesemappedresponsesarethenusedasthefinalcontextwhengen-\nerating the ‘reduced’ response. In the initial phase of the map-reduce process, RAG method is", "metadata": {"id": "6c1879219fb94bbc0205e088c5110411f1a12a41", "source": "Maximizing RAG efficiency.pdf", "page": 7, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "erating the ‘reduced’ response. In the initial phase of the map-reduce process, RAG method is\nsystematicallyappliedtoeachdocumentindependently(mappingphase),withtheresultingout-\nputfromthemethodtreatedasasingledocument.Subsequently,allnewlygenerateddocuments\naredirectedtoaseparatechaindesignedtoconsolidatethemintoasingleoutput(reducestep).\nTheMapReducemethodissuitablefortasksinvolvingshortdocuments,containingonlyafew\npagesperdocument.LongercontextsorlongdocumentsmightcausetheLLMtoreachitstoken\ncontextwindowlimit.\nMoreover,iftheLLMisnotlimitedbyamaximumtokenhyperparameterorisnotinstructed", "metadata": {"id": "5c7157fb09ef295ce225fe3c90d66591f5eb1408", "source": "Maximizing RAG efficiency.pdf", "page": 7, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "contextwindowlimit.\nMoreover,iftheLLMisnotlimitedbyamaximumtokenhyperparameterorisnotinstructed\ntoprovideconciseandshortanswers,the‘reduced’contextcouldalsocausetokencontextlimit\nissues,evenifnotcausedbythedocumentitself.\n2.4.4Mapre-rankmethod\nThis method closely resembles the Map Reduce method but incorporates a filtering application\noneachdocument basedonanassignedcosinesimilarityscore,whichrankseachresponsefrom\nn\nhttps://doi.org/10.1017/nlp.2024.53 Published online by Cambridge University Press", "metadata": {"id": "b5c699b5e0f146e0aeb6530af9946f2b94f86ce3", "source": "Maximizing RAG efficiency.pdf", "page": 7, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "8 T.S¸akarandH.Emekci\nFigure6. MapReducemethod.\nFigure7. Mapre-rankMethod.\nhighest to lowest in terms of similarity to the query (refer to Figure 7) (LangChain n.d.). The\nmapre-rankdocumentschaininitiatesapreliminaryqueryoneachdocument,aimingnotonly\ntoperformataskbutalsotoassignaconfidencescoretoeachanswer.Theanswerwiththehighest\nscoreisthenprovidedastheoutput.\n2.5 AdvancedRAG\nThe RAG methods discussed in Section 2.4 (2.4.1–2.4.4) primarily address issues related to\nthe quality of the final response. However, another significant challenge during retrieval stems", "metadata": {"id": "e71894873cabe3b21fb8e86de524b164af93620e", "source": "Maximizing RAG efficiency.pdf", "page": 8, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "the quality of the final response. However, another significant challenge during retrieval stems\nfrom the ambiguous nature of queries. This ambiguity can result in relevant information being\nhttps://doi.org/10.1017/nlp.2024.53 Published online by Cambridge University Press", "metadata": {"id": "ca9ecdec33a40900574156612a6813827f949ed7", "source": "Maximizing RAG efficiency.pdf", "page": 8, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "NaturalLanguageProcessing 9\nFigure8. Impactofambiguousvsspecificqueryonretrieval.\noverlooked during retrieval, leading to the inclusion of irrelevant documents as context during\ngeneration(LangChainn.d.;Zhengetal.2023;Rackauckas2024).\nTo enhance response accuracy and quality, and to prevent irrelevant documents from being\ninserted into the system prompt as context, more advanced methodologies are needed. These\nincludefilteringoutirrelevantdocuments,creatingalternativequestionsbasedontheoriginaland\nthenrankingeachanswerbasedonsimilarity(similartothe‘MapRe-rank’method),orgenerating", "metadata": {"id": "dc834a4ef15e857bd4af0582f8b935ee2f4e1afc", "source": "Maximizing RAG efficiency.pdf", "page": 9, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "thenrankingeachanswerbasedonsimilarity(similartothe‘MapRe-rank’method),orgenerating\nlessabstractalternativesoftheinputqueryandthenproceedingwithanswergeneration.Filtering\nirrelevant documents by applying similarity score thresholds can potentially exclude irrelevant\ndocumentsandenabletheretrievertoinsertonlyrelevantdocumentsascontext.Moreover,with\nsimilarityfilteringapplied,tokenusageefficiencyisachievedbylimitinginputtokenusageduring\nfiltering.\nAnotherapproachinvolvespopulatingtheinputquerywithlessabstractalternativequestions", "metadata": {"id": "4e7208296b99af8d2119d4fb5bdb9ccd08a67bf1", "source": "Maximizing RAG efficiency.pdf", "page": 9, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "filtering.\nAnotherapproachinvolvespopulatingtheinputquerywithlessabstractalternativequestions\nto address the ambiguity issue (refer to Figure 8). Attaching a question ‘generator chain’ could\npotentiallyprovidemorerelevantquestionsthatalignmorecloselywiththeactualintentofthe\noriginalquery.Thisreductioninabstractioncouldbeachievedbyprovidingasetofinstructions\nforthegeneratorchaintogeneratequestionsbasedonthecontentavailableinthevectorstore.\nConsequently,thegeneratorchainwouldnotproduceirrelevantquestions.Onedrawbackofthis\napproachisthateachqueryresultsinatotalofGC +Q LLMcalls.\ni j", "metadata": {"id": "d6c21bb75d8812aeabdf09709d5e39e2a99118a1", "source": "Maximizing RAG efficiency.pdf", "page": 9, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "approachisthateachqueryresultsinatotalofGC +Q LLMcalls.\ni j\n(cid:129) GC:Thenumberofgeneratorchaincallsperuserquery.\ni\n(cid:129) Q:Thenumberofalternativequestionsgeneratedperuserquery.\nj\nTo exemplify, in a scenario where each query is decomposed (generated by the ‘genera-\ntor chain’) into four different alternatives, and for each generated question, there will be four\nstandaloneRAGchaincalls,totalinguptofivecallsforeachqueryreceivedfromtheuser.\nhttps://doi.org/10.1017/nlp.2024.53 Published online by Cambridge University Press", "metadata": {"id": "1c8d05aa02fdd4794e1f366d9072cfc272a1bdcb", "source": "Maximizing RAG efficiency.pdf", "page": 9, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "10 T.S¸akarandH.Emekci\nFigure9. ContextualCompressormethodappliedontop-Kdocuments.\nAlternatively,thegeneratedquestionsandtheirfollow-upanswerscouldberankedbasedon\nhowcloselytheanswersmatchtheoriginalquery.WhilethisapproachwouldresultinmoreLLM\ncallsandcouldpotentiallyaddresstheambiguityissueandimproveresponsequality,itwouldalso\ndeterioratetokenusageefficiencyandruntimeperformance.\n2.5.1Contextualcompression\nContextual compression resolves the problem of inserting irrelevant documents caused by the\nambiguous nature of a query by compressing retrieved documents based on the cosine similar-", "metadata": {"id": "99393a8cdf59087fcbf2549f2211f0aaeb252ced", "source": "Maximizing RAG efficiency.pdf", "page": 10, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "ambiguous nature of a query by compressing retrieved documents based on the cosine similar-\nityscorebetweenthequeryandeachdocument(LangChainn.d.).TheContextualCompression\nretriever sends queries to the vector store, which initially filters out documents through the\nDocument Compressor. This first step shortens the document list by eliminating irrelevant\ncontent or documents based on the specified similarity threshold. Subsequently, if needed, the\nRedundancyCompressorcanbeappliedasasecondsteptoperformfurthersimilarityfilteringon", "metadata": {"id": "124e807a932bceda6de36e60bd4b424920247988", "source": "Maximizing RAG efficiency.pdf", "page": 10, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "RedundancyCompressorcanbeappliedasasecondsteptoperformfurthersimilarityfilteringon\ntheretrieveddocuments,compressingthecontextevenmore(refertoFigure9).\nSelectinganappropriatedocumentchainisessential.Enhancingthisprocesswithcontextual\nfiltersonembeddings,basedonsimilarityscores,cansignificantlyreducebothinputandoutput\ntokengeneration.However,applyingembeddingfiltersnecessitatesacarefulbalancebetweenthe\nthresholdscoreinsimilaritysearchandtherelevancyoftheresponse.Asthesimilaritythreshold\nscoresincrease,thenumberofcontextdocumentsdecreases,therebyaffectingtherelevanceand", "metadata": {"id": "64699911cf23f55e1870f57b6ed6f64425c2ba29", "source": "Maximizing RAG efficiency.pdf", "page": 10, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "scoresincrease,thenumberofcontextdocumentsdecreases,therebyaffectingtherelevanceand\ncomprehensivenessoftheresponses.\nUtilizingContextualCompressionalongwithvariousRAGmethodscouldpotentiallyprovide\nfurtherefficiencyinbothinputtokenandoutputtokengeneration.\n2.5.2Querystep-down\nTo address the issue of ambiguity, the ‘Query Step-Down’ method offers a potential solution\nbygeneratingvariantquestionsbasedonthecontentwithinthedocuments(refertoFigure10)\n(Zhengetal.2023).Thisapproachcanbeparticularlyusefulfordomain-specifictaskswhereusers\nmaylacksufficientinformationaboutthecontent.", "metadata": {"id": "df6724ce054cb218fc0096f6bec8cadd31ad6010", "source": "Maximizing RAG efficiency.pdf", "page": 10, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "maylacksufficientinformationaboutthecontent.\nTheQueryStep-downprocessinitiatesa‘generatorchain’taskedwithformulatingquestions\nbasedonthecontentavailablewithinthedocumentsandtheuserquery.Subsequently,foreach\ngenerated question, the vector store is employed to retrieve the top-k documents correspond-\ningtoeachquestion .Tofurtheroptimizethisapproach,employingdiverseRAGmethodscould\nn\nenhancevariousaspectssuchasresponsetime,tokenusage,andhardwareutilization.Giventhe\nhigh volume of LLM calls made during each conversational transaction, reducing the context", "metadata": {"id": "91566be85ba2a780461b28cff643ef609803f2fd", "source": "Maximizing RAG efficiency.pdf", "page": 10, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "high volume of LLM calls made during each conversational transaction, reducing the context\nretrievedfromthevectorstorethroughadvancedmethodologies(e.g.,ContextualCompression)\ncouldaddressefficiencyconcernseffectively.\n2.5.3ReciprocalRAG\nReciprocalRAG,akintothe‘QueryStep-Down’method,addressestheissueofambiguitythrough\na similarity score-based ranking process. Rather than aggregating all generated responses for\nhttps://doi.org/10.1017/nlp.2024.53 Published online by Cambridge University Press", "metadata": {"id": "b9ff7f34909df4fe4edf57d01cdd51b2256a7c86", "source": "Maximizing RAG efficiency.pdf", "page": 10, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "NaturalLanguageProcessing 11\nFigure10. QueryStep-Downmethod.\nFigure11. Reciprocalretrieval-augmentedgenerationmethod.\neach produced query to form a final answer, reciprocal RAG employs a ranking mechanism—\ncomparableto‘MapRe-Rank’—toselectivelyfilterandretainonlythemostpertinentorrelevant\nanswersfromthepoolofpossibleresponses(refertoFigure11).Thismethodpotentiallyreduces\ntokenusagebyprioritizingrelevanceandprecisioninthefinaloutput(Rackauckas2024).\nhttps://doi.org/10.1017/nlp.2024.53 Published online by Cambridge University Press", "metadata": {"id": "8904a3ddfa4f4dccd03aa965953c9a1e598f392a", "source": "Maximizing RAG efficiency.pdf", "page": 11, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "12 T.S¸akarandH.Emekci\n3. Data\nThis paper utilizes three domain-specific datasets. The first dataset, the Docugami Knowledge\nGraph RAG dataset, includes 20 PDF documents containing SEC 10Q filings from companies\nsuchasApple,Amazon,Nvidia,andIntel.These10QfilingscovertheperiodfromQ32022toQ3\n2023.Thedatasetcomprises196questionsderivedfromthedocuments,withreferenceanswers\ngeneratedbyGPT-4.TheseconddatasetemployedistheLlama2Paperdataset,whichconsistsof\ntheLlama2ArXivPDFasthedocument,alongwith100questionsandGPT-4generatedreference", "metadata": {"id": "12d6c195ac4d62f0b83c3b89a18a0eb7abb032c9", "source": "Maximizing RAG efficiency.pdf", "page": 12, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "theLlama2ArXivPDFasthedocument,alongwith100questionsandGPT-4generatedreference\nanswers. The third dataset utilized is the MedQA dataset, a medical examination QA dataset.\nThisdatasetfocusesonthereal-worldEnglishsubsetinMedQA,featuringquestionsfromtheUS\nMedicalLicensingExamination(MedQA-US),including1273four-optiontestsamples.\nThesignificanceofutilizingdiversedatasetsinoptimizingRAGprocessesisrootedinthecom-\nplexnatureofthecontentwithineachdataset.Thesedatasetsoftencontainavarietyofelements,\nincluding numerical values, multilingual terms, mathematical expressions, equations, and tech-", "metadata": {"id": "6923fa451129152d2b3cded798a7ecc8508dcd03", "source": "Maximizing RAG efficiency.pdf", "page": 12, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "including numerical values, multilingual terms, mathematical expressions, equations, and tech-\nnicalterminology.Giventhedistinctnatureofthesedocuments,optimizingtheRAGprocesses\nbecomescrucial.Thisoptimizationensuresefficienthandlingoftheretrievalprocessduringsub-\nsequentanalysis.Toprocessthedata,weinitiallysplitthecharactersintotokensbasedon1000\nchunksizealongwith100overlappingchunksforeachdataset.Weemployedthetiktokenencoder\nandtheRecursiveCharacterSplitmethodforrecursivesplitting,ensuringthatsplitsdonotexceed", "metadata": {"id": "eae2be3f6d5ce327ea62df53f4c95933ebfd2e4c", "source": "Maximizing RAG efficiency.pdf", "page": 12, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "andtheRecursiveCharacterSplitmethodforrecursivesplitting,ensuringthatsplitsdonotexceed\nthe specified chunk size. The subsequent merging of these splits together completes the data\nprocessingsteps.\nInthenextstep,agridsearchoptimizationwasconducted,exploringdifferentdatasets,vector\ndatabases,RAGmethods,LLMs,EmbeddingModels,andembeddingfilterscores.RAGperfor-\nmancewasassessedbymeasuringthecosinesimilaritybetweentheembeddedLLManswerand\nthereferenceanswerfromeachdatasetandquestion-answerpairs.\nAdditionally, various performance metrics were created to monitor parameters such as run", "metadata": {"id": "6c33559a120370c97e3abc769d6ccc43051caac2", "source": "Maximizing RAG efficiency.pdf", "page": 12, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "Additionally, various performance metrics were created to monitor parameters such as run\ntime(sec),centralprocessingunit(CPU)usage(%),memoryusage(%),tokenusage,andcosine\nsimilarity scores. Token usage calculation was done by applying the following formula: T =\ni\n(LR ×4)/3,whereT isthetokenusageattheithiteration,andLR isthelengthoftheresponse\ni i i\ngeneratedbyLLMfortheithiteration.\n4. Results\nAcomprehensivesetof23,625grid-searchiterationswasconductedtoobtaintheresultspresented\nherein. Various embedding models were employed, including OpenAI’s flagship embedding", "metadata": {"id": "60889ed05d354ccefde4ba9c3ce933bab9a38970", "source": "Maximizing RAG efficiency.pdf", "page": 12, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "herein. Various embedding models were employed, including OpenAI’s flagship embedding\nmodel (text-embedding-v3-large), BAAI’s (Beijing Academy of Artificial Intelligence) open-\nsourcebge-en-small,andCohere’scohere-en-v3.TheLLMsusedinthisstudycomprisedGPT-3.5,\nGPT-4o-mini, and Cohere’s Command-R. For vectorstores, we utilized ChromaDB, FAISS, and\nPinecone. Additionally, seven different RAG methodologies were implemented to complete the\ntrials,andlastly,wedeployedawiderangeofcontextualcompressionfilters.Intotal,42.12million", "metadata": {"id": "c97e46691512993bb1bd7adba1fc35ae6bb26862", "source": "Maximizing RAG efficiency.pdf", "page": 12, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "trials,andlastly,wedeployedawiderangeofcontextualcompressionfilters.Intotal,42.12million\nembeddingtokensand18.46milliontokens(combinedinputandoutput)weregeneratedbythe\ndeployedLLMs.Thecumulativeruntimeforalliterationswasapproximately112uninterrupted\nhours.\n4.1 Similarityscoreperformances\nWeimplementedarangeofRAGmethodologies,LLMs,embeddingmodels,anddatasetstodeter-\nminewhichcombinationwouldproducethehighestsimilarityscore.AmongtheRAGmethods,\nReciprocalRAGemergedasthemosteffectivewitha%91similarity(Figure12),followedbyStep-", "metadata": {"id": "2de24c2afa65de57e42a42ae34398e683a9055ec", "source": "Maximizing RAG efficiency.pdf", "page": 12, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "ReciprocalRAGemergedasthemosteffectivewitha%91similarity(Figure12),followedbyStep-\nDown(%87),Stuff(%86),andMapReduce(%85)methodologies.AselaboratedinSection2.5.3,\nReciprocalRAGaimstopopulateinputqueriesbasedonthecontentwithinthedocumentsand\nhttps://doi.org/10.1017/nlp.2024.53 Published online by Cambridge University Press", "metadata": {"id": "6e442bb6bf014fc6c3e65180eb479e2451746e51", "source": "Maximizing RAG efficiency.pdf", "page": 12, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "NaturalLanguageProcessing 13\nFigure12. Medianruntime(sec)comparisonsbyretrieval-augmentedgenerationmethods,datasets,embeddingmodels,\nandlargelanguagemodels.\nthegeneratedalternativequestions.Eachquestionisthenqueriedwithinthevectorstoretogen-\nerate an answer. Subsequently, all generated answers are filtered based on a desired number of\ndocumentsorsimilarityscore.Weused50%similaritythreshold,whichisthedefaultvalueacross\nalliterationswhereReciprocalRAGmethodisutilized.\nThisfilteringstepisparticularlyadvantageousinscenarioswheresomeofthegeneratedques-", "metadata": {"id": "2b3c9749d732e8e01ad5e65edbd974cc55b336e4", "source": "Maximizing RAG efficiency.pdf", "page": 13, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "Thisfilteringstepisparticularlyadvantageousinscenarioswheresomeofthegeneratedques-\ntionsmaybelesspertinenttotheoriginalquery.Byomittingtheanswersanddocumentsretrieved\nfromless-relevantandpopulatedqueries(refertoSection2.5.3),thisfilteringprocessensuresthat\nonlythemostrelevantinformationisretained.ThisdistinguishesReciprocalRAGfromtheStep-\nDown method, which aggregates all answers without filtering (refer to Section 2.5.2), as well as\nfromothermethodologiesthatdonotadequatelyaddressoraimtoresolvequeryambiguity.", "metadata": {"id": "63a5b9bed83f590e1be705bb93f80fe26e387780", "source": "Maximizing RAG efficiency.pdf", "page": 13, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "fromothermethodologiesthatdonotadequatelyaddressoraimtoresolvequeryambiguity.\nIn the evaluation of LLMs, Cohere’s Command-R model stands out as the top performer,\nachievinganimpressivesimilarityscoreof83%,whichis2.4%higherthanthatofGPT-3.5and\n3.75%higherthanGPT-4o-Mini(refertoFigure12).ThesignificanceofemployingvariousLLMs\nisunderscoredwhentestedagainstdifferentdatasetsacrossdiversedomains.Forinstance,10Q\ndocuments,whichpredominantlycontainfinancialinformationsuchasincomestatementsand\nbalancesheets,arelargelycomposedofnumericaldata.Similarly,LLama2documentsincludenot", "metadata": {"id": "e319daed79a67f06e2a02026b53cf8aaa72db152", "source": "Maximizing RAG efficiency.pdf", "page": 13, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "balancesheets,arelargelycomposedofnumericaldata.Similarly,LLama2documentsincludenot\nonlynumericalvaluesbutalsomathematicalexpressions,notations,andformulas.Consequently,\nanyerrorsingenerationbyanLLMwhendealingwithcomplexorchallengingcontextscouldsub-\nstantiallyimpactthesimilarityscore.Furthermore,MedQAdocumentsarerepletewithtechnical\nand multilingual terminology, primarily in Latin. Certain LLMs may not be extensively trained\non such documents or may have been trained on a smaller corpus of similar contentcompared", "metadata": {"id": "a64a5c59778fb759ab7c67f38e41f55d2610d3ab", "source": "Maximizing RAG efficiency.pdf", "page": 13, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "on such documents or may have been trained on a smaller corpus of similar contentcompared\nto other datasets. Therefore, evaluating different datasets enables a comprehensive capability\nhttps://doi.org/10.1017/nlp.2024.53 Published online by Cambridge University Press", "metadata": {"id": "ab8546c3417e8e4d04a6151d0c45191d5e9d0176", "source": "Maximizing RAG efficiency.pdf", "page": 13, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "14 T.S¸akarandH.Emekci\nTable1. Similarityscores,runtime,andtokenusageforvariousretrieval-augmentedgenerationmethods\nacrossdifferentdatasets.Thedatasetsusedinthisstudydifferincomplexityanddomainspecificity,thusthe\nresultsareseparatedandevaluatedseparately\nDataset RAGmethod Similarityscores Runtime(sec) Tokenusage\nMedian Std Median Std Median Std\n10Q Reciprocal 0.971 ±0.015 24.56 ±2.51 3187 ±411", "metadata": {"id": "179420fd9e3acb2659cbdc1cf8c311915e840b89", "source": "Maximizing RAG efficiency.pdf", "page": 14, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "Median Std Median Std Median Std\n10Q Reciprocal 0.971 ±0.015 24.56 ±2.51 3187 ±411\n........................................................................................................................................................................................................................................\n10Q Stuff 0.895 ±0.188 14.93 ±2.48 937 ±642.57\n........................................................................................................................................................................................................................................", "metadata": {"id": "1a301390b2f7b79fa86164312788f71c8dab9635", "source": "Maximizing RAG efficiency.pdf", "page": 14, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "10Q Step-Down 0.890 ±0.029 33.46 ±8.85 5527 ±926\n........................................................................................................................................................................................................................................\n10Q MapReduce 0.873 ±0.190 24.25 ±8.40 896 ±609\n........................................................................................................................................................................................................................................", "metadata": {"id": "de87e6818dc4517079252c04b23604eea2ba3e70", "source": "Maximizing RAG efficiency.pdf", "page": 14, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "10Q Refine 0.836 ±0.088 29.74 ±11.04 2298 ±940\n........................................................................................................................................................................................................................................\n10Q MapRerank 0.801 ±0.278 18.74 ±3.48 308 ±365\n........................................................................................................................................................................................................................................", "metadata": {"id": "f3b738c35bb2ede485cd8cda07872ccb768602eb", "source": "Maximizing RAG efficiency.pdf", "page": 14, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "Llama2 Reciprocal 0.930 ±0.105 24.81 ±3.04 2707 ±689\n........................................................................................................................................................................................................................................\nLlama2 Step-Down 0.927 ±0.061 34.94 ±2.61 4308 ±1189\n........................................................................................................................................................................................................................................", "metadata": {"id": "661f19a95f245dad364d88f7059136a399fc254f", "source": "Maximizing RAG efficiency.pdf", "page": 14, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "Llama2 Stuff 0.905 ±0.200 15.05 ±3.81 890.66 ±1147\n........................................................................................................................................................................................................................................\nLlama2 MapReduce 0.892 ±0.207 25.89 ±13.64 1190 ±1240\n........................................................................................................................................................................................................................................", "metadata": {"id": "52d915edf709798b10cc8a56b036deb3aec50b34", "source": "Maximizing RAG efficiency.pdf", "page": 14, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "Llama2 Refine 0.830 ±0.135 29.16 ±15.28 1982 ±1509\n........................................................................................................................................................................................................................................\nLlama2 MapRerank 0.827 ±0.310 17.62 ±3.90 418 ±400\n........................................................................................................................................................................................................................................", "metadata": {"id": "57a105bdd965ee2492dc3c7e3d5f58f48f39f208", "source": "Maximizing RAG efficiency.pdf", "page": 14, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "MedQA Step-Down 0.678 ±0.045 33.01 ±3.97 4393 ±600\n........................................................................................................................................................................................................................................\nMedQA Reciprocal 0.673 ±0.183 19.97 ±5.55 703 ±629\n........................................................................................................................................................................................................................................", "metadata": {"id": "fb45a487f46ad9addd0e372dbd06bd1904635869", "source": "Maximizing RAG efficiency.pdf", "page": 14, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "MedQA Stuff 0.641 ±0.284 13.48 ±0.86 155 ±192\n........................................................................................................................................................................................................................................\nMedQA Refine 0.609 ±0.095 25.18 ±5.42 3174 ±558\n........................................................................................................................................................................................................................................", "metadata": {"id": "b40ed08eb72f198a5203c4e49da5d2fc30216587", "source": "Maximizing RAG efficiency.pdf", "page": 14, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "MedQA MapReduce 0.569 ±0.288 17.66 ±1.58 220 ±275\n........................................................................................................................................................................................................................................\nMedQA MapRerank 0.148 ±0.312 15.21 ±0.60 56 ±76\ncomparisonofvariousLLMs,particularlyintaskssuchasaccuratelyhandlingnumericalvalues,\nmathematicalexpressions,andmultilingualterminology.\nIntheevaluationofembeddingmodels,BAAI’s’Bge-en-small’modeldemonstratedremark-", "metadata": {"id": "16566183271143c686d096912f7bd391d6d1855d", "source": "Maximizing RAG efficiency.pdf", "page": 14, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "Intheevaluationofembeddingmodels,BAAI’s’Bge-en-small’modeldemonstratedremark-\nableperformance,achievingamediansimilarityscoreof94%.Thisscoreisnotably20.5%higher\nthanCohere’sflagshipembeddingmodel’Cohere-en-v3’and22%higherthanOpenAI’sflagship\nembeddingmodel’Text-embedding-v3-large’(refertoFigure12).Thesignificanceofembedding\nmodels is paramount, extending beyond the initial conversion of documents into dense vector\nrepresentations (embeddings) to encompass the retrieval of semantically meaningful text from\nthevectorspace(vectorstore).Embeddingmodelstrainedforcomplextasks—suchascapturing", "metadata": {"id": "c7905fe229cf9762bd2796a974a15dfe8ce93392", "source": "Maximizing RAG efficiency.pdf", "page": 14, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "thevectorspace(vectorstore).Embeddingmodelstrainedforcomplextasks—suchascapturing\nsemanticrelationshipsbetweentextualandnumericaldata,mathematicalexpressions,ormulti-\nlingualtexts—introduceadditionalconsiderationsforenhancingtheprocessesandefficiencyof\nRAGsystems.\nUponallpossibleiterationsanddatasets,’ReciprocalRAG’achievedthehighestsimilarityscore\n(Medianscore:97.1%,Std:±0.015)acrossbothDokugami’s10QandLlama2datasets,yetnotthe\nlowestruntime(sec)ortokenusage(refertoTable1).Thetradeoffbetweenresponseaccuracy,", "metadata": {"id": "82d6f9bbb3e181a4c5dc470f8f5e9e98739e2f2e", "source": "Maximizing RAG efficiency.pdf", "page": 14, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "lowestruntime(sec)ortokenusage(refertoTable1).Thetradeoffbetweenresponseaccuracy,\nrun time, and token usage plays crucial significance when designing RAG process for specific\ntasks.\nhttps://doi.org/10.1017/nlp.2024.53 Published online by Cambridge University Press", "metadata": {"id": "8242e0224e3388be24ec70e41e82bb1167bc8a5a", "source": "Maximizing RAG efficiency.pdf", "page": 14, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "NaturalLanguageProcessing 15\nAs a result, we concluded that in RAG-based applications where response accuracy is\nparamount, Reciprocal RAG yields the best performances. RAG processes that might require\nthe highest possible response accuracy could involve financial, insurance, and research-related\ndocuments. On the other hand, in applications where response time and token usage are\nmore significant, such as building chatbots for high volume usages, ’Stuff’ method could\nbe utilized, since it yielded the 70.5% lower token usage, along with 38.9% faster response", "metadata": {"id": "c30cfa8cfaa2436eb36c97506464275720bb40c4", "source": "Maximizing RAG efficiency.pdf", "page": 15, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "be utilized, since it yielded the 70.5% lower token usage, along with 38.9% faster response\ntime, while only giving up 7.2% response accuracy compared to ‘Reciprocal RAG’ (refer to\nTable1).\nAdditionally, in terms of minimizing token usage, the ’Map Re-rank’ method demonstrated\nexceptionalperformance.Itachievedsimilarityscoresof83.0%and82.7%withtheLlama2and\n10Qdocuments,respectively,whilegeneratingonly418and308outputtokens.\nAllRAGmethodsexhibitedsignificantlylowersimilarityscores—someevenyieldingthepoor-\nestresults—whenappliedtotheMedQAdataset.Thisoutcomecanbeprimarilyattributedtothe", "metadata": {"id": "9c8a13de21165fd3bd9482a1cb41b5862af3b3c9", "source": "Maximizing RAG efficiency.pdf", "page": 15, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "estresults—whenappliedtotheMedQAdataset.Thisoutcomecanbeprimarilyattributedtothe\ndataset’snature,whichinvolveschallengingquestion-and-answerpairswithinahighlyspecialized\ndomain.MedQAdocumentsfocusonmedicalsurgerycontent,whereabstractornuancedques-\ntions can substantially alter the expected answers, presenting a more formidable challenge for\nRAG-based applications. Consequently, methods designed to address ambiguity yielded higher\nsimilarity scores, with ’Step-Down’ achieving 67.8% and ‘Reciprocal’ attaining 67.3% (refer to", "metadata": {"id": "00db5f0657155fd4f5fabbb3468e2937ee13a2b9", "source": "Maximizing RAG efficiency.pdf", "page": 15, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "similarity scores, with ’Step-Down’ achieving 67.8% and ‘Reciprocal’ attaining 67.3% (refer to\nTable1),comparedtoothermethodologiesthatdonotaddresssuchambiguity.Moreover,‘Map\nRe-rank’ yielded the lowest score on MedQA dataset with only 14.8% similarity. These results\nunderscoretheimportanceofretrievingthecorrectdocumentswithsufficientcontext,highlight-\ning that filtering context for LLMs prior to generation is not of even greater significance than\nthe quality context. The results demonstrate that even with less content if it is irrelevant, the\nperformanceisadverselyaffected.", "metadata": {"id": "fdd6d8310f5fbdafaccbb6f9eeff42d58ca761f6", "source": "Maximizing RAG efficiency.pdf", "page": 15, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "performanceisadverselyaffected.\n4.2 Hardwareutilization\nAnothercriticalaspectofthisresearchistoassesshowvariousLLMs,embeddingmodels,datasets,\nandRAGmethodsimpacthardwareconsumption,includingCPU(%)usage,memory(%)usage,\nandruntime.HardwareutilizationbecomesincreasinglysignificantwhendeployingRAG-based\napplicationsforhigh-volumeusage.\nFurther, we evaluated hardware utilization across three categories: runtime (in seconds),\nwhichmeasuresthespeedofobtainingananswer;CPUusage(percentage),whichindicatesthe\nproportion of the workload handled by the CPU; and memory usage, which assesses the mem-", "metadata": {"id": "32430d5f50466376aef24920dd5436c50c244645", "source": "Maximizing RAG efficiency.pdf", "page": 15, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "proportion of the workload handled by the CPU; and memory usage, which assesses the mem-\nory consumption during retrieval. Additionally, we considered the impact of different LLMs,\nembeddingmodels,RAGmethods,anddatasetsonthesemetrics.\nWeobservedthatthe’Step-Down’methodresultedinthehighestmedianruntimeof34.33s,\nwhereasthe’Stuff’methodexhibitedthelowestruntimeat14.29s(refertoFigure13).Asdetailed\ninTable1,thedifferenceinsimilarityscoresbetweenthetop-performingmethodsisonly7.2%\ncomparedtothe’Stuff’method.Additionally,the’Stuff’methoddemonstrateda71.7%improve-", "metadata": {"id": "3b2c1f731890c6fff322854a5938d16b80e75e1f", "source": "Maximizing RAG efficiency.pdf", "page": 15, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "comparedtothe’Stuff’method.Additionally,the’Stuff’methoddemonstrateda71.7%improve-\nmentintokenusageefficiencyrelativetothetop-performingmethod.Despitebeingthesimplest\napproachandnotaddressingqueryambiguity,the’Stuff’methodprovestobethemostefficient,\nalbeitwithmarginallylowersimilarityscores.\nFurthermore, the comparative results revealed that among the embedding models, Cohere’s\n’Cohere-en-v3’ exhibited the slowest median run time of 20.19s, whereas OpenAI’s ‘Text-\nembedding-v3-large’ demonstrated the fastest median run time of 18.55s, achieving a 0.91%", "metadata": {"id": "574776b8ae470a06f48f74df21b9a821952d29d8", "source": "Maximizing RAG efficiency.pdf", "page": 15, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "embedding-v3-large’ demonstrated the fastest median run time of 18.55s, achieving a 0.91%\nimprovement in run time efficiency (refer to Figure 13). The inclusion of embedding mod-\nels in the experiment was intended to assess which model could achieve the most rapid run\ntime. This aspect is crucial for high-volume and high-scale applications with even denser\nvectorstores.\nhttps://doi.org/10.1017/nlp.2024.53 Published online by Cambridge University Press", "metadata": {"id": "0a318c8026c2963699e46c5451810edd8e6554a3", "source": "Maximizing RAG efficiency.pdf", "page": 15, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "16 T.S¸akarandH.Emekci\nFigure13. Medianruntime(sec)comparisonsbyretrieval-augmentedgenerationmethods,datasets,embeddingmodels,\nandlargelanguagemodels.\nAdditionally, we assessed the run time differences among various LLMs. The OpenAI GPT-\n3.5-Turbomodelemergedasthetopperformerintermsofgenerationtime,withamedianrun\ntimeof18.15s.Thismodelachievedgenerationtimesthatwere9.7%and17.0%fastercompared\ntoGPT-4o-MiniandCommand-R,respectively(refertoFigure13).Achievingrapidgeneration\ntimesisparticularlycrucialforapplicationssuchaseducationalchatbots,whereswiftresponses", "metadata": {"id": "362aab69eb439922eab0024170430b33037cce2e", "source": "Maximizing RAG efficiency.pdf", "page": 16, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "timesisparticularlycrucialforapplicationssuchaseducationalchatbots,whereswiftresponses\nareneededtoaccommodatelowerattentionspans.Moreover,thetradeoffbetweenruntimeand\nresponseaccuracybecomesmorepronouncedastasksordomainsvary.Incaseswherethepri-\nmarygoalistomitigateresponsehallucinationoraddressqueryambiguity,generationtimemay\nbeoflesserconcern.Conversely,chatbotapplicationsdesignedforrolessuchasinformationdesks\nor promotional purposes, such as university promotion, may tolerate certain levels of response\nhallucinationinfavorofminimizingresponsetime.", "metadata": {"id": "08b4705bce487460ba8c321bc259a2dc450ccae0", "source": "Maximizing RAG efficiency.pdf", "page": 16, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "hallucinationinfavorofminimizingresponsetime.\nWe also evaluated the performance of different embedding models and LLM combinations\nintermsofhardwareutilization.GPT-3.5-Turbo,whenpairedwith‘Bge-en-small‘,achievedthe\nfastest run time of 17.55 ±5.92s, though it did not exhibit the lowest CPU utilization percent-\nage(refertoTable2).Generally,GPT-3.5-Turboisidentifiedasthefastestinresponsetime,but\nthe choice of embedding model also significantly impacts this efficiency. The difference in run\ntimeforGPT-3.5-Turbo,whenusedwithvariousembeddingmodels,is3.7%betweenthefastest", "metadata": {"id": "cd7ecc3d14af3cbb34d67805bba8303e400b47b2", "source": "Maximizing RAG efficiency.pdf", "page": 16, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "timeforGPT-3.5-Turbo,whenusedwithvariousembeddingmodels,is3.7%betweenthefastest\nand slowest configurations (refer to Figure 14). Considering CPU and memory usage–critical\nfactors for cloud-deployed applications handling high-volume requests–we concluded that the\n‘Text-embedding-v3-large’ embedding model when used with the GPT-3.5-Turbo LLM, results\ninthelowestCPU%usage.Thisconfigurationsacrificesonly3.7%ofruntimecomparedtothe\nfastestsetup,whichisGPT-3.5-Turbopairedwith‘Bge-en-small’(refertoTable2).\nhttps://doi.org/10.1017/nlp.2024.53 Published online by Cambridge University Press", "metadata": {"id": "2aa067339340089c482734c44ca51b885f82b905", "source": "Maximizing RAG efficiency.pdf", "page": 16, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "NaturalLanguageProcessing 17\nTable2.Utilizationmetricsforvariousembeddingmodelsandlargelanguagemodels\nLLM EmbeddingModel Runtime(sec) CPUusage Memoryusage\nMedian Std Median Std Median Std\ngpt-3.5-turbo bge-en-small 17.55 ±5.92 2.95 ±8.31 0.0 ±0.5904\n........................................................................................................................................................................................................................................\ngpt-3.5-turbo cohere-en-v3 18.11 ±6.09 2.70 ±4.326 0.0 ±0.5590", "metadata": {"id": "40b6b59297bf6d9a4dd1a8fdca10066d360516b5", "source": "Maximizing RAG efficiency.pdf", "page": 17, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "gpt-3.5-turbo cohere-en-v3 18.11 ±6.09 2.70 ±4.326 0.0 ±0.5590\n........................................................................................................................................................................................................................................\ngpt-3.5-turbo text-embedding-v3-large 18.28 ±5.60 1.40 ±4.759 0.0 ±0.2674", "metadata": {"id": "24e43a1474f5e7dd5b404b1b945b1292fe4d66ff", "source": "Maximizing RAG efficiency.pdf", "page": 17, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "gpt-3.5-turbo text-embedding-v3-large 18.28 ±5.60 1.40 ±4.759 0.0 ±0.2674\n........................................................................................................................................................................................................................................\ngpt-4o-mini bge-en-small 18.67 ±10.17 3.20 ±9.45 0.0 ±0.2645", "metadata": {"id": "58115c2287a3397a4abb3739268265f3fd4b7f33", "source": "Maximizing RAG efficiency.pdf", "page": 17, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "gpt-4o-mini bge-en-small 18.67 ±10.17 3.20 ±9.45 0.0 ±0.2645\n........................................................................................................................................................................................................................................\ngpt-4o-mini text-embedding-v3-large 19.87 ±9.68 3.19 ±9.147 0.1 ±0.2247\n........................................................................................................................................................................................................................................", "metadata": {"id": "058e5244cf084aca103f02b5c3215e5321129097", "source": "Maximizing RAG efficiency.pdf", "page": 17, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "command-r text-embedding-v3-large 20.41 ±14.09 3.95 ±7.907 0.1 ±0.2827\n........................................................................................................................................................................................................................................\ncommand-r bge-en-small 21.80 ±17.36 3.20 ±7.912 0.1 ±0.2492\n........................................................................................................................................................................................................................................", "metadata": {"id": "ef0e204528c2da56241f0cff6a426eabf0edb058", "source": "Maximizing RAG efficiency.pdf", "page": 17, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "gpt-4o-mini cohere-en-v3 22.07 ±10.42 3.09 ±3.683 0.1 ±0.797\n........................................................................................................................................................................................................................................\ncommand-r cohere-en-v3 25.64 ±18.42 2.00 ±3.964 0.0 ±0.3547\nFigure14. MedianCPU(%)usagecomparisonsbyretrieval-augmentedgenerationmethods,datasets,embeddingmodels,\nandlargelanguagemodels.\n4.3 Vectordatabases\nAnother aspect of our evaluation involved identifying which combination of vectorstores and", "metadata": {"id": "178d67a030f6cc88099eb7e2df13814998e68033", "source": "Maximizing RAG efficiency.pdf", "page": 17, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "Another aspect of our evaluation involved identifying which combination of vectorstores and\nembeddingmodelsprovidesthelowestruntimewhilealsominimizinghardwareutilization.This\nhttps://doi.org/10.1017/nlp.2024.53 Published online by Cambridge University Press", "metadata": {"id": "65fbc5b7e77fa65845f637830d5f1f6cfe3b804f", "source": "Maximizing RAG efficiency.pdf", "page": 17, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "18 T.S¸akarandH.Emekci\nTable3.Performancemetricsforvectorstoresystemswithdifferentembeddingmodels\nVectorstore Embeddingmodel Runtime CPUusage Memoryusage\nmedian std median std median std\nChromaDB text-embedding-v3-large 18.34 ±6.21 1.50 ±1.93 0.00 ±0.20\n........................................................................................................................................................................................................\nbge-en-small 19.15 ±13.998 1.98 ±2.20 0.00 ±0.432", "metadata": {"id": "b70463e8d562da3072c1d1058f8f5746330510fa", "source": "Maximizing RAG efficiency.pdf", "page": 18, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "bge-en-small 19.15 ±13.998 1.98 ±2.20 0.00 ±0.432\n........................................................................................................................................................................................................\ncohere-en-v3 19.22 ±13.528 2.65 ±2.59 0.00 ±0.485\n........................................................................................................................................................................................................................................\nFAISS bge-en-small 19.40 ±11.33 9.75 ±10.178 0.10 ±0.35", "metadata": {"id": "0234c5fb8ec40984c98f353ea3142030311f4cbf", "source": "Maximizing RAG efficiency.pdf", "page": 18, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "FAISS bge-en-small 19.40 ±11.33 9.75 ±10.178 0.10 ±0.35\n........................................................................................................................................................................................................\ntext-embedding-v3-large 19.75 ±12.22 6.80 ±10.89 0.10 ±0.365\n........................................................................................................................................................................................................\ncohere-en-v3 33.60 ±7.69 3.54 ±1.044 0.15 ±0.936", "metadata": {"id": "827127d65ff36f9449dbd5c703bceff0af74d480", "source": "Maximizing RAG efficiency.pdf", "page": 18, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "cohere-en-v3 33.60 ±7.69 3.54 ±1.044 0.15 ±0.936\n........................................................................................................................................................................................................................................\nPinecone bge-en-small 19.64 ±9.59 1.70 ±7.66 0.10 ±0.52\n........................................................................................................................................................................................................\ntext-embedding-v3-large 21.85 ±8.31 1.79 ±8.38 0.10 ±0.65", "metadata": {"id": "0b901ee23f1d19ee4b0c7334539000b9fe66873d", "source": "Maximizing RAG efficiency.pdf", "page": 18, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "text-embedding-v3-large 21.85 ±8.31 1.79 ±8.38 0.10 ±0.65\n........................................................................................................................................................................................................\ncohere-en-v3 24.835 ±9.88 2.84 ±8.68 0.15 ±0.62\nconfigurationiscrucialforachievingcost-efficiencyinhigh-volume,scalable,andcloud-deployed\napplications. Notably, maintaining scalability of the vectorstore without significantly compro-\nmising performance is essential, as the primary goal of RAG is to expand the vectorstore with", "metadata": {"id": "1b2cf674f01d4b3adebad9f16cf17bb9a9fbc4b4", "source": "Maximizing RAG efficiency.pdf", "page": 18, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "mising performance is essential, as the primary goal of RAG is to expand the vectorstore with\nadditionaldocuments.TheperformancemetricsdisplayedinTable3underscorethesignificance\nofselectingtheappropriatevectorstoreandembeddingmodelcombination,asvariationsinrun\ntime,CPUusage,andmemoryusageareevidentacrossdifferentsetups.Forinstance,ChromaDB\npaired with the ’Text-embedding-v3-large’ model exhibits a median run time of 18.34 ± 6.21s,\nalongsideamedianCPUusageof1.50%andvirtuallynegligiblememoryusage.\nConversely,thecombinationofFAISSandthe’Cohere-en-v3’modelresultsinahighermedian", "metadata": {"id": "86ee804da100e6580a894865c6784d0413cec06b", "source": "Maximizing RAG efficiency.pdf", "page": 18, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "Conversely,thecombinationofFAISSandthe’Cohere-en-v3’modelresultsinahighermedian\nruntimeof33.60±7.69sandanincreasedmemoryusageof0.15±0.936%,alongwitharelatively\nhighCPUdemandcomparedtheremainingconfigurations.Thesediscrepancieshighlightthata\ntailoredapproachinconfiguringvectorstoresandembeddingmodelsisessentialforoptimizing\nperformanceparameterscriticaltoRAGtasks.Thus,identifyingthebestconfigurationcansignif-\nicantlyenhanceprocessingefficiency,resourcemanagement,andoverallsystemresponsiveness,\nwhicharecrucialforhigh-volumeandlarge-scaleRAG-basedapplications.Uponfurtherexami-", "metadata": {"id": "79c5d1b8d4837ed131b82e025d39ab85f704fdcc", "source": "Maximizing RAG efficiency.pdf", "page": 18, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "whicharecrucialforhigh-volumeandlarge-scaleRAG-basedapplications.Uponfurtherexami-\nnationofvariousRAGmethodologiesandvectorstores,weobservedthatthe’Stuff’method,when\ncombinedwiththePineconevectorstore,achievedthelowestruntime,averaging12.14±1.136s\n(refer to Figure 15). Conversely, despite being one of the top performers in terms of similarity\nscores, the ’Step-Down’methodconsistently resulted in the slowest run times, regardless of the\nvectorstoreconfiguration.\nAdditionally, the discrepancy between response accuracy and hardware utilization becomes", "metadata": {"id": "08ad2ab17d0a169a01e736034694d24ed64195a5", "source": "Maximizing RAG efficiency.pdf", "page": 18, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "Additionally, the discrepancy between response accuracy and hardware utilization becomes\nmorepronouncedwhenconsideringCPU(%)andMemory(%)usages,asdisplayedinFigure15\nfor both the Reciprocal and Step-Down RAG methodologies. Although both methodologies\ndemonstrate impressive performances in similarity scores (refer to Table 1), they are also con-\ncluded to be the most demanding in terms of hardware utilization. These results signify the\nimportanceofselectingasuitableconfigurationforRAGapplications,whichutterlydependson", "metadata": {"id": "d0860d6c20c3f8402126fa6a3e3a6f6ec3dffc4b", "source": "Maximizing RAG efficiency.pdf", "page": 18, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "importanceofselectingasuitableconfigurationforRAGapplications,whichutterlydependson\ntheusecaseandavailableresources.Thesefindingsunderscorethecriticalimportanceofselect-\ning an appropriate configuration for RAG applications, which should be determined based on\nthe specific use case and available resources. We also observed that across all RAG methodolo-\ngies,ChromaDByieldedthelowestruntime,CPU(%),andmemory(%)consumption(referto\nhttps://doi.org/10.1017/nlp.2024.53 Published online by Cambridge University Press", "metadata": {"id": "5239fea5df336b6b3d53ed7c464041c08453ae99", "source": "Maximizing RAG efficiency.pdf", "page": 18, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "NaturalLanguageProcessing 19\nFigure15. Medianruntime(sec),CPU,andmemoryusagecomparisonsbyretrieval-augmentedgenerationmethodson\ndifferentvectorstores.\nhttps://doi.org/10.1017/nlp.2024.53 Published online by Cambridge University Press", "metadata": {"id": "c1de1635c730c60f2b59a1177583507ff46759f6", "source": "Maximizing RAG efficiency.pdf", "page": 19, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "20 T.S¸akarandH.Emekci\nTable4.Performancemetricsforvectorstoresystems\nRuntime CPUusage Memoryusage\nVectorstore median std median std median std\nChromaDB 18.635112 ±9.733899 1.800 ±2.133639 0.0 ±0.319043\n........................................................................................................................................................................................................................................\nFAISS 19.888412 ±11.792252 7.300 ±10.447708 0.1 ±0.459941", "metadata": {"id": "4de681b3e415d89fe45364cfa6be95b813921d78", "source": "Maximizing RAG efficiency.pdf", "page": 20, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "FAISS 19.888412 ±11.792252 7.300 ±10.447708 0.1 ±0.459941\n........................................................................................................................................................................................................................................\nPinecone 22.582000 ±9.277844 2.495 ±8.196688 0.1 ±0.598845\nTable4)with18.63±9.73,1.80±2.13,and0.0±0.31,runtime(sec),CPU(%),andmemory(%)\nusage,respectively.\nConsideringtheabovefactors,theoptimalconfigurationoftheRAGmethod,LLM,embedding", "metadata": {"id": "09607e809c41a8d251012414ba31c0437180f4e8", "source": "Maximizing RAG efficiency.pdf", "page": 20, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "usage,respectively.\nConsideringtheabovefactors,theoptimalconfigurationoftheRAGmethod,LLM,embedding\nmodel,andvectorstoreishighlydependentonthespecificusecase.However,thecombination\nofthe’Stuff’method,‘GPT-3.5-Turbo’astheLLM,‘ChromaDB’vectorstore,and‘Bge-en-small’\nembeddingmodeldemonstratedbetterperformance,exhibitinglowermedianscoresinruntime,\nCPUusage(%),andmemoryusage(%).\n4.4 Embeddingfilters\nToenhanceeachRAGmethodology,weappliedContextualCompression(seeSection2.5.1)to\neachretrievalprocess.Thisapproachaimedtoreducetokenusage,lowerruntimes,andimprove", "metadata": {"id": "9b2c87dd8c146ccf863a400504e18ec6838e6556", "source": "Maximizing RAG efficiency.pdf", "page": 20, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "eachretrievalprocess.Thisapproachaimedtoreducetokenusage,lowerruntimes,andimprove\nsimilarityscoreperformance.Duetothedelicatebalancebetweenfilters—suchassimilarityand\nredundancy thresholds—and the similarity between document embeddings in vectorspace and\nuserqueries,weconductedagridsearchoverarangeoffiltersoneachRAGmethodanddifferent\ndatasets.Startingfrom0.5forsimilarityandredundancyscoresto0.9,weusedastepvalueof0.1\ntoderivetheresultspresentedbelow(refertoTable5).\nAshighlightedinTable5,weachievedsignificantefficiencyintokenusageandruntimeacross", "metadata": {"id": "f6330a5090e60eaaa404733b5a24863c6928b953", "source": "Maximizing RAG efficiency.pdf", "page": 20, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "AshighlightedinTable5,weachievedsignificantefficiencyintokenusageandruntimeacross\nvariousRAGmethods.Withthe‘MapReduce’method,weobservedan8.99%reductionintoken\nusage and a 7.2% decrease in runtime. Conversely, the ‘Map Re-rank’ method did not show\nimprovementsintokenusageorruntime;however,itconcludedwithlowerstandarddeviations.\nMoreover,the’Reciprocal’methoddemonstratedsubstantialefficiencywitha12.5%reductionin\ntokenusageanda4.40%decreaseinruntime.Similarly,the’Refine’methodresultedinanimpres-\nsive 18.6% reduction in token usage and a 3.05% improvement in runtime. Additionally, the", "metadata": {"id": "1cfa904c720f72a89be1628f4beb626079112129", "source": "Maximizing RAG efficiency.pdf", "page": 20, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "sive 18.6% reduction in token usage and a 3.05% improvement in runtime. Additionally, the\n’Step-Down’methodachievedan8.04%reductionintokenusageandanotable13.98%improve-\nmentinruntime.Lastly,the’Stuff’methoddidnotshowamedianimprovementintokenusage;\nhowever, it achieved a 13.18% reduction in standard deviation and a 1.39% improvement in\nruntime.\nWhile filtering out document embeddings below a certain threshold value, some valuable\ninformation might not retrieved by the retriever. This issue is particularly pronounced when", "metadata": {"id": "47685951a3990ce97d6878e1456cf48a3316a2f2", "source": "Maximizing RAG efficiency.pdf", "page": 20, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "information might not retrieved by the retriever. This issue is particularly pronounced when\nsimilarity scores are evaluated after applying the contextual compression filters. Our analy-\nsis indicates that across all RAG methods, the similarity score deteriorates, emphasizing the\ntradeoff between response accuracy and resource management once again. Specifically, in the\n’Map Reduce’ method, the similarity score deteriorated by 4.7%, in ’Map Re-rank’ by 7.89%,\nin ’Reciprocal’ by 7.69%, in ’Refine’ by 7.59%, and in ’Stuff’ by a notable 17.44% (refer to\nTable6).", "metadata": {"id": "17974fd69e25f6071c6925523baf0a09ec189340", "source": "Maximizing RAG efficiency.pdf", "page": 20, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "Table6).\nAsdiscussedinSection2.5.1,contextualcompressionfiltersoutirrelevantdocumentsbyeval-\nuatingdocumentembeddingsbasedontheirsimilarityscoresrelativetotheuserquery.Ifcertain\ndocument embeddings contain both valuable and irrelevant information, they may be filtered\nout due to a lower overall similarity score resulting from the redundant information within.\nhttps://doi.org/10.1017/nlp.2024.53 Published online by Cambridge University Press", "metadata": {"id": "3be93c235afb8182e68114cc15face846e73b143", "source": "Maximizing RAG efficiency.pdf", "page": 20, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "NaturalLanguageProcessing 21\nTable5.ComparisonofContextualCompressiononmedianandstandarddeviationoftokenusage,runtime,\nandscore\nRAGMethod Tokenusage Runtime Score\nMedian Std Median Std Median Std\nCC− map_reduce 602.67 ±964.04 21.09 ±10.50 0.85 ±0.28\n................................................................................................................................................................................................................\nmap_rerank 60.00 ±346.94 16.49 ±3.45 0.76 ±0.35", "metadata": {"id": "1bf5bbc2d31134d3cdc78d277cf20eda0289e049", "source": "Maximizing RAG efficiency.pdf", "page": 21, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "map_rerank 60.00 ±346.94 16.49 ±3.45 0.76 ±0.35\n................................................................................................................................................................................................................\nreciprocal 3132.00 ±570.94 24.98 ±3.87 0.91 ±0.16\n................................................................................................................................................................................................................\nrefine 1768.00 ±1314.15 26.16 ±12.61 0.79 ±0.15", "metadata": {"id": "05f02f2cca83633f99d5118f77cffdea4f98a841", "source": "Maximizing RAG efficiency.pdf", "page": 21, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "refine 1768.00 ±1314.15 26.16 ±12.61 0.79 ±0.15\n................................................................................................................................................................................................................\nstep-down 4436.00 ±938.43 34.33 ±6.03 0.87 ±0.12\n................................................................................................................................................................................................................\nstuff 606.00 ±909.19 14.29 ±2.93 0.86 ±0.26", "metadata": {"id": "a1a900d1ce2c2bca447be38d1fe69dabf028b25b", "source": "Maximizing RAG efficiency.pdf", "page": 21, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "stuff 606.00 ±909.19 14.29 ±2.93 0.86 ±0.26\n..........................................................................................................................................................................................................................................\nCC+ map_reduce 548.44 ±813.37 19.57 ±6.81 0.81 ±0.24\n................................................................................................................................................................................................................\nmap_rerank 64.48 ±267.78 16.89 ±1.35 0.70 ±0.28", "metadata": {"id": "0518a4b96972141dd502dcbba4d1918583575c0f", "source": "Maximizing RAG efficiency.pdf", "page": 21, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "map_rerank 64.48 ±267.78 16.89 ±1.35 0.70 ±0.28\n................................................................................................................................................................................................................\nreciprocal 2738.00 ±743.04 23.88 ±3.00 0.84 ±0.25\n................................................................................................................................................................................................................\nrefine 1439.01 ±1153.74 25.36 ±2.21 0.73 ±0.24", "metadata": {"id": "794b5fa1b79955fcdb3a4ea5bc63dccfc8c7ceb9", "source": "Maximizing RAG efficiency.pdf", "page": 21, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "refine 1439.01 ±1153.74 25.36 ±2.21 0.73 ±0.24\n................................................................................................................................................................................................................\nstep-down 4079.28 ±1187.14 29.53 ±3.10 0.78 ±0.19\n................................................................................................................................................................................................................\nstuff 670.00 ±789.30 14.09 ±1.43 0.71 ±0.21", "metadata": {"id": "8ac15fa796ec0fb7d116a72096d64e2d52973cfb", "source": "Maximizing RAG efficiency.pdf", "page": 21, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "stuff 670.00 ±789.30 14.09 ±1.43 0.71 ±0.21\nNote:CC+ContextualCompressionmethodapplied\nTable6.Comparisonofruntime,score,andsimilaritythreshold\nDataset CC−Score CC+Similarity CC+Redundancy CC+Score\nthreshold threshold\nMedian Std Median Std Median Std Median Std\n10Q 0.865 ±0.200 0.80 ±0.100 0.80 ±0.100 0.821 ±0.189\n........................................................................................................................................................................................................................................", "metadata": {"id": "371119bc48cf9f178dc22f39e293933ae4d5d23c", "source": "Maximizing RAG efficiency.pdf", "page": 21, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "Llama2 0.871 ±0.217 0.70 ±0.100 0.80 ±0.100 0.791 ±0.346\n........................................................................................................................................................................................................................................\nMedQA 0.605 ±0.291 0.50 ±0.100 0.60 ±0.100 0.510 ±0.170\nNote:CC+ContextualCompressionmethodapplied\nConsequently,theretrieveddocumentmaylacksufficientinformationtoprovideadequatecon-\ntext for the LLM to generate a relevant answer. Furthermore, applying a redundancy threshold", "metadata": {"id": "2afb79d290dedcb74dbe90333995f3e2dbea462f", "source": "Maximizing RAG efficiency.pdf", "page": 21, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "text for the LLM to generate a relevant answer. Furthermore, applying a redundancy threshold\naddsanotherlayeroffiltering,potentiallyexcludingadditionaldocumentsandlimitingtheLLM’s\ncontext to generate a comprehensive response. This issue, where documents contain dispersed\nandmixedinformation,significantlydeterioratestheresponseaccuracyperformancewhencon-\ntextualcompressionisapplied.AsobservedinTable6,weconcludethattheoptimalconfiguration\nofbothsimilarityandredundancythresholdfiltersvariessignificantlydependingonthequality\nofthedocumentsorwhetherthereisadisparityofinformationwithindocuments.", "metadata": {"id": "c7668b3e2f7043065726c31aa94e359d5d744492", "source": "Maximizing RAG efficiency.pdf", "page": 21, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "ofthedocumentsorwhetherthereisadisparityofinformationwithindocuments.\nOur findings indicate that each dataset possesses distinct optimal filter thresholds, beyond\nwhichperformancesignificantlydeterioratesduetosufficientinformationnotbeingavailableat\nhigherthresholdswhengeneratingadequateresponses.Forthe10Qdataset,wedeterminedthat\nasimilaritythresholdof80%andaredundancythresholdof80%wouldlimitthedeteriorationin\nsimilarityscoreto5.08%.InthecaseoftheLlama2dataset,weconcludedthattheoptimalthresh-\noldvaluesare70%forsimilarityand80%forredundancy.Lastly,fortheMedQAdataset,alower", "metadata": {"id": "3963f91dcad3b2fc6ba5d39c29f51b619e93e140", "source": "Maximizing RAG efficiency.pdf", "page": 21, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "oldvaluesare70%forsimilarityand80%forredundancy.Lastly,fortheMedQAdataset,alower\nhttps://doi.org/10.1017/nlp.2024.53 Published online by Cambridge University Press", "metadata": {"id": "a31d383cd6ed4d3ed173de1d2792e4b53fea2524", "source": "Maximizing RAG efficiency.pdf", "page": 21, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "22 T.S¸akarandH.Emekci\nthresholdvaluewasrequiredtomitigatedeterioration,withthebestconfigurationbeing50%for\nsimilarityand60%forredundancy,resultingina15.7%reductioninsimilarityscoreperformance\n(refertoTable6).Beyondtheseoptimalfilteringvalues,orthresholds,weobservedasignificant\ndeclineinthesimilarityscore.Thisdeclineisattributabletoeitheraninsufficientnumberofrele-\nvantdocumentsortheomissionofcriticalinformationburiedwithinotherirrelevantdocument\nembeddings.(refertoTable6).\nIn essence, contextual compression should be employed to achieve a balance between token", "metadata": {"id": "63a6a64686163ad439866305ec91b52e769a57aa", "source": "Maximizing RAG efficiency.pdf", "page": 22, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "In essence, contextual compression should be employed to achieve a balance between token\nusage, run-time, and similarity score. However, this balance must be carefully fine-tuned to\nminimizethedeteriorationofthesimilarityscore.\n5. Conclusion\nIn addressing the existing gap in research on the optimization of RAG processes, this paper\nembarked on a comprehensive exploration of various methodologies, particularly focusing on\nRAGmethods,vectordatabases/stores,LLMs,embeddingmodels,anddatasets.Themotivation\nstemmedfromtherecognizedsignificanceofoptimizingandimprovingRAGprocesses,asunder-", "metadata": {"id": "08e21b4e4bc9f1780951b2c19bc98e8533eb36e1", "source": "Maximizing RAG efficiency.pdf", "page": 22, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "stemmedfromtherecognizedsignificanceofoptimizingandimprovingRAGprocesses,asunder-\nscoredbynumerousstudies(Vaithilingametal.2022;Nairetal.2023;TopsakalandAkinci2023;\nManathunga and Illangasekara 2023; Nigam et al. 2023; Pesaru et al. 2023; Peng et al. 2023;\nKonstantinosandPouwelseAndriopoulosandJohan2023;Roychowdhuryetal.2023).\nThroughacomprehensivegrid-searchoptimizationencompassing23,625iterations,wecon-\nducted a series of experiments to evaluate the performance of various RAG methods (Map\nRe-rank,Stuff,MapReduce,Refine,QueryStep-Down,Reciprocal).Theseexperimentsinvolved", "metadata": {"id": "ed881c3f8416ba5b0830e472d038e14835120878", "source": "Maximizing RAG efficiency.pdf", "page": 22, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "Re-rank,Stuff,MapReduce,Refine,QueryStep-Down,Reciprocal).Theseexperimentsinvolved\ndifferent vectorstores (ChromaDB, FAISS, and Pinecone), embedding models (Bge-en-small,\nCohere-en-v3, and Text-embedding-v3-large), LLMs (Command-R, GPT-3.5-Turbo, and GPT-\n4o-Mini), datasets (Dokugami 10Q, LLama2, and MedQA), and contextual compression filters\n(SimilarityandRedundancyThresholds).\nOur findings highlight the significance of optimizing the parameters involved in developing\nRAG-basedapplications.Specifically,weemphasizeaspectssuchasresponseaccuracy(similarity", "metadata": {"id": "4d933e005394b59df9a5cfb2f1bb6621ef52a1e0", "source": "Maximizing RAG efficiency.pdf", "page": 22, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "RAG-basedapplications.Specifically,weemphasizeaspectssuchasresponseaccuracy(similarity\nscore performance), vectorstore scalability, hardware utilizations; run-time efficiency, CPU(%),\nandMemory(%)usages.Theseconsiderationsarecrucialundervariousconditions,includingdif-\nferentembeddingmodels,diversedatasetsacrossvariousdomains,differentLLMs,andvarious\nRAGmethodologies.\nThe results of our grid-search optimization highlight the significance of context quality over\nsimilarity-basedrankingprocessesorothermethodsthataggregateallresponsesiterativelyasthe", "metadata": {"id": "db245698060012c4346aa9a35eee820b73f935aa", "source": "Maximizing RAG efficiency.pdf", "page": 22, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "similarity-basedrankingprocessesorothermethodsthataggregateallresponsesiterativelyasthe\nfinaloutput.Specifically,contextqualitydemonstratesgreaterimportancethansimplyapplying\nsimilarity-basedmethods,whichmayresultinonlymarginalimprovementsinsimilarityscores\nbyexcludinglessrelevantcontextfromtherankings.Additionally,thediscussiononthedistinc-\ntion between ambiguity and specificity in user queries (see Section 2.5) further emphasizes the\nneed for increased focus on this methodology. Methods addressing the ’ambiguity’ issue yield", "metadata": {"id": "6a6e4f7b4d713393695a834b6b0c56e576ec729d", "source": "Maximizing RAG efficiency.pdf", "page": 22, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "need for increased focus on this methodology. Methods addressing the ’ambiguity’ issue yield\nhigher similarity scores (refer to Table 1) compared to those that do not address it. However, a\nhighersimilarityscoreincursacost,creatingatradeoffbetweenresponseaccuracy(representedby\nthesimilarityscore)andresourcemanagement,whichincludesrun-time,tokenusage,andhard-\nwareutilization(seeSections4.1–4.4).Toaddressthisissue,ourobjectivewastodeterminethe\noptimal configurations that balance similarity score performance with run-time efficiency, vec-", "metadata": {"id": "b28007270716a8cd641bc3a64f98d853a0e5d9f1", "source": "Maximizing RAG efficiency.pdf", "page": 22, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "optimal configurations that balance similarity score performance with run-time efficiency, vec-\ntorstorescalability,tokenusage,aswellasCPUandmemoryusage.Theresultsrevealednuanced\nperformancesacrossdifferentiterations.\nInourevaluationsforRAGmethods,The’Reciprocal’methodemergedasaparticularlyeffec-\ntive approach, demonstrating higher similarity score compared to other methods, achieving up\nto 91% (see Figure 13 and Table 1). However, this method also led to an increased number of\nhttps://doi.org/10.1017/nlp.2024.53 Published online by Cambridge University Press", "metadata": {"id": "ca94d23bb70e4989ee4b8bbc7f81791757ba357e", "source": "Maximizing RAG efficiency.pdf", "page": 22, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "NaturalLanguageProcessing 23\nLLM calls, resulting in greater token usage and extended run time, as it addresses the query-\nambiguity problem (refer to Section 2.5.3, Figure 14). In contrast, although the ’Stuff’ method\ndoesnotaddressquery-ambiguityissue,itexhibitedanacceptablylowersimilarityscoreperfor-\nmancecomparedtothatofthe’Reciprocal’RAGmethodwhileonlysacrificing7.2%performance,\nwhile cutting down token usage by 71.3% and improving run time by 33.89%(see Table 1 and\nFigure14).Moreover,wealsoobservedthat,wheninadequatecontextisretrieved,regardlessof", "metadata": {"id": "cebd4c41e32031a3f7f2b55205072cfed101ce91", "source": "Maximizing RAG efficiency.pdf", "page": 23, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "Figure14).Moreover,wealsoobservedthat,wheninadequatecontextisretrieved,regardlessof\nany post-retrieval ranking processes, the response accuracy significantly deteriorates across all\ndatasets(refertoTable1),especiallywith’MapRe-rank’methodandMedQAdataset.\nIn consideration of hardware utilization, our analysis reveals that GPT-3.5-Turbo, when\nemployed with the ’Bge-en-small’ embedding model, demonstrated a median run-time perfor-\nmanceof17.55±5.92s,alongsideaCPUusageof2.95%±8.31.Conversely,whenintegratedwith\nthe’Text-embedding-v3-large’embeddingmodel,wenotedamarginal3.99%reductioninrun-", "metadata": {"id": "940138c392e6feafd9d58526427689bf2cde3f1b", "source": "Maximizing RAG efficiency.pdf", "page": 23, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "the’Text-embedding-v3-large’embeddingmodel,wenotedamarginal3.99%reductioninrun-\ntime,pairedwithasignificant52.5%enhancementinCPUusageefficiencyforGPT-3.5-Turbo.\nThisunderscorestheimportanceofselectingtheoptimalconfigurationofembeddingmodelsand\nLLMstoachievescalabilityimprovementsinhigh-volumeRAG-basedapplications.\nFurthermore, we conclude that vectorstores (vector databases) also play a crucial role in\noptimizing hardware utilization. Notably, the ChromaDB vectorstore demonstrated superior\nperformance in terms of runtime, CPU, and memory usage across all iterations with different", "metadata": {"id": "4a6851fee7f870cb94cbb413bf49cd3b3aeecb4d", "source": "Maximizing RAG efficiency.pdf", "page": 23, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "performance in terms of runtime, CPU, and memory usage across all iterations with different\nembeddingmodelsandvectorstores.Specifically,whenemployedwiththe’Text-embedding-v3-\nlarge’ embedding model, ChromaDB achieved a runtime as low as 18.34 ± 6.21s, and a CPU\nusage of 1.50% ± 1.93. These results put forth another aspect of developing RAG-based appli-\ncationswithmorescalableandstableconfigurationsinordertoexpandthecapabilitiesofRAG\nprocesses.\nIncontextualcompressionevaluations,weobservedthatthetradeoffbetweensimilarityscore", "metadata": {"id": "5d01b6a47af0e83aff59462911a97bbbc512d0aa", "source": "Maximizing RAG efficiency.pdf", "page": 23, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "processes.\nIncontextualcompressionevaluations,weobservedthatthetradeoffbetweensimilarityscore\nperformance and runtime, along with token usage, necessitates a tailored optimization process\ndue to the sensitive nature of similarity search. Inadequate context can significantly deteriorate\nsimilarityscoresduetotheapplicationofhigherfilters(refertoTable6)wherewecanobservethe\nevidentreductioninsimilarityscoreperformances,suchas5.08%for10Q,9.18%forLlama2and\n15.7%forMedQAdatasets.Conversely,identifyingthemostoptimalconfigurationcansubstan-", "metadata": {"id": "4ad75fa01366f620bdf05f2ac6c74f0d6d6b62af", "source": "Maximizing RAG efficiency.pdf", "page": 23, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "15.7%forMedQAdatasets.Conversely,identifyingthemostoptimalconfigurationcansubstan-\ntiallyreducebothtokenusageandruntime.Thishighlightsthenecessityofdeployingcontextual\ncompressionforspecificusecaseswhereslightdeteriorationsinsimilarityscorescanbetolerated\ninexchangeforlowerruntimeandtokenusage(refertoTables5and6).\nInlightofourfindings,itisevidentthatoptimizingRAG-basedapplicationsisessential,given\nthecriticalrolethatparameterssuchassimilarityscoreperformance,run-timeefficiency,vector-\nstorescalability,tokenusage,andCPUandmemoryutilizationplayintheeffectivenessofvarious", "metadata": {"id": "5cd88bea2dd9e946d3e4a941093d78eee547cda3", "source": "Maximizing RAG efficiency.pdf", "page": 23, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "storescalability,tokenusage,andCPUandmemoryutilizationplayintheeffectivenessofvarious\ntools,AIchatbots,andAIagents.Consequently,weemphasizetheimportanceoffurtherresearch\nintooptimizingRAGprocesses,whichcouldleadtosignificantadvancementsinperformance.\nSupplementarymaterial. Thesupplementarymaterialforthisarticlecanbefoundathttps://doi.org/10.1017/nlp.2024.53.\nReferences\nAndriopoulosK.andJohanP.(2023).AugmentingLLMswithknowledge:asurveyonhallucinationprevention.arXiv.\nhttps://doi.org/10.48550/arXiv.2309.16459.", "metadata": {"id": "c84526574cd8aefc8cac764d4c406815e84e31a7", "source": "Maximizing RAG efficiency.pdf", "page": 23, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "https://doi.org/10.48550/arXiv.2309.16459.\nBentleyJ.L.(1975).Multidimensionalbinarysearchtreesusedforassociativesearching.CommunicationsoftheACM18,\n509–517.https://doi.org/10.1145/361002.361007.\nBrownT.,MannB.,RyderN.,SubbiahM.,KaplanJ.D.,DhariwalP.andNeelakantanA.(2020).Languagemodelsare\nfew-shotlearners.AdvancesinNeuralInformationProcessingSystems33,1877–1901\nChen W., Chen J., Zou F., Li Y.-F., Lu P. and Zhao W. (2019). RobustiQ: a robust ANN search method for billion-\nscale similarity search on GPUs. Proceedings of the 2019 International Conference On Multimedia Retrieval (pp.", "metadata": {"id": "bc8a43b0fd139f6ad37b0dc9e1397993526a1a25", "source": "Maximizing RAG efficiency.pdf", "page": 23, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "132–140).https://doi.org/10.1145/3323873.3325018.\nhttps://doi.org/10.1017/nlp.2024.53 Published online by Cambridge University Press", "metadata": {"id": "0fd5d699d3dc835afd4b4e2bdd14536c90952fc7", "source": "Maximizing RAG efficiency.pdf", "page": 23, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "24 T.S¸akarandH.Emekci\nChoiJ.,JungE.,SuhJ.andRheeW.(2021).Improvingbi-encoderdocumentrankingmodelswithtworankersandmulti-\nteacherdistillation.InProceedingsofthe44thInternationalACMSIGIRConferenceOnResearchandDevelopmentin\nInformationRetrieval,2192–2196.https://doi.org/10.1145/3404835.3463076\nChowdheryA.,NarangS.,DevlinJ.,BosmaM.,MishraG.,RobertsA.andBarhamP.(2023).PaLM:scalinglanguage\nmodelingwithpathways.JournalofMachineLearningResearch24,1–113.\nChristianiT.(2019).Fastlocality-sensitivehashingframeworksforapproximatenearneighborsearch.InAmatoG.,Gennaro", "metadata": {"id": "f3b093b90954b8875b53599d728e091866eef6d3", "source": "Maximizing RAG efficiency.pdf", "page": 24, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "C.,OriaV.andRadovanovic´M.(eds),SimilaritySearchandApplications.SpringerInternationalPublishing,pp.3–17.\nhttps://doi.org/10.1007/978-3-030-32047-81.\nCuiJ.,LiZ.,YanY.,ChenB.andYuanL.(2023).ChatLaw:open-sourcelegallargelanguagemodelwithintegratedexternal\nknowledgebases.https://doi.org/10.48550/arXiv.2306.16092.\nDasgupta A., Kumar R. and Sarlos T. (2011). Fast locality-sensitive hashing, In Proceedings of the\n17th ACM SIGKDD International Conference On Knowledge Discovery and Data Mining, 1073–1081.\nhttps://doi.org/10.1145/2020408.2020578,", "metadata": {"id": "9d5ff83c7536a13e1b4036c6391d46988fbaac96", "source": "Maximizing RAG efficiency.pdf", "page": 24, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "https://doi.org/10.1145/2020408.2020578,\nDevlin J., Chang M.-W., Lee K. and Toutanova K. (2019). BERT: pre-training of deep bidirectional transformers for\nlanguageunderstanding.arXiv.https://doi.org/10.48550/arXiv.1810.04805.\nDolatshah M., Hadian A. and Minaei-Bidgoli B. (2015). Ball∗-tree: efficient spatial indexing for constrained nearest-\nneighborsearchinmetricspaces.https://doi.org/10.48550/arXiv.1511.00628.arXiv:1511.00628.\nGhojoghB.,SharifianS.andMohammadzadeH.(2018).Tree-basedoptimization:ameta-algorithmformetaheuristic\noptimization,arXiv:1809.09284.", "metadata": {"id": "eb0fe47da8abecbd0bf68d6f4667d4dca46975e5", "source": "Maximizing RAG efficiency.pdf", "page": 24, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "optimization,arXiv:1809.09284.\nGuptaUtkarsh (2023). GPT-investAR: enhancing stockinvestment strategies throughannualreportanalysis withlarge\nlanguagemodels.SSRNElectronicJournal,arXiv:2309.03079.\nHanYikun,LiuChunjiangandWangPengfei(2023).Acomprehensivesurveyonvectordatabase:storageandretrieval\ntechnique,Challenge.arXiv,2023-10-18.\nJégouH.,DouzeM.,SchmidC.(2011).Productquantizationfornearestneighborsearch.IEEETransactionsOnPattern\nAnalysisandMachineIntelligence33,117–128.https://doi.org/10.1109/TPAMI.2010.57.", "metadata": {"id": "9d8cce60944f24dd0e3d13e954cacb48d7f242af", "source": "Maximizing RAG efficiency.pdf", "page": 24, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "AnalysisandMachineIntelligence33,117–128.https://doi.org/10.1109/TPAMI.2010.57.\nJeong Cheonsu (2023). A study on the implementation of generative AI services using an enterprise data-based LLM\napplicationarchitecture.arXiv.https://doi.org/10.48550/arXiv.2309.\nJohnsonJ.,DouzeM.andJégouH.(2021).Billion-scalesimilaritysearchwithGPUs.IEEETransactionsOnBigData7,\n535–547.\nLangChain. (n.d.) MapRerankDocumentsChain documentation. langChain API documentation. Available at:\nhttps://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.map_rerank.MapRerank", "metadata": {"id": "ca016ac2d8e71913711f61cb54304dd05adbbd3c", "source": "Maximizing RAG efficiency.pdf", "page": 24, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "DocumentsChain.html#langchain.chains.combine_documents.map_rerank.MapRerankDocumentsChain.\nLangChain. (n.d.) Contextual compression documentation. langChain API documentation. Available at:\nhttps://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/contextual_compression/.\nLangChain. (n.d.) MapReduceDocumentsChain documentation. langChain API documentation. Available at:\nhttps://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.map_reduce.MapReduce\nDocumentsChain.html#langchain.chains.combine_documents.map_reduce.MapReduceDocumentsChain.", "metadata": {"id": "688002b90212d5eb67608e7f87bd4b7b8002bb34", "source": "Maximizing RAG efficiency.pdf", "page": 24, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "DocumentsChain.html#langchain.chains.combine_documents.map_reduce.MapReduceDocumentsChain.\nLangChain. (n.d.) RefineDocumentsChain documentation. langChain API documentation. Available at:\nhttps://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.refine.RefineDocuments\nChain.html#langchain.chains.combine_documents.refine.RefineDocumentsChain.\nLangChain. (n.d.) StuffDocumentsChain documentation. langChain API documentation, Available at: https://api.python.\nlangchain.com/en/latest/chains/langchain.chains.combine_documents.stuff.StuffDocumentsChain.html#langchain.", "metadata": {"id": "153fee7fa856b0c2548a9979507a0038506bea35", "source": "Maximizing RAG efficiency.pdf", "page": 24, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "chains.combine_documents.stuff.StuffDocumentsChain.\nLewisM.,LiuY.,GoyalN.,GhazvininejadM.,MohamedA.,LevyO.,StoyanovV.andZettlemoyerL.(2019).BART:\ndenoising sequence-to-sequence pre-training for natural language generation, Translation, and and Comprehension.\narXiv,1910.13461.\nLewisP.,PerezE.,PiktusA.,PetroniF.,KarpukhinV.,GoyalN.andKüttlerH.(2020).Retrieval-augmentedgeneration\nforknowledge-intensiveNLPtasks.InAdvancesinNeuralInformationProcessingSystems,33,CurranAssociates,Inc,pp.\n9459–9474.", "metadata": {"id": "260b52515cd5ddc850bfc55a9b44bcd259f3948a", "source": "Maximizing RAG efficiency.pdf", "page": 24, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "9459–9474.\nLiL.andHuQ.(2020).Optimizedhighorderproductquantizationforapproximatenearestneighborssearch.Frontiersof\nComputerScience14,259–272.\nLinJimmy,PradeepRonak,TeofiliTommasoandXianJasper(2023).VectorsearchwithopenAIembeddings:luceneis\nallyouneed.arXiv,https://doi.org/10.48550/arXiv.2308.14963August28,2023.\nMalkovY.A.andYashuninD.A.(2020).Efficientandrobustapproximatenearestneighborsearchusinghierarchical\nnavigablesmallworldgraphs.IEEETransactionsOnPatternAnalysisandMachineIntelligence42,824–836.", "metadata": {"id": "dbacad73b583b000f595cd3273aea5ec3fc14cb7", "source": "Maximizing RAG efficiency.pdf", "page": 24, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "navigablesmallworldgraphs.IEEETransactionsOnPatternAnalysisandMachineIntelligence42,824–836.\nManathungaS.S.andIllangasekaraY.A.(2023).Retrievalaugmentedgenerationandrepresentativevectorsummarization\nforlargeunstructuredtextualdatainmedicaleducation.arXiv,arXiv:2308.00479.\nMialonG.,DessíR.,LomeliM.,NalmpantisC.,PasunuruR.,RaileanuR.,RozièreB.,(2023).Augmentedlanguagemodels:\nasurvey.arXiv,\nhttps://doi.org/10.1017/nlp.2024.53 Published online by Cambridge University Press", "metadata": {"id": "f8d349511425b36ee094ec0ae4810f9ba6cd33b7", "source": "Maximizing RAG efficiency.pdf", "page": 24, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "NaturalLanguageProcessing 25\nNairI.,SomasundaramS.,SaxenaA.andGoswamiK.(2023).DrillingdownintothediscoursestructurewithLLMsfor\nlongdocumentquestionanswering.arXiv,14593–14606,2311.13565.\nNigamS.K.,MishraS.K.,MishraA.K.,ShallumN.andBhattacharyaA.(2023).Legalquestion-answeringintheIndian\ncontext:efficacy,challenges,andpotentialofmodernAImodels.arXiv,preprintarXiv:2309.14735.\nPengR.,LiuK.,YangP.,YuanZ.andLiS.(2023).Embedding-basedretrievalwithLLMforeffectiveagricultureinformation\nextractingfromunstructureddata.arXiv,2308.03107,https://arxiv.org/abs/2308.03107", "metadata": {"id": "ba76bd2269b02b0cba05d3aae8b93e877dc97a27", "source": "Maximizing RAG efficiency.pdf", "page": 25, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "extractingfromunstructureddata.arXiv,2308.03107,https://arxiv.org/abs/2308.03107\nPesaru A., Gill T. and Tangella A. (2023). AI assistant for document management using lang chain and pinecone.\nInternationalResearchJournalofModernizationinEngineeringTechnologyandScience.\nRackauckasZ.(2024).Rag-fusion:anewtakeonretrieval-augmentedgeneration.InternationalJournalOnNaturalLanguage\nComputing13,37–47,arXivpreprintarXiv:2402.03367.\nRoychowdhuryS.,AlvarezA.,MooreB.,KremaM.,GelpiM.P.,RodriguezF.M.,RodriguezA.,CabrejasJ.R.,SerranoP.", "metadata": {"id": "c43077539159692095db89bdb36e23d4cf254302", "source": "Maximizing RAG efficiency.pdf", "page": 25, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "RoychowdhuryS.,AlvarezA.,MooreB.,KremaM.,GelpiM.P.,RodriguezF.M.,RodriguezA.,CabrejasJ.R.,SerranoP.\nM.,AgrawalP.andMukherjeeA.(2023).Hallucination-minimizedData-to-answerFrameworkforFinancialDecision-\nmakers,arXiv,2311.07592.\nSageA.(2023).Greatalgorithmsarenotenough—,pinecone,RetrievedDecember19,2023.\nSchwaber-CohenR.(2023).ChunkingstrategiesforLLMapplications,Pinecone,RetrievedDecember13,2023,\nSinghA.,SubramanyaS.J.,KrishnaswamyR.andSimhadriH.V.(2021).FreshDiskANN:afastandstreamingsimilarity\nsearch,arXiv,preprintarXiv:2105.09613.", "metadata": {"id": "677ee4373fdcda0f59c20c781436f6778a159d41", "source": "Maximizing RAG efficiency.pdf", "page": 25, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "search,arXiv,preprintarXiv:2105.09613.\nSutskeverIlya,VinyalsOriolandLeQuocV.(2014).Sequencetosequencelearningwithneuralnetworks,arXiv,1409.3215.\narXiv.\nTopsakalO.andAkinciT.C.(2023).CreatinglargelanguagemodelapplicationsutilizinglangChain:aprimerondeveloping\nLLMappsfast.InternationalConferenceOnAppliedEngineeringandNaturalSciences1050–1056.\nVaithilingamP.,ZhangT.andGlassmanE.L.(2022).EvaluatingtheUsabilityofCodeGenerationToolsPoweredbyLarge\nLanguageModels.InExtendedAbstractsofthe2022CHIConferenceonHumanFactorsinComputingSystems", "metadata": {"id": "e6a8e2a4f68107fae3dd50fee23adb091726206d", "source": "Maximizing RAG efficiency.pdf", "page": 25, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "LanguageModels.InExtendedAbstractsofthe2022CHIConferenceonHumanFactorsinComputingSystems\nWeiJ.,WangX.,SchuurmansD.,BosmaM.,IchterB.,XiaF.,ChiE.,LeQ.V.andZhouD.(2022).Chain-of-thought\npromptingelicitsreasoninginlargelanguagemodels.AdvancesinNeuralInformationProcessingSystems35,24824–24837.\nZhangM.andHeY.(2019).GRIP:Multi-StoreCapacity-OptimizedHigh-PerformanceNearestNeighborSearchforVector\nSearchEngine.InProceedingsofthe28thACMInternationalConferenceonInformationandKnowledgeManagement,pp.\n1673–1682.", "metadata": {"id": "f3e85fa18db2889cb3915aef35a75df632e85ede", "source": "Maximizing RAG efficiency.pdf", "page": 25, "created_at": "2025-09-12T03:11:47Z"}}
{"content": "1673–1682.\nZhengH.S.,MishraS.,ChenX.,ChengH.T.,ChiE.H.,LeQ.V.andZhouD.(2023).Takeastepback:evokingreasoning\nviaabstractioninlargelanguagemodels.arXiv,preprintarXiv:2310.06117.\nZhouX.,LiG.andLiuZ.(2023).Llmasdba.arXivpreprintarXiv:2308.05481.\nCitethisarticle:S¸akarTandEmekciH(2025).MaximizingRAGefficiency:AcomparativeanalysisofRAGmethods.Natural\nLanguageProcessing31,1–25.https://doi.org/10.1017/nlp.2024.53\nhttps://doi.org/10.1017/nlp.2024.53 Published online by Cambridge University Press", "metadata": {"id": "31f8c4fec5219c8f38a74e7645690974a58fa6fd", "source": "Maximizing RAG efficiency.pdf", "page": 25, "created_at": "2025-09-12T03:11:47Z"}}
