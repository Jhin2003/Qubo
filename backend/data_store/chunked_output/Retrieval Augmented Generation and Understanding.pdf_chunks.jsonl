{"content": "1\nRetrieval Augmented Generation and Understanding\nin Vision: A Survey and New Outlook\nXu Zheng∗†1,2, Ziqiao Weng∗1,4, Yuanhuiyi Lyu1, Lutao Jiang1, Haiwei Xue1,5, Bin Ren2,7,8, Danda Paudel2,\nNicu Sebe8, Luc Van Gool2,6, Xuming Hu‡1,3,\n1 HKUST(GZ), 2 INSAIT, Sofia University “St. Kliment Ohridski”, 3 HKUST 4 Sichuan University, 5 Tinghua\nUniversity, 6 ETH Zurich, 7 University of Pisa, 8 University of Trento,\n∗: Equal Contributions, †: Project Lead ‡: Corresponding Author.\nPublicationsOnGoogleScholar Understanding Generation\nlsthereamountainin w/o RAG 2D:ACybertruckisspeeding", "metadata": {"id": "2fc7ccb88bb7e64649d9de5c8462189ed7f0fe9f", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 1, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "lsthereamountainin w/o RAG 2D:ACybertruckisspeeding\nthebackground? alongtheGreatWall.\nUser 3D: User\nPrompt\nNo\nLush trees and a Multimodal LLMs\nmountain create a Output Retrieved\npeacefullandscape. Knowledge\nQuery\nYes\nElephants are\nwith RAG Database\nsurrounded by\nnature'sembrace.\nsrepaPdehsilbuPforebmuN\nFig. 1: Cases from retrieval augmented visual understanding and generation.\nAbstract—Retrieval-augmentedgeneration(RAG)hasemerged I. Introduction\nas a pivotal technique in artificial intelligence (AI), particu-\nlarly in enhancing the capabilities of large language models A. Background", "metadata": {"id": "5c7d7f897c615fc2464c7c1ab70e9c85f84137b6", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 1, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "larly in enhancing the capabilities of large language models A. Background\n(LLMs) by enabling access to external, reliable, and up-to-\ndate knowledge sources. In the context of AI-Generated Content Retrieval-augmented generation (RAG) is a transformative\n(AIGC), RAG has proven invaluable by augmenting model out- technique in generative AI, particularly in natural language\nputs with supplementary, relevant information, thus improving processing (NLP) and recommendation systems. It improves\ntheirquality.Recently,thepotentialofRAGhasextendedbeyond", "metadata": {"id": "98880299f649f7eb2a7c2222d4af24e278be62e8", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 1, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "theirquality.Recently,thepotentialofRAGhasextendedbeyond\ncontent quality by integrating external, up-to-date informa-\nnatural language processing, with emerging methods integrating\ntion from knowledge sources [1]. Despite the impressive\nretrieval-augmented strategies into the computer vision (CV)\ndomain. These approaches aim to address the limitations of performance of large language models (LLMs), challenges\nrelying solely on internal model knowledge by incorporating like hallucinations, outdated knowledge, and lack of domain-", "metadata": {"id": "85074dc400563ec8a2fbb6dc9ff5127896b2ddd3", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 1, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "authoritative external knowledge bases, thereby improving both specific expertise remain [1]. RAG addresses these issues by\nthe understanding and generation capabilities of vision models.\nsupplying LLMs with relevant, retrieved factual information,\nThis survey provides a comprehensive review of the current\nenhancing model outputs.\nstate of retrieval-augmented techniques in CV, focusing on two\nmain areas: (I) visual understanding and (II) visual genera- RAG works by using a retriever to extract relevant knowl-", "metadata": {"id": "da3db3f78c2a35fa1b5d8ec339f8b2723ef62b39", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 1, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "tion. In the realm of visual understanding, we systematically edge from external databases [12], which is then combined\nreview tasks ranging from basic image recognition to complex with the model’s input to provide enriched context [13]. This\napplications such as medical report generation and multimodal\napproach is efficient, requiring minimal adaptation and often\nquestion answering. For visual content generation, we examine\nno additional training [2]. Recent studies highlight RAG’s\nthe application of RAG in tasks related to image, video, and", "metadata": {"id": "06682811d19ab5c690d97949e98a06bc3b0d16dc", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 1, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "the application of RAG in tasks related to image, video, and\n3D generation. Furthermore, we explore recent advancements in potential,notonlyforknowledge-intensivetasksbutalsofora\nRAG for embodied AI, with a particular focus on applications broadrangeoflanguage-basedapplications[2],enablingmore\nin planning, task execution, multimodal perception, interaction, accurate and up-to-date outputs.\nand specialized domains. Given that the integration of retrieval-\nWhile traditional RAG pipelines are text-based, real-world\naugmented techniques in CV is still in its early stages, we also", "metadata": {"id": "b3091b198fdd452df605dd737e6acc8267d886e0", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 1, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "augmented techniques in CV is still in its early stages, we also\nhighlight the key limitations of current approaches and propose knowledgeisoftenmultimodal,intheformofimages,videos,\nfuture research directions to drive the development of this and 3D models. This creates challenges when applying RAG\npromising area. Updated information about this survey can be to computer vision (CV). In CV, visual understanding tasks\nfound at https://github.com/zhengxuJosh/Awesome-RAG-Vision.\nlike object identification [14, 15], anomaly detection [16],", "metadata": {"id": "e909ddb5c7254ff97e819c8fc2ee2e1d6f01563b", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 1, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "like object identification [14, 15], anomaly detection [16],\nand segmentation [17] require integrating external knowledge\nIndex Terms—Retrieval Augmented Generation (RAG), Com- to improve accuracy [13]. Similarly, visual generation tasks,\nputer Vision (CV), Understanding, Generation suchastransformingtextualdescriptionsintorealisticimages,\n5202\nraM\n32\n]VC.sc[\n1v61081.3052:viXra", "metadata": {"id": "c3f6d4da6cd7d5dd8eee3412b6675872da0760a7", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 1, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "2\nTABLE I: Comparison of related survey papers on RAG topic.\nYear Paper Focused Areas Main Context GitHub\n2023 Gao et al. [1] LLMs / NLP RAG paradigms and components -\n2024 Fan et al. [2] LLMs / NLP RA-LLMs’ architectures, training, and applications link\n2024 Hu et al. [3] LLMs / NLP RA-LMs’ components, evaluation, and limitations link\n2024 Zhao et al. [4] LLMs / NLP challenges in data-augmented LLMs -\n2024 Gupta et al. [5] LLMs / NLP Advancements and downstream tasks of RAG -\n2024 Zhao et al. [6] RAG in AIGC RAG applications across modalities link", "metadata": {"id": "6735989d638a91b3c3861446b4cc3c0020f43e67", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 2, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "2024 Zhao et al. [6] RAG in AIGC RAG applications across modalities link\n2024 Yu et al. [7] LLMs / NLP Unified evalutaiton process of RAG link\n2024 Procko et al. [8] Graph Learning Knowledge graphs with LLM RAG -\n2024 Zhou et al. [9] Trustworthiness AI Six dimensions and benchmarks about Trustworthy RAG link\n2025 Singh et al. [10] AI Agent Participles and evaluation link\n2025 Ni et al. [11] Trustworthiness AI Road-map and discussion link\n2025 Ours Computer Vision RAG for visual understanding and generation link", "metadata": {"id": "81d8fa87436e7f1d3c69060b242d917f15ceecf9", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 2, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "2025 Ours Computer Vision RAG for visual understanding and generation link\ncan benefit from external knowledge like scene layouts, object retrieval efficiency, modality alignment, computational cost,\nrelationships, and temporal dynamics in videos [15]. and domain adaptation, and discuss the challenges hindering\nGiven the complexity of visual data, RAG can significantly wider adoption. (Section V) (IV) We propose future research\nimprove model performance. For example, scene generation directions to advance RAG in computer vision, focusing on", "metadata": {"id": "15fd66dfab1412361bf8ef3ffb9ff363a2b09c91", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 2, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "models can benefit from knowledge about object interactions real-time retrieval optimization, cross-modal retrieval fusion,\nandspatialrelationships,whileimageclassificationmodelscan privacy-aware retrieval, and retrieval-based generative model-\nenhance accuracy by retrieving up-to-date visual references. ing,highlightingnewopportunitiesforexploration.(SectionV)\nByintegratingexternalknowledge,RAGenhancesbothvisual (V) We extend RAG applications beyond text-based retrieval,\nunderstandingandgeneration,helpingovercomeinherentchal- exploring multimodal frameworks for enhancing vision mod-", "metadata": {"id": "fa97ccfba9a053dae60a3737f93bfc51c88344a0", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 2, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "lenges in vision tasks. As shown in Figure 1, recent research els, and discuss its potential for embodied AI, 3D content\nhasstartedexploringRAG’sintegrationintoCV,aimingtoim- generation, and multimodal learning in robotics, autonomous\nprove both understanding and generation. While large vision- driving, and real-world decision-making.(Section V)\nlanguagemodels(LVLMs)haveshownpromise,theystillface This survey serves as a foundational resource, synthesizing\nchallenges with image generalization and understanding [18]. existing works and offering insights into the future develop-", "metadata": {"id": "e2ffb39d3cd48c66a8729776c3219d0b1119f570", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 2, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "In3Dmodeling,toolslikePhidias[19]useretrieved3Dmod- ment of retrieval-augmented techniques in computer vision.\nelstoguidethegenerationofnewones,improvingqualityand\ngeneralization.However,thefullpotentialofRAG—especially\nC. Related Works\ninenhancingmodeltrustworthiness,robustness,andadaptabil-\nTable I summarizes key contributions in the field of RAG,\nityindynamicenvironments—remainsunderexplored,offering\nprimarily focusing on language models (LLMs) and the in-\na valuable opportunity for future research.\ntegration of external knowledge across modalities. Gao et", "metadata": {"id": "a160185b112b6869b79db9f46a366afddf8de639", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 2, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "tegration of external knowledge across modalities. Gao et\nal. [1] provide an overview of RAG paradigms, highlight-\nB. Contributions ing how external knowledge enhances LLMs. Fan et al. [2]\nThissurveypresentsacomprehensiveandsystematicreview expand on this by discussing RA-LLMs, their architectures,\nofRAGtechniqueswithinthedomainofcomputervision,cov- training strategies, and applications. Hu et al. [3] offer a\nering visual understanding, visual generation, and embodied comprehensive evaluation of Retrieval-Augmented Language", "metadata": {"id": "0cbb51149b1fdd6f543d15ec386869c98df11d1c", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 2, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "vision. Our work synthesizes the advancements, challenges, Models (RALMs), addressing their components, limitations,\nandfuturedirectionsofRAGinCV,makingthefollowingkey and areas for improvement. Zhao et al. [4] identify challenges\ncontributions: (I) We analyze the role of retrieval-augmented in data-augmented LLMs, focusing on retrieval quality and\nmethods in visual understanding(Section II), visual genera- dataintegration.Guptaetal.[5]reviewadvancementsinRAG\ntion(Section III), and embodied vision(Section IV), focusing and explore downstream tasks, shedding light on its evolving", "metadata": {"id": "a3bb9e68ed93500869ddb14faca6689edb20a755", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 2, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "on image, video, and multimodal understanding tasks, as well role in language processing. Zhao et al. [6] extend RAG’s\nas 3D generation, to demonstrate how RAG enhances these scopetoAI-GeneratedContent(AIGC),emphasizingitscross-\ntasks.(II)WeintroduceataxonomyofRAG-basedtechniques modal applications. Yu et al. [7] propose a unified evaluation\nacrossvariousvisiontasks,highlightingkeycontributionsand frameworkforRAG,standardizingperformancemetricsacross\ndifferences, and comparing methods in pattern recognition, tasks.Prockoetal.[8]investigatetheintegrationofknowledge", "metadata": {"id": "4e26c55f099749e182d2d4f23f8122dccfd12998", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 2, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "medical vision, and video analysis. (Section II) (III) We graphswithLLM-basedRAGsystems.Zhouetal.[9]focuson\nidentify key limitations in current RAG applications, such as trustworthiness in RAG, advocating for robust benchmarks to", "metadata": {"id": "552a734c0ac32086cabd993e0dede92526011e43", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 2, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "3\nEg.MMED-RAG Eg.RULE Eg.ColPali Eg.Animate-AS Vision-based\nEg.RAGFlow Eg.RealRAG Text-based\nBalance\nFactuality ImageGeneration Eg.DoFaiR\nEg.FLMR MLLMs OCR VideoGeneration\nEg.MuRAG Single-modal DB Medical DocumentsVQA Generation Inference-based Eg.Phidias\nMulti-modal DB VQA RAG for\n3DGeneration Optimization-based\nMulti-modal Computer Vision\nEg.ReDream Eg.Enwar\nEg.REACT Multi-task\nEg.Animate-A-Story\nUnderstanding Embodied AI\nApplications Multi-Modal Perception\nVideoUnderstanding and Interaction\nImageUnderstanding Planningand\nTaskExecution\nUnderstanding\nLongVideo Recognition Specialized Domains", "metadata": {"id": "e29db9730369e929ef5ab520095d7c20b873511e", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 3, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "TaskExecution\nUnderstanding\nLongVideo Recognition Specialized Domains\nEg.Video-RAG ImageCaptioning\nEg.VideoRAG Image-based Eg.Realgen\nRecognition Eg.RAC Eg.P-rag\nImage and Caption Caption-based\nDetection\nEg.EXTRA Eg.SACO Eg.RALF\nEg.EVCAP\nFig. 2: The main categorization of this survey.\nensurereliableperformance.Singhetal.[10]andNietal.[11] 2) Image Captioning: Image captioning involves generat-\nexploreagenticRAGandtrustworthiness,respectively,offering ing textual descriptions for images. Retrieval-augmented ap-", "metadata": {"id": "43e9f0aa50f3c336c54ee4ad59dfbad80bb8ce2c", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 3, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "frameworks for evaluating and discussing future directions. proaches fall into three categories: (1) retrieving similar or\nBuilding on these foundational works, our research is the style-aware images before caption generation, (2) retrieving\nfirst to focus on RAG in computer vision (CV). We provide related captions, and (3) retrieving both image and caption\na concise review of RAG for visual understanding and gen- embeddings. In the first category, SACO [30] addresses the\neration, emphasizing its potential and challenges. Our work relationship between linguistic style and visual content using", "metadata": {"id": "7e4619faacac7dbe39594f07131706ccc87071f2", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 3, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "extends RAG from language models to visual tasks, offering object-, RoI-, and triplet-based retrieval to capture style-\ninsights into its impact on CV and paves the way for future relevant visual features. The second category includes EV-\nresearch by exploring methods that combine retrieval-based CAP [31], which retrieves object names from an external\ntechniques with visual perception, aiming to improve perfor- visual memory to prompt LLMs for caption generation. In\nmance and real-world applicability. The systemic overview of the third category, EXTRA [32] encodes both input images", "metadata": {"id": "b98efbe632e317fb5adecbdeea88479e58e138bd", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 3, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "our paper is in Figure 2. and retrieved captions to enhance textual context. SAMLL-\nCAP[33]exploresmodelrobustnessbytrainingwithsampled\nretrieved captions from a larger pool, demonstrating that\nII. Retrieval-AugmentedUnderstandinginVision\ndynamic retrieval improves captioning resilience over fixed\nA. Image Understanding top-k selection.\n1) Pattern Recognition: Real-world data follows a long-\ntailed distribution, making it impractical to store every visual B. Video Understanding\ncue due to hardware constraints. To address this, retrieval- 1) Video Understanding: As shown in Table II, video", "metadata": {"id": "0246a5a0f8eeff5b4caf0b5b0e19192b37637055", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 3, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "augmented methods enhance pattern recognition tasks like retrievalandunderstandinghaveadvancedsignificantly,driven\nsegmentation and detection. RAC [28] augments standard by multimodal architectures and retrieval-augmented gener-\nimage classification pipelines with a retrieval module, incor- ation (RAG). This section examines key technical develop-\nporating relevant external information to improve recognition ments and their impact. MM-REACT [34] pioneered a multi-\naccuracy. Beyond classification, retrieval-augmented methods expert system integrating ChatGPT with vision specialists", "metadata": {"id": "bd509d9dc1d8a081713dfb93d8a7df4336ffc510", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 3, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "benefitdetectiontasks,particularlyinopen-setscenarioswhere via prompting. This foundation led to modular frameworks\ntraditionalobjectdetectionstruggleswithclosedcategorysets. like FlashRAG [35], enhancing RAG efficiency, and domain-\nExpanding the vocabulary and incorporating diverse concepts specific adaptations such as VideoRAG [20] and Visual\nsignificantly enhance open-vocabulary detection. RALF [29] RAG [36] for video understanding and visual knowledge\nretrieves related classes and verbalizes concepts, enriching expansion. However, balancing computational efficiency with", "metadata": {"id": "91d6ce34452586d7934c19847357dad5565ea697", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 3, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "visual features with semantically meaningful context. For seg- comprehensive understanding remains a challenge. Practical\nmentation, [4] utilizes DINOv2 features as queries to retrieve implementations have refined video understanding. ViTA [22]\nsimilar samples from a limited annotated dataset, encoding optimizedavideo-to-textpipelineforproduction,whileOmA-\nthem into a memory bank. Using SAM 2’s memory attention gent[25]employedadivide-and-conquerstrategyforcomplex\nmechanism,themodelconditionssegmentationpredictionson tasks. Recent work has improved retrieval accuracy and effi-", "metadata": {"id": "4bcb6ed092faaeb4dced212b335d3921ba18b191", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 3, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "these stored samples, improving accuracy. ciency. Enhanced Multimodal RAG-LLM [37] boosted visual", "metadata": {"id": "5ea55ef347a4347112c007bc7cee88401f2cb9e9", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 3, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "4\n(a)eg: N Spolaôr et al. / Yang Chen et al. / (b) eg: MM-REACT / VideoRAG / Visual RAG / Jihoon Chung et al. /\nZijun Long et al. / FrameFusion / StreamingRAG Video-of-Thought / Goldfish / Rui Li et al. / Vinci / Video-Panda\nIndexing Retrieval Indexing Retrieval Geneator\nLLMs\nVideo Database Question Video Database Question\n(c) eg: ViTA / Haomiao Xiong et al.\n(d) eg: FlashRAG / ViPCap / iRAG\nchunking Indexing\nIndexing Retrieval\nVideo Clips Database\nVideo Database\nRetrieval Geneator Reranker Refiner\nLLMs\nQuestion\nQuestion", "metadata": {"id": "a1dfb0eaa2a0a98bf9b0d0054d16554df80cf7c4", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 4, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "Video Clips Database\nVideo Database\nRetrieval Geneator Reranker Refiner\nLLMs\nQuestion\nQuestion\n(e) eg: Xinmiao Yu et al. / Junxiao Xue et al. / VideoAuteur / Video-RAG / OmAgent\nchunking Analysis Tools Indexing Retrieval Geneator\nLLMs\nVideo Clips LLMs Database Question\nFig. 3: Five different RAG for video retrieval and understanding pipelines.\nTABLE II: Video Retrieval and Understanding Methods\nMethod Retrieval Metric Augmentation Method Video Specific Operation Video Corpus\nVideoRAG[20] Cosinesimilarity Concatenation Adaptiveframeselection Howto100m[21]", "metadata": {"id": "6e45df0ab015a15f27d63525dc129f092191b3f2", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 4, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "VideoRAG[20] Cosinesimilarity Concatenation Adaptiveframeselection Howto100m[21]\nViTA[22] MLLMscaptions Textualdescription Videoclips StreetAware[23]&TokyoMODI[24]\nOmAgent[25] MLLMscaptions Scenedescriptions Detection/Facerecognition Sok-bench[26]\nStreamingRAG[27] MLLMscaptions Knowledgebase Temporalcontextidentifier -\nquestion answering, while ViPCap [38] demonstrated the ad- their architectural approaches:\nvantages of integrating video and text retrieval. (a)BasicRetrievalPipeline(e.g.,NSpolaôretal.[43],Yang", "metadata": {"id": "42ebdb3a220a8ca8f36344aa2bff99fbd80f15f3", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 4, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "2) Long Video Retrieval and Understanding: Long video Chen et al. [44]) employs direct indexing and retrieval from\nunderstanding poses significant challenges, necessitating in- video databases without additional processing modules.\nnovative solutions across multiple technical dimensions. A\n(b) LLM-Augmented Pipeline (e.g., MM-REACT [34],\nkey area of advancement is real-time processing and stream-\nVideoRAG [20]) integrates large language models (LLMs)\ning capabilities, with various approaches addressing different\nas generators to enhance retrieval outputs through semantic", "metadata": {"id": "f3a56bb553f6d42b440721b038dcfd3c7f3a5b73", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 4, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "as generators to enhance retrieval outputs through semantic\nfacets of this problem. StreamingRAG [27] pioneered real-\nunderstanding.\ntime contextual retrieval, while Vinci [39] demonstrated its\n(c) Chunking-Based Pipeline (e.g., ViTA [22], Haomiao\npractical utility in embodied assistance. Xiong et al. [40] fur-\nXiong et al. [40]) introduces video chunking and sub-clip\ntheradvancedthefieldbyenhancingmemory-drivenstreaming\nprocessing before retrieval, enabling fine-grained analysis of\ncomprehension. Despite these advancements, maintaining an\nsegmented content.", "metadata": {"id": "7588a66fdb8c7a3d9cf8fa5da689d9d8fb010d82", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 4, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "comprehension. Despite these advancements, maintaining an\nsegmented content.\noptimalbalancebetweenprocessingspeedandcomprehension\n(d) Multi-Stage Refinement Pipeline (e.g., FlashRAG [35],\naccuracy remains a critical challenge. To address specific\nViPCap [38]) incorporates rerankers and refiners to iteratively\naspects of long video understanding, specialized optimization\noptimize retrieval results through post-processing stages.\ntechniques have been proposed. Video-RAG [41] introduced", "metadata": {"id": "7bfc2c4f8ae15d0428da3d8ea157c65b7c19ec23", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 4, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "techniques have been proposed. Video-RAG [41] introduced\nvisually-aligned retrieval augmentation to enhance represen- (e) Tool-Enhanced Pipeline (e.g., Xinmiao Yu et al. [45],\ntation learning, while iRAG [42] developed incremental pro- Video-RAG[41])combineschunkingwithspecializedanalysis\ncessingtechniquestoimprovecomputationalefficiency.These tools and multi-step LLM processing, enabling sophisticated\nmethodscontributetoadvancinglongvideoanalysis;however, multimodal reasoning.\nchallengesrelatedtomaintainingconsistencyoverextendedse- These pipelines demonstrate evolving methodologies in", "metadata": {"id": "d0d08b7c6df9edc739037a4394195af8a98b563a", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 4, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "quences and handling complex temporal dependencies persist. video RAG systems, ranging from basic retrieval to com-\nAs illustrated in Figure 3, five distinct RAG pipelines for plex multimodal architectures leveraging LLMs and domain-\nvideo retrieval and understanding are categorized based on specific tools.", "metadata": {"id": "a28f8e607cb562ee082c520c49f5ff1667984ba3", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 4, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "5\nTABLE III: Comparison of multimodal Understanding Benchmarks. This table presents a comprehensive overview of various\nbenchmarkscategorizedbytheirprimaryevaluationfocus.Thescaleinformationisapproximateandbasedonreportednumbers\nin the papers. Some benchmarks may use multiple metrics or combined datasets.\nCategory Benchmark Year Scale KeyFeatures EvaluationFocus\nVQAv2[55] 2017 250Kimages,1.1MQApairs Balancedvisual-linguisticunderstanding Open-endedanswergeneration\nVizwiz-VQA[56] 2018 20,523questionpairs Queriesforvisuallyimpairedusers Robustnessandreal-worldadaptability", "metadata": {"id": "014772919ef9edb3d9c1ecd9e3f08da2a2697d1d", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 5, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "OK-VQA[57] 2019 14kQApairs Externalknowledge-basedVQA Knowledgeretrievalandreasoning\nTraditional A-OKVQA[58] 2022 25kQApairs ExtendedOK-VQA Complexreasoning\nVQA ScienceQA[59] 2022 21kmultiple-choicequeries 3Majordomains,379skills InterdisciplinaryIntegratedReasoning\nOVEN[60] 2023 selectamong6MWikipediaentities knowledge-intensiveQA Deepintegrationofvisualsemantics\nInfoSeek[61] 2023 8.9Khuman-writtenIQA Information-seekingVQA Fine-grainedvisualattributes\nM3DocVQA[62] 2024 2,441multi-hopqueries Multi-page,multi-documentretrieval Open-domainmulti-hopQA", "metadata": {"id": "e7bb83190b7c9438d89c71a63214107e0b95e452", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 5, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "M3DocVQA[62] 2024 2,441multi-hopqueries Multi-page,multi-documentretrieval Open-domainmulti-hopQA\nTraditional ViDoRe[63] 2024 Widecoverageofdomains Documentvisualcontextretrieval Visualelementsensitivity\nDoc-VQA MMDocIR[64] 2025 1,685QApairs,313documents Long-documentsupport Long-contextunderstanding\nMS-COCO[65] 2014 330Kimages(>200Klabeled) Cross-task Imagecaptioningquality\nTraditional LVIS[66] 2019 164kimages,1,203categories Zero-shotrecognition Rarecategoryrecognitionprecision", "metadata": {"id": "c1388eb19b04648e38989e88de8a41d2dabbde6f", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 5, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "FundamentalVisionTasks V3Det[67] 2023 13,204categories Ultra-fine-grainedobjectdetection Large-scalecategorygeneralization\nELEVATER[68] 2022 20imageclassificationdatasets Unifiedevaluationtoolkit Cross-taskgeneralization\nMMBench[69] 2023 2,974multiple-choicequeries 20Abilitydimensions Multi-dimensionalcapability\nGeneral\nSEED-Bench[70] 2023 19kmultiple-choicequeries 12Evaluationdimensions Dynamicsceneunderstanding\nMMStar[71] 2024 1,500samples High-qualityreview Comprehensivecognitivecapability\nVisual-RAG[72] 2024 400Qs,103824images Text-imagejointretrieval Qualityandmultimodalalignment", "metadata": {"id": "cfe50aeba14e6c50af0ab5dd26c41138de268d3b", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 5, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "Visual-RAG[72] 2024 400Qs,103824images Text-imagejointretrieval Qualityandmultimodalalignment\nTechnical MRAG-Bench[73] 2024 16,130imagesand1,353queries Visual-centeredretrievalenhancement Visualsemanticretrievaleffectiveness\nRAG REAL-MM-RAG[74] 2025 8000pagesand5000queries Real-worldmultimodalretrieval Multimodalretrieval\nTechnical Pope[75] 2023 AdaptionfromMSCOCO Detectionobjecthallucination Hallucinationsuppression\nHallucination RAG-Check[76] 2025 121,000samples Multi-sourceintegration Relevancyscore\n3) Other Retrieval and Application Tasks: The application C. Multimodal Understanding", "metadata": {"id": "2b98d9fc0856a5b8f9dc60164c1c05709f85979e", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 5, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "3) Other Retrieval and Application Tasks: The application C. Multimodal Understanding\nof retrieval-augmented approaches has expanded into diverse\nThough the application of RAG in the NLP domain has\ndomains, each presenting distinct challenges and opportuni-\nbeen extensively explored in various downstream tasks [2],\nties. This section examines key advancements across various\nits adaptation within the multimodal learning domain remains\napplication areas, highlighting technical innovations and per-\nin its early stages. Pioneering works such as MuRAG [77]", "metadata": {"id": "86374318565ed3b2fed6183b1a9220a793074517", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 5, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "in its early stages. Pioneering works such as MuRAG [77]\nsistent challenges. Retrieval-augmented methods have played\nandREACT[78]havehighlightedthepotentialofmultimodal\na crucial role in enhancing storytelling and visualization.\nretrieval and reasoning mechanisms in enhancing the infer-\nAnimate-A-Story [46] demonstrated the potential of retrieval-\nential capabilities of vision-language models. As shown in\naugmentedvideogenerationforproducingcoherentnarratives,\nFigure 5, the applications of RAG in multimodal settings can\nwhile Dialogue Director [47] facilitated the transformation", "metadata": {"id": "6fd50c0b658fa9956d4b915a2a9abb0793ae16b5", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 5, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "while Dialogue Director [47] facilitated the transformation\nbe primarily categorized into three major directions according\nof dialogue-centric scripts into visual storyboards. Despite\nto the downstream tasks: visual question answering, docu-\nthese advancements, maintaining narrative consistency and\nment understanding, medical visual question answering, and\nvisualcoherenceacrossgeneratedcontentremainsasignificant\ngeneral-purposemulti-taskintegrationwithclassificationasan\nchallenge. Ensuring the reliability and security of video un-\nexample. The overall architecture of a multimodal retriever is", "metadata": {"id": "b09fa85c15c78cadd232940e6fe12c595361dd64", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 5, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "example. The overall architecture of a multimodal retriever is\nderstanding systems has become increasingly important. Wen\nillustrated in Figure 6, which provides a comprehensive view\net al. [48] introduced ensemble-based approaches for short-\nof the retrieval pipeline.\nform video quality assessment using multimodal LLMs. Con-\ncurrently, Fang et al. [49] uncovered security vulnerabilities 1) Benchmark Datasets: In multimodal retrieval-\naugmented understanding (MMRAU), benchmark datasets\nin retrieval-augmented diffusion models through contrastive", "metadata": {"id": "887776041d9fdd6e84cd497193292241207c365d", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 5, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "in retrieval-augmented diffusion models through contrastive\nare essential for evaluating model performance, advancing\nbackdoor attacks, emphasizing the necessity of robust secu-\ntechnology, and standardizing research. As shown in\nrity measures in retrieval-augmented systems. The versatility\nTable III, we categorize these datasets into three types:\nof retrieval-augmented approaches has been demonstrated in\ntraditional evaluation datasets, general-purpose benchmarks,\nvarious specialized domains. Hong et al. [50] advanced free-\nand technology-focused datasets. This classification captures", "metadata": {"id": "5c8b9f5d1b7488de20456e9812c04836869db74d", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 5, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "and technology-focused datasets. This classification captures\nviewpoint human animation through pose-correlated reference\nthe diverse requirements of various applications and research\nselection, while Luo et al. [51] tackled cross-domain person\ndirections, highlighting differences in scale, characteristics,\nretrieval using graph-based knowledge distillation. Video mo-\nand evaluation priorities. Traditional datasets can be\nment retrieval has seen significant improvements, with Xu et\ncategorized into Visual Question Answering (VQA),\nal.proposing zero-shot retrieval [52] and multimodal fusion", "metadata": {"id": "ea74065266d6ad13bf7c82af4e99e4969dd8110a", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 5, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "al.proposing zero-shot retrieval [52] and multimodal fusion\nDocument VQA (DocVQA), and fundamental vision tasks.\ntechniques [53]. Additionally, Chen et al. [54] contributed to\nVQAdatasetsemphasizeimage-textinteraction.VQAv2[55]\ndataset expansion through human-guided image generation.\nevaluates open-ended answer generation, while Vizwiz-\nVQA [56] targets visually impaired users. OK-VQA [57]\nand A-OKVQA [58] introduce external knowledge, and\ndatasets like ScienceQA [59] and OVEN [60] demand cross-", "metadata": {"id": "d95bc2780be5f40213cea82ed62ab6b21922e53c", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 5, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "6\n(a) grounding all modalities into a single unified modality\nText\nMulti-modal Encoder\nImage LLM Text Summary Images\nText Retrieval\nEmbedding\nText\nSingle\nEncoder\nQuestion Modal\nText Embedding Database Texts\n(b) embedding all modalities into a shared vector space\nImage Multi-modal Images\nModel Image Embedding Retrieval\nEncoder\nMulti-modal\nDatabase\nQuestion\nTexts\nText Embedding\n(c) maintaining separate databases for each modality\nImage\nEncoder Retrieval\nImage Images\nImage Embedding Single Modal Database\nText\nEncoder Retrieval\nQuestion\nTexts\nText Embedding Single Modal Database", "metadata": {"id": "b4bd7e81e13ace04b8ebbbe819c233981e2cf6a3", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 6, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "Text\nEncoder Retrieval\nQuestion\nTexts\nText Embedding Single Modal Database\nFig. 4: Three different multimodal RAG pipelines. (a) grounding all modalities into a single unified modality; (b) embedding\nall modalities into a shared vector space; (c) maintaining separate databases for each modality.\ndisciplinary reasoning and deep visual-semantic integration. RAG datasets focus on retrieval-augmented generation tasks.\nThese serve as comprehensive multimodal QA benchmarks. Visual-RAG [72] centers on joint text-image retrieval, empha-", "metadata": {"id": "728e84570d1e88b5b54a790fc826591129c69013", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 6, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "DocVQA datasets focus on visual-textual integration in sizing retrieval quality and multimodal alignment. MRAG-\ndocuments. M3DocVQA [62] supports multi-page, multi- Bench [73] shifts the focus toward vision-centric retrieval\ndocument retrieval, ViDoRe [63] emphasizes document enhancement. REAL-MM-RAG [74] extends to real-world\ncontext retrieval, and MMDocIR [64] addresses long- multimodal retrieval tasks. Hallucination datasets aim to eval-\ndocument comprehension—filling gaps in traditional VQA. uate a model’s ability to suppress hallucinations during the", "metadata": {"id": "5a153c28a26eddd1f3cb34c88fd359f7909f3121", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 6, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "Fundamentalvisiontaskdatasetssupportcorecomputervision generation process. Pope [75], adapted from MS-COCO [79],\nbenchmarks. MS-COCO [79] is used for image captioning, is specifically designed for detecting object hallucinations.\nLVIS [66] for zero-shot recognition, and V3Det [67] for RAG-Check [76] further expands to multi-source information\nultra-fine-grained object detection. While all focus on images, integration, focusing on relevance scoring.\ntheyvaryintaskcomplexity,classbalance,andgeneralization.\n2) Visual Question Answering (VQA): The integration of", "metadata": {"id": "02f70503d74735b4ec193ccdafc46539c86d4fe3", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 6, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "2) Visual Question Answering (VQA): The integration of\ntextual and visual information has become a key focus in en-\nThe second category consists of general-purpose datasets\nhancing model performance for the visual question answering\ndesigned to assess a model’s comprehensive multimodal ca-\n(VQA) task. However, many existing methods predominantly\npabilities across various tasks. ELEVATER [68] provides a\nemphasizetheknowledgeembeddedinthelanguagemodality,\nunifiedevaluationframework,focusingonimageclassification\noften overlooking the rich and valuable information provided", "metadata": {"id": "c1f13a1b8ce4cfd0493f9eb403bb26287f957188", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 6, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "often overlooking the rich and valuable information provided\nand object detection, while MMBench [69] introduces a fine-\nby other modalities, such as images [80].\ngrained capability-based assessment. SEED-Bench [70] and\nMMStar [71] set higher benchmarks in dynamic scene under- To address this limitation, leveraging multimodal data from\nstanding and integrated cognitive abilities. However, current comprehensivemultimodaldatabasespresentsapromisingso-\ngeneral-purpose datasets face challenges such as insufficient lution. The MuRAG framework [77] was the first to construct", "metadata": {"id": "18563ac4ce56c6b6bead5442ff2081e5916ae695", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 6, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "evaluation dimensions and imbalanced task coverage. Future such a database, combining retrieval and language augmen-\nimprovements are necessary to better align with the diverse tation techniques to generate high-quality outputs. This ap-\nrequirements. The final category encompasses datasets with a proach,whichintegratesmultimodalretrieval,iscategorizedas\nstrongemphasisonspecifictechnologicalaspects.Wedivideit pipeline(b),asillustratedinFigure4.Similarly,UDKAG[81]\nintoRAG-relateddatasetsandhallucinationdetectiondatasets. adopts a similar methodology but places a stronger emphasis", "metadata": {"id": "4578d5c4864b714654c0ce152748a73716f6b49f", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 6, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "7\n(a) Visual Question Answering (b) Document Visual Question Answering\nInput Retrivial knowledge Answer Input Retrivial knowledge Answer\nText: Where is this TheGreatWallofChinais This image depicts a\nplaceinChina? an architectural feat that famous watchtower Text: What economic Addressing the econo-\nImage: s a t c r r e o t s c s he n s or o th v e e r r n 1 C 3, h 0 i 0 n 0 a.. m ... iles c T a o l w le e d r(\" th Ji e ang G ju e n n L e o r u a \" l s h t e ra a t l e th g c ie a s r c e a s n y stems m di i s c eas im es p i a n c v t ol o v f es c : hronic", "metadata": {"id": "0e04849412a0d4fae387e11ff86eab6c36509c80", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 7, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "in Chinese), which is adopt to manage the Prevention:Investingin\nlocated at the Pan- financial impact of health education and\nlongshan section of chronic diseases? preventivemeasures\nthe Gubeikou Great … ……\nWallinChina.\n(c)MedicalVisual Question Answering (d)Classification\nInput Retrivial knowledge Answer Input Retrivial knowledge Answer\nText:Is there any focal Airspace disease can be Image: Angorarabbitsareatype\nairspace consolidation acute or chronic and com- oflong-hairedrabbitknown A brown hair\non the patient's X-ray? monlypresentasconsolida- fortheirsoft,fluffycoats. Angorarabbit.", "metadata": {"id": "253e2fd4541798c4870046d9d4af147859a0cefb", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 7, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "on the patient's X-ray? monlypresentasconsolida- fortheirsoft,fluffycoats. Angorarabbit.\nImage: tionorground-glassopacity No,itshowsnofocal TheyoriginatedinTurkey\n…… airspaceconsolidation. .....\nRetriever Generator\n… …\nFig. 5: Frameworks for different multimodal RAG application.\non the recency of knowledge by incorporating web-retrieved question answering (VQA) that focuses on answering textual\ndata during inference. queriesbyinterpretingtheinformationcontainedwithindocu-\nIn contrast, pipeline (a) in Figure 4 generates image repre- ments) [85]. Currently, the majority of DocVQA applications", "metadata": {"id": "97d5666f950c27799829779d3fc8bc077c180b97", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 7, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "sentations through image-to-text conversion, encoding visual arebasedonmultimodallargelanguagemodels(MLLMs)[88]\ncontent into descriptive text before further processing. While by splitting documents into individual page images, corre-\nthis method can capture certain aspects of the image, it sponding to pipeline (b) in Figure 4. While this architecture\noften fails to preserve the full richness and complexity of offers simplicity, it imposes high demands on the model [62],\nvisual information. This shortcoming arises from the inherent requiring an embedding system capable of accurate different", "metadata": {"id": "b18a35b4feeeb937b0079f67060514b0b9466c2a", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 7, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "limitationsofconvertingimagesintotext,wherecrucialvisual modalitiesintegration.Thismethodalsofacesdifficultyinhan-\ndetails can be lost or inaccurately represented. dling long, multi-page documents, particularly in maintaining\nthe relationships between pages.\nTo overcome these challenges, FLMR [82] introduces an\nadvanced approach that combines late interaction with multi- Methods in industrial scenarios often focus on OCR-based\ndimensional representations to more effectively capture fine- RAG [86], which first summarizes the image content into a", "metadata": {"id": "9caaeb471579bade3ae71e4942cc75f6f5dcc7d6", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 7, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "grained, cross-modal relevance between text and images. Un- text summary, corresponding to pipeline (a) in Figure 4. This\nlike image-to-text conversion, FLMR fosters a more nuanced approach relies on the direct retrieval and generation of text\ninteraction between the modalities, preserving intricate visual blocks, which effectively improves the quality of document\nfeatures that are essential for accurate reasoning. By utiliz- understanding. However, it faces the potential loss of critical", "metadata": {"id": "1f2bb049dbf87b042416ed6c6ef2738883e7e802", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 7, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "ing multi-dimensional representations, FLMR enhances the visual information in conversion, such as graphics.\nmodel’s capacity to capture deeper, more detailed information To address these challenges, ColPali [63] utilizes vision-\nfrom both image and text, thereby improving cross-modal language models to generate high-quality contextualized em-\nunderstandingandperformanceintasksrequiringsophisticated beddingsfromdocumentpages,significantlyenhancingperfor-\nintegration of visual and textual data. This method offers a mance in visually rich document retrieval. Unlike traditional", "metadata": {"id": "da6c796a2e850d8f8a067f19ac55267fd930c5ac", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 7, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "compelling alternative to traditional image-to-text approaches, OCRmethods,thisapproachfocusesonimprovingmultimodal\naddressing the issue of incomplete image representations and retrieval and falls under pipeline (b) in Figure 4. However,\nimproving the quality of multimodal interactions. it primarily addresses innovations in the retrieval component,\nRecent advancements in multimodal retrieval-augmented without emphasizing the generation aspect.\ngeneration (RAG) for VQA, such as RMR [83] and Building on ColPali [63], VisRAG [13] and", "metadata": {"id": "724683f8d7c766804057bd590e20f57a7f911d53", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 7, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "generation (RAG) for VQA, such as RMR [83] and Building on ColPali [63], VisRAG [13] and\nRagLLaVA [84], have further refined this approach. These M3DocRAG [62] integrate the generation module and\nmethodsarebasedonpipeline(c)inFigure4,withRMR[83] perform end-to-end RAG evaluations. M3DocRAG [62], in\nenhancing the model’s reasoning capabilities by introducing particular, emphasizes handling multi-page, multi-document\nin-context learning (ICL) within the multimodal RAG frame- scenarios.Similarly,BeyondText[87]explorestheapplication", "metadata": {"id": "8a1babc4ab0211ca32f0d4adce585e5990486c74", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 7, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "work.Meanwhile,RagLLaVA[84]improvesmodelrobustness of multimodal RAG in industrial environments, highlighting\nby incorporating knowledge-enhanced re-ranking and noise the promising future prospects of this approach.\ninjection during training. 4) General-purpose Multi-task Integration: Most multi-\n3) Document Understanding: Document visual question modal RAG methods are designed for single tasks, such\nanswering (DocVQA) is a specialized application of visual as VQA [84] or DocVQA [77]. Due to the high cost of", "metadata": {"id": "d06ed561310465266ce7163ffc85b76b60a1c6f1", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 7, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "8\nTABLE IV: Summary of Document Understanding Methods with Core Techniques and Retrieval Models\nMethod Document Processing Methods Retrieval Methods / Models\nDocVQA [85] Document page segmentation Text-based retriever with MLLMs\nOCR-based RAG [86] OCR-based text summarization OCR and text-based retriever\nColPali [63] Embedding images of documents VLM-based retriever\nVisRAG [13] Treating documents as image VLM-based retriever\nM3DocRAG [62] Multi-page, multi-document RAG Multimodal retriever\nBeyond Text [87] Industrial RAG applications Multimodal retriever", "metadata": {"id": "08b19585a2a47130d493c9d3a231630f5c82cbff", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 8, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "Beyond Text [87] Industrial RAG applications Multimodal retriever\npretrainingandextraparameteroverhead[89],multi-taskRAG\nQuery Input Multimodal Retriever Related Sources\nfor vision-language models (VLMs) remains underexplored.\nCurrent multi-task setups primarily adopt pipeline (b) in\nFigure 4, with few using pipeline (c). Pipeline (b) benefits\nfrom a unified vector space, enabling efficient cross-modal Image Video Images Videos\ncomparison and reducing integration complexity. In contrast,\npipeline (c) handles heterogeneous data types, increasing\ndatabase management complexity.\nQuestion Texts", "metadata": {"id": "36b4d4bd9e70a1e198f884fe028ca339b0a5ec91", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 8, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "database management complexity.\nQuestion Texts\nReVeaL [90] first introduces a knowledge-aware self-\nsupervised learning approach, effectively integrating world\nknowledge into the model. Similarly, REAVL [91] explores\ntheroleofretrievingworldknowledgefromknowledgegraphs.\nThose methods enhance the multimodal learning capabilities\nMultimodal Prompt Multimodal LLM Result\nand benefiting tasks such as VQA and image captioning.\nFor classic downstream tasks in computer vision, such w/wo\nas classification, detection, and segmentation, REACT [78]", "metadata": {"id": "9d5f53824e426db78a9eb375409f6d6cf648e8cd", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 8, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "as classification, detection, and segmentation, REACT [78]\npioneered task-specific model enhancement by training new Visual Prompt GPT-4V Gimini …… Image\nblocks without altering original weights. RAVEN [92], on\nthe other hand, utilizes a multi-task learning framework, Qwen2-VLInternVL\nText Prompt Answer\nsimultaneously handling multiple tasks. Frameworks such as\nSURf [93] and RoRA-VLM [94] complement this by selec- Fig. 6: The general framework of multimodal RAG tasks.\ntively utilizing retrieved information to enhance the model’s\nrobustness against irrelevant or misleading data. Similarly,", "metadata": {"id": "f3f7248a6e629ba2af7f56a722118557534c72bb", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 8, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "robustness against irrelevant or misleading data. Similarly,\nRAR[18]optimizesperformancethroughretrievalandranking a domain-aware retrieval mechanism to selectively extract\nwith MLLM for fine-grained knowledge tasks. relevantmedicalknowledgefromastructuredknowledgebase,\n5) RAGinMedicalDomain: Medicallargevision-language ensuring the generated responses are both accurate and con-\nmodels (Med-LVLMs) have shown significant promise in textually relevant.\nadvancing interactive and intelligent diagnosis [95, 96]. These Further, the process of retrieval can be augmented by", "metadata": {"id": "0747a2bce914ebccfacc3fa0b25b5a25b0641ffd", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 8, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "models integrate medical knowledge from visual inputs (such advanced mechanisms that refine the selection of context,\nas images, videos, and scans) and textual data, enabling them RULE[96]addressesthechallengesofdirectlyapplyingRAG\ntounderstandcomplexmedicalcontextsandassistindecision- toMed-LVLMs,suchaslimitationsinthenumberofretrieved\nmaking processes. However, despite their potential, current contexts and the potential over-reliance on external sources.\nMed-LVLMs still face significant challenges, particularly with Byintroducingamoreadaptiveretrievalandcontextselection", "metadata": {"id": "36d28aa3fedbf758e8253e56a493ef26949f5477", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 8, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "hallucinations—generating non-factual or misleading medi- process, these systems aim to balance external information\ncal responses—which undermines their reliability in critical with the internal reasoning of the model, ensuring a more\nhealthcare applications. accurate and reliable output in clinical environments.\nTo address these limitations, RAG has emerged as a viable\nsolution.Byintegratingexternalknowledgesourcesduringthe\nIII. Retrieval-augmentedGenerationinVision\ninference process, RAG enhances the accuracy and reliability", "metadata": {"id": "5b875164476248d1a8b62b2e6dda2d903069e051", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 8, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "inference process, RAG enhances the accuracy and reliability\nof Med-LVLMs. This approach works by retrieving relevant, A. Image Generation\ndomain-specific knowledge from external databases, such as Table V presents an overview of retrieval-augmented im-\nmedical literature or image repositories, and incorporating it age generation methods, categorized based on their retrieval\nintothemodel’sdecision-making.MMED-RAG[95]proposes strategies and challenges addressed. As shown in Figure 8,", "metadata": {"id": "839b0d5c5a0e59ea3bc21fa39b0afd8b788c89c5", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 8, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "a versatile multimodal RAG system specifically designed to these methods can be broadly classified into three categories\nimprove the factuality of Med-LVLMs. This system integrates basedonthedatamodalityusedintheirretrievaldatabase:(1)", "metadata": {"id": "aa1f528cfcbc4889ec666d4486143f20d443ba33", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 8, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "9\nFig. 7: The visual results of unseen novel object generation from FLUX and RAG-based RealRAG [97].\nTABLE V: Image Generation Methods with RAG techniques.\nYear Method CoreTechniques RetrievalModel ChallengesAddressed\n2022 RA-Diffusion[98] Semi-parametricgenerativemodel Nearestneighborlookup Efficientgenerationwithfewerpa-\nrameters\n2023 Cioni[99] Domain-specificdatabaseaugmen- Zero-shotretrievalwithCLIPpre- Inaccurate generation in special-\ntation(e.g.,artworks) trainedonYFCC izeddomains,e.g.,artwork", "metadata": {"id": "dd4030229c2d095bbee9dbf2590305184f378116", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 9, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "tation(e.g.,artworks) trainedonYFCC izeddomains,e.g.,artwork\n2023 ReMoDiffuse[100] Diffusion-based motion generation Hybridretrievalbasedonsemantic Improvesmotiongenerationdiver-\nwithretrieval /kinematicsimilarities sityandgeneralization\n2024 iRAG[101] - Image retrieval integrated with Solving \"factuality hallucination\"\ngeneration problem\n2024 FAI[102] Fact-AugmentedIntervention(FAI) GPT4+ExpertQA[103] Nonfactual demographic genera-\nusingLLMstoincorporatefactual tion, gender / racial composition\ndata issues", "metadata": {"id": "4af4ff0bb3cf7052bec184dd3717d1298d74602e", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 9, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "usingLLMstoincorporatefactual tion, gender / racial composition\ndata issues\n2025 ImageRAG[104] Dynamicimageretrievalwithtext GPT4+Cationthenusetext-image Reducing gap between generated\npromptsforcontextualguidance similaritymetricfortop-ksamples andreal-worldimages\n2025 RealRAG[97] Self-reflective contrastive learning Reflectiveretriever Enhances fine-grained object gen-\nforfine-grainedobjectgeneration eration\nText-based frameworks, (2) Vision-based frameworks, and (3) synthesis,resultinginremarkablevisualqualityacrossvarious", "metadata": {"id": "2a711e261854f256bd2af465af6d7d08afc0204a", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 9, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "Multimodal frameworks. tasks. This success is largely attributed to the scalability of\nthesearchitectures,whichinturnhasledtoadramaticincrease\n1) Text-basedframework: Prompt-basedtext-to-imagegen-\ninmodelcomplexityandthecomputationalresourcesrequired\neration relies on knowledge stored in the model’s training\nfor training. However, two main challenges persist: (1) As\nmemory to generate images. However, this approach often\ndemonstrated in Figure 8, training pre-trained models in real-\nresults in nonfactual demographic distributions, particularly", "metadata": {"id": "aea578dc374cfb1077a26f572913702c688da96f", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 9, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "results in nonfactual demographic distributions, particularly\ntime remains difficult due to constraints posed by environ-\nwhen generating images that require specialized knowledge\nmental factors, such as limited computational resources; and\n(e.g., historical images [102] and cultural images [99]). To\n(2) the generated objects often appear too virtual, exhibiting a\naddress these issues, [102] propose the Fact-Augmented Inter-\nnoticeablegapbetweenthemandreal-worldobjects(Figure8).\nvention(FAI),amethodthatinstructsaLargeLanguageModel\n(LLM)toreflectonverbalizedorretrievedfactualinformation", "metadata": {"id": "2c0bd92c63bbba08682928f504ce36c436f1c40c", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 9, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "(LLM)toreflectonverbalizedorretrievedfactualinformation\nToaddressthefirstchallenge,RA-Diffusion[98]introduces\nregarding the gender and racial compositions of subjects in\na retrieval-augmented framework designed to create smaller\nhistorical contexts, and to incorporate this information into\ngenerative models with access to a large image database.\nthegenerationprocessofT2Imodels.Additionally,[99]intro-\nDuringtraining,theresultingsemi-parametricgenerativemod-\nduces a specialized domain-specific database (e.g., artworks)\nels leverage this database through nearest neighbor lookups,", "metadata": {"id": "1bd7b5f0007bf3db5592f2697535f4327dc461f6", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 9, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "els leverage this database through nearest neighbor lookups,\nto augment the generation process.\neliminating the need to generate data \"from scratch.\" Instead,\n2) Vision-based framework: Recent advancements in novel they learn to compose new scenes based on retrieved visual\narchitectures have significantly enhanced generative image instances. This approach not only improves generative perfor-", "metadata": {"id": "82ac022d1dfba659d6490da289b8a4b1cf2d3c76", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 9, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "10\n3) Multimodal Framework: As illustrated in Figure 8,\nmultimodal frameworks [100, 108] leverage diverse data\nInputs Image Modal Generator sourcesfrommultimodaldatabases(e.g.,MSCOCO[79],We-\nDatabase bQA[109])toimprovevisualgeneration.ReMoDiffuse[100],\nTexts Condition\nRetrieve a diffusion-based motion generation model, incorporates re-\n(a)\ntrieval to refine denoising. It introduces three components:\n(1) Hybrid Retrieval, selecting database references via seman-\nInputs Image Modal Generator tic and kinematic similarity; (2) Semantic-Modulated Trans-\nDatabase", "metadata": {"id": "78263e20069d78749b561c5d7b5517b4f0bed7b1", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 10, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "Inputs Image Modal Generator tic and kinematic similarity; (2) Semantic-Modulated Trans-\nDatabase\nImages former, aligning retrieved information with target motion;\nCondition\nRetrieve (3) Condition Mixture, optimizing database use during in-\n(b)\nference to mitigate classifier-free guidance sensitivity. These\nframeworks enhance generative models by fusing multimodal\nImage Modal features, improving completeness and diversity.\nDatabase Texts\nInputs Retrieve Generator\nB. Video Generation\nCondition\nImage Modal Thefieldofgenerativevideosynthesishasadvancedsignif-", "metadata": {"id": "661ebc1c44ce86e8fb2ed1a0259ad06a9509e8dd", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 10, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "B. Video Generation\nCondition\nImage Modal Thefieldofgenerativevideosynthesishasadvancedsignif-\nDatabase Images icantly, with the integration of Retrieval-Augmented Genera-\nRetrieve tion (RAG) providing a promising solution to the challenges\n(c)\nof creating high-quality, coherent, and contextually accurate\nFig. 8: The overall of the retrieval-augmented image (video)\nvideos. Recent works, such as Animate-AS [46], explore the\ngenerationframeworks.(a)Text-basedframework;(b)Vision-\npotentialofleveragingpre-existingvideoassetsinconjunction\nbased framework; (c) Dual-branch multimodal framework", "metadata": {"id": "a3a5debdf07f61a38b67695243ad994b99f46404", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 10, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "based framework; (c) Dual-branch multimodal framework\nwith generative models to enhance storytelling and video\nproduction efficiency. This approach utilizes two core mod-\nules:MotionStructureRetrievalandStructure-GuidedText-to-\nVideo Synthesis. The first module retrieves video candidates\nmance with a reduced parameter count but also reduces com-\nbased on text prompts, focusing on the motion structure of\nputational requirements during training. RA-Diffusion further\nthe retrieved clips, while the second module synthesizes new\ndemonstratesthepotentialofgeneralizingtonewknowledgein", "metadata": {"id": "4df87b180cea6ccd52727b300e8d1487caa7d65e", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 10, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "demonstratesthepotentialofgeneralizingtonewknowledgein\nvideocontentundertheguidanceoftheseretrievedstructures.\nthe form of alternative image databases, without necessitating\nThis technique allows for the efficient generation of videos\nadditional training. Furthermore, ImageRAG [104] dynami-\nwith specific motions or layouts by reusing existing content\ncally retrieves relevant images based on a given text prompt\nandadjustingitsappearance,ratherthangeneratingeverything\nand incorporates them as contextual information to guide\nfrom scratch.", "metadata": {"id": "77a54de8e9c4e0f57fe08727956c31894f745fc0", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 10, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "and incorporates them as contextual information to guide\nfrom scratch.\nthe generation process. In contrast, previous methods that\nRAG-based video generation models represent a significant\nleveraged retrieved images for enhancing generation typically\nleap forward in generative video technology. By combining\nrequired models to be specially trained for retrieval-based\nthe advantages of retrieval systems and generative models,\ngeneration. Following these approaches, various works have\nRAG offers a powerful framework for producing high-quality,", "metadata": {"id": "41c07f5ac24310023daed961c52e598339e24b6a", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 10, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "RAG offers a powerful framework for producing high-quality,\nincorporated domain-specific image databases, such as gar-\npersonalized,andcontextuallyaccuratevideos.Withcontinued\nment[105],traffic[106],andlayout[107].Thesemethodspave\nadvancements in personalization, efficiency, and structural\nthe way for more efficient and sustainable generative mod-\nguidance, RAG methods hold immense potential for creative\nels for image generation. However, these retrieval-augmented\nand commercial applications in video generation.\nmethods still overlook the challenge of improving the realism", "metadata": {"id": "1e350e496586f850d94a417ba9b2aa5ea36c844b", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 10, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "methods still overlook the challenge of improving the realism\nof generated images (mentioned in challenge 2).\nAs the scale of image generation models continues to C. 3D Generation\nexpand, the scope of domains they can adapt to is also broad- Advancements in 2D diffusion models have significantly\nening. Consequently, the realism of the objects in generated influenced progress in 3D generation. However, two key\nimages has become an increasingly important consideration. challenges persist in this field. The first challenge concerns", "metadata": {"id": "1bb093638687422c1efe089809140505d19d4afe", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 10, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "iRAG [101] introduces the concept of \"factuality hallucina- data availability. Compared to large-scale 2D datasets, which\ntion\" in visual generation models and addresses the issue by contain billions of data pairs [113, 114], the largest 3D\naugmenting the realism of generated objects through image datasets [115, 116] contain only around 10 million objects.\nretrieval. iRAG highlights the limitations of large-scale image Moreover, a substantial portion of these objects is excluded", "metadata": {"id": "34e9a133d5d86a41c474389c7e994f77fb79698a", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 10, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "generation models and defines the \"factuality hallucination\" fromtrainingduetoqualityissues[112,117,118],limitingthe\nproblem. Furthermore, RealRAG [97] achieves significant performanceof3Dgeneratorsrelativetotheir2Dcounterparts.\nperformance improvement in fine-grained and unseen object The second challenge is generation quality. 3D generation\ngeneration, which is powered by the proposed self-reflective is often constrained to text or single-image inputs, making\ncontrastive learning. As shown in Figure 7, the RealRAG it an ill-posed problem—i.e., the available conditions are", "metadata": {"id": "61f235e2843a6c6776a53a0bd370b8a2bbee6f45", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 10, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "demonstratessignificantperformancegaininfine-grainedtext- insufficient to generate a fully accurate 3D model. This limi-\nto-image generation. tation restricts the realism and structural fidelity of generated", "metadata": {"id": "f8e62f68192f77550005187ecb82bc08ea116e49", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 10, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "11\na)Inference-based DirectGeneration\nInput 3D Database Multi-view 3D Model\nRetriever Generator Reconstruct\nA colorful or\neagle\nText Image\nb)Optimization-based Score Distillation from 2D Diffusion\nInput 3D Database 3D Model 2D Prior\nRetriever Initialize Optimize Generated 3D\nModel\nA colorful or\neagle\nDiffusion\nText Image Model Multi-view\nFig.9:Theoverallpipelineoftheretrieval-augmented3Dgenerationframeworks.SomeimagesareborrowedfromPhidias[19].\nGAR\ndrawrof-deeF\ntupnI\nsaidihP\nMRLnepO\nMGL\nhseMtnatsnI\nPhidias first retrieves a 3D reference model to render multi-", "metadata": {"id": "86391c73ed6d8e560a7370a4ed33ac76f1c64dde", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 11, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "tupnI\nsaidihP\nMRLnepO\nMGL\nhseMtnatsnI\nPhidias first retrieves a 3D reference model to render multi-\nview images, which are then used by a sparse-view-to-3D\ngenerator to reconstruct the 3D model. As demonstrated in\nFigure 10, the RAG-based method can generate more rea-\nsonable content compared with feed-forward methods [110–\n112]. Moreover, as illustrated in Figure 9 b), ReDream [119]\nutilizes a retriever to find the most similar 3D object in the\ndatabase, then optimizes this 3D object with a 2D diffusion\nprior. Additionally, IRDiff [120] has explored 3D molecular", "metadata": {"id": "91e91e13c8cf7de8a351a012f0a8cf95c47946ab", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 11, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "prior. Additionally, IRDiff [120] has explored 3D molecular\ngeneration by integrating the diffusion model with RAG tech-\nniques.ReMoDiffuse[121]proposestocombineRAGintothe\nfieldof3Dhumanmotiongeneration.Recently,Diorama[122]\nproposes to compose a scene by retrieving the components\nand placing them in the corresponding positions from some\nboundingboxes.However,itisnotfollowedbyanoptimization\nstage, making it hard to render a consistent image compared\nto the input image.\nIV. Retrieval-AugmentedGenerationinEmbodiedAI\nRetrieval-augmentedgeneration(RAG)enhancesEmbodied", "metadata": {"id": "930304a7dc0ca61111cc28ae5fa0464cf1d50be7", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 11, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "IV. Retrieval-AugmentedGenerationinEmbodiedAI\nRetrieval-augmentedgeneration(RAG)enhancesEmbodied\nAI by enabling agents to retrieve relevant external knowledge\nand generate informed actions or responses. This capability\nimproves decision-making, adaptability, and overall perfor-\nmance in dynamic environments. As embodied agents grow\nmore complex, integrating RAG strengthens task execution,\nFig. 10: The comparison between RAG-based method\nmultimodal perception, and domain-specific applications.\nPhidias [19] and Feed-forward methods, OpenLRM [110],\nLGM [111], and InstantMesh [112].", "metadata": {"id": "5c1f2c5dd95126de022a027044a1747d5e456274", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 11, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "Phidias [19] and Feed-forward methods, OpenLRM [110],\nLGM [111], and InstantMesh [112].\nA. RAG in Planning and Task Execution\nEfficient task planning and execution are essential for intel-\n3D content, underscoring the need for improved conditioning ligent agents operating in unpredictable environments. RAG\nmechanisms. enables agents to retrieve relevant knowledge in real time,\nFor 3D object generation, there are two mainstream improving adaptability and performance. For instance, [123]", "metadata": {"id": "dda2a419962706d96d37a6340f88d34cbea7f02d", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 11, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "paradigms that can be used to address these limitations. As introduces a progressive retrieval model where agents contin-\nshown in Figure 9 a), Phidias [19] makes the early attempt uously access external information to refine task execution.\nand proposes a novel approach that utilizes a 3D database The system updates a database containing goal instructions,\nand leverages an RAG mechanism to enhance 3D generation. scene graphs, trajectory history, and task completion status", "metadata": {"id": "9bb7a3689cc9371e86ceb7b773be2a59464a72b1", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 11, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "12\naftereachinteractioninenvironmentslikeMINI-BEHAVIOR. understanding by integrating external knowledge to enhance\nThis iterative learning process enhances task efficiency and contextual comprehension. However, current methods face\nadaptability. Similarly, [124] explores how RAG improves persistent challenges in balancing computational efficiency\ndecision-making by retrieving external knowledge during task withtemporalreasoning.Whileretrievalmechanismshaveim-\nexecution. This augmentation helps agents handle complex provedcontextualgrounding,theystillstrugglewithefficiently", "metadata": {"id": "abbf92bb05e58776118fceef5b78e8207d2d5ea3", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 12, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "tasks and respond to unforeseen circumstances more effec- processing long videos and maintaining coherence across\ntively.ThesestudiesdemonstratehowRAGenhancesplanning extended sequences. Recent innovations such as arbitrary-\nand execution, enabling agents to dynamically adjust to new length video processing and token reduction techniques have\ncontexts, a crucial capability for real-world applications. improved efficiency, but challenges remain in areas like mem-\nory constraints and long-range dependency modeling. Addi-", "metadata": {"id": "09b00eac8e53fc5461cb7dfebe517c78e7d7e63a", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 12, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "ory constraints and long-range dependency modeling. Addi-\nB. RAG in MultiModal Perception and Interaction tionally, maintaining narrative consistency, visual coherence,\nand security in retrieval-based video applications remains an\nEmbodied AI agents must process diverse sensory in-\nopen challenge.\nputs—visual, auditory, and tactile—posing a challenge for\nThe future directions could be: (1) Dynamic Resource\neffective perception and interaction. RAG facilitates the in-\nAllocation:Futuresystemsshouldoptimizeretrievalefficiency\ntegration of multimodal data, improving agents’ ability to", "metadata": {"id": "e8a53ba31d053cf3e6005d49f7e870f8cb6f3c48", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 12, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "tegration of multimodal data, improving agents’ ability to\nby prioritizing critical video segments without compromis-\nunderstand and respond to their environment. For example,\ning comprehension. and (2) Enhanced Memory Management:\n[125] presents a framework that combines multimodal data,\nImproving memory efficiency in retrieval-augmented video\nincluding wireless sensor inputs, with RAG to enhance situ-\nmodels is essential for processing long sequences while main-\national awareness. By retrieving relevant contextual informa-\ntaining semantic coherence.", "metadata": {"id": "f659cc2e0ecea5fc65e517ba7b9e29c93e25d5f6", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 12, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "ational awareness. By retrieving relevant contextual informa-\ntaining semantic coherence.\ntion, agents improve their ability to interpret complex envi-\nronments and make informed decisions. Additionally, [126] 2) RAU in Multimodal Learning: Incorporating external\nknowledge into vision-language models has proven effective\nproposesamethodusingnon-parametricmemorytostoreand\nfor multimodal tasks. However, large-scale vision-language\nretrieve sensory data, enhancing adaptability. By integrating", "metadata": {"id": "9874bb2153ea625059d1ea811687ad4360954dcf", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 12, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "retrieve sensory data, enhancing adaptability. By integrating\nRAG with memory systems, agents can better process past models, such as the LLaVA series, remain prone to hallu-\ncinations when encountering novel knowledge. Multimodal\nexperiences and interact more effectively in dynamic settings.\nThese advancements highlight RAG’s role in refining multi-\nRAG (mRAG) methods offer a partial solution by retrieving\nrelevant external information to improve factual alignment.\nmodal perception, enabling more natural and context-aware\ninteractions.", "metadata": {"id": "1e608d076dfa90d89d73a002be15a3a86d59f20f", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 12, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "modal perception, enabling more natural and context-aware\ninteractions.\nDespite progress in mRAG-based models, challenges persist\nin balancing modality interactions in multi-task settings and\navoiding over-reliance on text retrieval. Some models dispro-\nC. RAG in Specialized Domains\nportionatelyprioritizetextualinformationwhileunderutilizing\nRAG also offers significant benefits in specialized domains\nvisual inputs, leading to imbalances in cross-modal under-\nsuch as autonomous driving and traffic management, where\nstanding. Additionally, the overlap between training datasets", "metadata": {"id": "7546eedd7e564ac9f58ca0710064f1aa812ea620", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 12, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "standing. Additionally, the overlap between training datasets\nagents must make real-time, context-aware decisions. For in-\nand retrieved external data can impact both fairness and\nstance,[127]demonstrateshowRAGretrievesreal-timetraffic\nretrieval effectiveness.\ndata and historical patterns to optimize traffic flow, improving\nThe future directions could be:\nurban traffic management efficiency. Similarly, [128] applies\nOptimized Multimodal Sampling: Enhancing content selec-\nRAGtoautonomousdriving,enhancingexplainabilitythrough\ntion and integration across modalities can improve robustness", "metadata": {"id": "12fd0b2fcbfabc50c69ffa715c70656294c404c2", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 12, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "tion and integration across modalities can improve robustness\nretrieval-augmentedin-contextlearning.Bygeneratingnatural\nin multi-task settings.\nlanguage explanations for driving decisions based on contex-\nAdvanced Multimodal Fusion: Better balancing modality\ntual demonstrations, the system increases transparency and\ncontributions can prevent over-reliance on textual cues.\ntrustworthiness. Notably, the model generalizes well to new\nCross-Task Generalization:Developingretrieval-basedmod-\nenvironments, underscoring RAG’s effectiveness in real-world", "metadata": {"id": "5ef79b2110d023065b79983e8ea44fceab3f3e1b", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 12, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "environments, underscoring RAG’s effectiveness in real-world\nels that transfer knowledge across tasks will enhance broader\napplications. These examples illustrate how RAG enhances\napplicability.\ndomain-specific decision-making, providing adaptability and\nRAU continues to reshape pattern recognition, video analy-\ntransparencyinareasliketrafficoptimizationandautonomous\nsis,andmultimodallearningbyintegratingexternalknowledge\nsystems. Integrating RAG into Embodied AI significantly\nsourcestoenhancemodelperformance.Whilerecentadvance-\nenhances agents’ ability to execute tasks, perceive their envi-", "metadata": {"id": "ed66d778cbb09f9d696d9848088a866fa1ed6909", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 12, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "enhances agents’ ability to execute tasks, perceive their envi-\nments have addressed challenges related to factual grounding,\nronment, and interact naturally by leveraging external knowl-\nefficiency, and cross-modal interactions, future research must\nedge in real time. This improves adaptability and decision-\nfocus on developing adaptive, scalable, and secure retrieval\nmakingindynamicenvironments,pavingthewayforadvanced\nstrategies to further improve generalization and reliability in\napplicationsindomainssuchasautonomousdrivingandtraffic\nreal-world applications.\nmanagement.", "metadata": {"id": "b1ed27d848ed662dc3e35eabf04de9aa06de0117", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 12, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "applicationsindomainssuchasautonomousdrivingandtraffic\nreal-world applications.\nmanagement.\nV. InsightsandNewOutlook\nB. Insights and New Outlook for RAG\nA. Insights and New Outlook for RAU 1) RAG for Image Generation: By leveraging external\n1) RAU in Video Understanding: Retrieval-augmented databasestoretrieverelevantinformation,RAG-basedmethods\ntechniques have advanced multimodal video retrieval and address several longstanding challenges in generative models,", "metadata": {"id": "64f22a6c9e378c99c555137f1a8d3dc25cf82f86", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 12, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "13\nKeyInsights FutureDirections\nMitigatingFactualityHallucinationTraditionalgenerativemodelssufferfrom Real-Time and Dynamic Retrieval SystemsFuturesystemsshouldintegrate\nfactualinconsistencies.iRAG[101]ensuresfactualalignmentbyretrievingreal- real-time retrieval from live databases (e.g., Wikipedia, news archives) to\nworldimages. enhancefactualalignmentdynamically.\nEnhancingRealismandFine-GrainedDetailsRealRAG[97]improvesobject Bias-AwareandFairness-EnhancedRetrievalFuturemethodsshouldincor-", "metadata": {"id": "7b8e06acbc20b8f0f333144a9f44088d7f8b5d0e", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 13, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "generationwithfinedetails,producinghigh-fidelityimagesusingself-reflective poratebias-detectionpipelinestomitigatedisparitiesindemographicrepresen-\ncontrastivelearning. tation(e.g.,FAI [102]).\nReducing Computational Overhead Models like RA-Diffusion [98] reduce Hybrid Multi-Scale Retrieval for Improved Realism Combining low-level\ncomputationalresourcesbyretrievingimagesforefficientgeneration. textureretrievalwithhigh-levelsemanticretrievalcouldimprovephotorealism,\nespeciallyforcomplexscenes.", "metadata": {"id": "7a46b92820bbb063a225b725ac25886ae2aa2386", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 13, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "especiallyforcomplexscenes.\nLeveraging Multimodal Retrieval for Enriched GenerationRecentmodels Federated and Distributed Retrieval for Privacy-Preserving Generation\nlikeReMoDiffuse[100]integratemotionretrievalfordynamicgeneration. Futureworkshouldexploredecentralizedfederatedretrievalmodelstoprotect\nuserdataprivacywhilepullingfromdiversedatasets.\nTowardsAdaptiveandContext-AwareGenerationImageRAG[104]dynam- InteractiveandUser-AdaptiveRetrievalMechanismsFuturesystemsshould", "metadata": {"id": "ddaa1c75e01b3000d883d9fc2d681770ab524569", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 13, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "ically guides image generation based on contextual information, improving offerinteractiveretrievalinterfaces,allowinguserstomodifyparametersinreal\ndomainadaptability. timeforpersonalizedoutputs.\nTABLE VI: Insights and Future Directions for RAG-based Image Generation\nincluding factuality hallucination, domain adaptation, compu- C. RAG in Embodied AI\ntational efficiency, and realism in generated images.\nAs shown in Table VII, we list the insights and future\nWe summarize key insights from recent advancements and directions of RAG in embodied AI. RAG has significantly", "metadata": {"id": "6907d6db13f1f836fbb7660da196fa62be5f25ae", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 13, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "promising future directions in retrieval-augmented image gen- advanced Embodied AI by integrating external knowledge\neration in the table labeled Table VI. Despite recent progress, retrievalintodecision-making,enhancingtaskexecution,mul-\nseveralchallengesremain,andthetablepresentstheseinsights timodalperception,andspecializedapplications.Despitethese\nand future directions in a structured manner. advancements, challenges remain in optimizing retrieval ef-\nficiency, improving cross-modal reasoning, and scaling real-\n2) RAG in 3D Content Generation: The integration of world applications.", "metadata": {"id": "bca52af92ee1e0c148a023988509919e33ca2098", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 13, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "2) RAG in 3D Content Generation: The integration of world applications.\nRAG mechanisms into 3D object- and scene-level generation\npresents significant potential for future research. One of the\nD. Application\nprimary challenges in this domain is the scarcity of high-\nquality 3D object datasets, a limitation that extends to 3D 1) MultimodalInteractionandUnderstanding: Multimodal\nscene generation. Current approaches in scene synthesis often interaction and understanding represent a significant applica-", "metadata": {"id": "47e212ac76976ee325e4681cd2cb75618bd1cfcf", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 13, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "rely on component composition [129] and scene graph repre- tiondomainforcomputervision,involvingthecomprehension\nsentations[130].IncorporatingRAGinto3Dscenegeneration of multiple modalities such as text, images, and videos.\ncould enhance diversity and quality by leveraging external By integrating information from different modalities, RAG\nknowledge sources to guide the synthesis process. models can achieve a better understanding of complex scenes\nand tasks. For instance, in Chatbot applications using visual\nAdvancements in 2D diffusion models have driven progress", "metadata": {"id": "b3888c0fabb86a0214b4fa7a893631a2db5acca8", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 13, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "Advancements in 2D diffusion models have driven progress\nquestion answering (VQA) [77], RAG enhances the abil-\nin 3D generation, yet two major challenges persist:\nity to generate accurate and contextually relevant responses.\nData Availability: Compared to large-scale 2D datasets con- Additionally, RAG has been effectively applied to various\ntaining billions of image-text pairs [113, 114], the largest 3D downstream tasks [78], such as image classification, object\ndatasets [115, 116] contain only around 10 million objects. detection, and segmentation, to enhance model performance", "metadata": {"id": "8bffa98154f2b87436cd06d9cc5b68edfab48f5d", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 13, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "Moreover, a significant portion of these objects is excluded and adaptability through external knowledge retrieval.\nfromtrainingduetoqualityconcerns[112,117,118],limiting 2) VisionGenerationTasks: Visualgenerationtasksaimto\nthe performance of 3D generative models relative to their 2D enhancethemodels’generativecapabilitiesbyretrievingexter-\ncounterparts. nal knowledge, thereby producing high-quality visual content.\nThese tasks encompass not only generating images or videos\nGeneration Quality: 3D generation remains an inherently\nfrom textual descriptions but also creating 3D models. The", "metadata": {"id": "cc99f7c48d2982f6275c6c6e2d733086cbea3152", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 13, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "from textual descriptions but also creating 3D models. The\nill-posed problem, as it is typically conditioned on text or\napplications of generation tasks are extensive, with existing\nsingle-image inputs, which provide insufficient information\nuses including providing inspiration for creative design and\nto fully reconstruct a detailed 3D model. This constraint\nartistic creation [99]. Potential future applications could in-\naffects the realism and structural fidelity of generated content,\nvolvecreatingimmersiveexperiencesinvirtualandaugmented", "metadata": {"id": "c8278bf759e9e5a99b2c2ac408828ffb8580ff8f", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 13, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "volvecreatingimmersiveexperiencesinvirtualandaugmented\nhighlighting the need for enhanced conditioning mechanisms\nreality, as well as generating personalized multimedia content\nthat incorporate richer external knowledge sources.\nin content creation to meet the diverse needs of users.\nBy addressing these challenges, the integration of RAG 3) Multifaceted Applications of Embodied AI: Embodied\ninto 3D content generation could significantly improve data AI, which integrates perception, interaction, and action ca-", "metadata": {"id": "c24338e25eb386d5a3d3ab537a45f2bfa3de047c", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 13, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "efficiency and synthesis quality, opening new avenues for pabilities, enables agents to interact with the physical world.\nresearch and practical applications. RAG plays a crucial role in embodied AI by retrieving", "metadata": {"id": "d08fe91cc2d7b7dd7316ed0331d348b3891963fd", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 13, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "14\nKeyInsights FutureDirections\nAdaptive Decision-Making in Dynamic Environments RAG-enabled sys- Optimized Real-Time Retrieval Efficient retrieval pipelines are essential for\ntems, such as those in [123], retrieve relevant task knowledge in real time, real-time applications in robotics and autonomous systems. Future research\nallowingagentstoadapttonovelchallengesandoptimizeperformance. shouldfocusonlow-latencyretrievalmechanismsthatbalanceefficiencywith\naccuracy.", "metadata": {"id": "4683eb96955b09b31a55261e4c871595630ff301", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 14, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "accuracy.\nEnhanced Multimodal Perception and Interaction By leveraging RAG, Scalable Multimodal Fusion Improved multimodal fusion techniques are\nframeworks such as [125] and [126] improve perception accuracy and enable neededforseamlessintegrationofretrievedinformation,includinghierarchical\nmorenaturalinteractions. retrievalarchitecturesfordynamiccross-modalreasoning.\nOptimizedDecision-MakinginSpecializedDomainsSystemslike[127]and Context-AwareandPersonalizedRetrievalFutureRAG-enhancedEmbodied", "metadata": {"id": "11af7401ba14acd0b6c053fbe9dd0cfaad197e41", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 14, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "[128]improvedecisionaccuracyandexplainability,makingAI-drivensystems AIshouldimplementadaptiveretrievalmechanismstailoredtotask-specific\nmorereliable. andenvironmentalcontexts.\nMemory-Augmented Planning and Execution Progressive retrieval tech- Privacy-Preserving Retrieval Future RAG frameworks should incorporate\nniques,asshownin[123],improveadaptabilityandefficiencybydynamically privacy-aware retrieval to ensure data security and regulatory compliance,\nrefiningactionplans. especiallyinsensitiveapplicationslikepersonalassistantsandmedicalrobotics.", "metadata": {"id": "403567d049ae6dbed9fefcb19cb0b76785fb1714", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 14, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "refiningactionplans. especiallyinsensitiveapplicationslikepersonalassistantsandmedicalrobotics.\nTABLE VII: Insights and Future Directions for RAG in Embodied AI\nexternal knowledge in real time, significantly enhancing the how these methods can be applied to enhance real-world\nagents’ interaction and decision-making capabilities. RAG decision-making and model adaptability. We hope our work\nallows agents to dynamically adjust their strategies, and opti- encourages further research that will continue to push the", "metadata": {"id": "1eedf3e10205f7a872a645764b4bc92843967318", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 14, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "mize task planning and execution. These enhancements make boundaries of RAG’s applications in CV, leading to more\nembodied AI agents more adaptable and efficient, paving the robust, intelligent, and adaptable vision systems.\nway for practical applications in areas such as autonomous\ndriving [127] and intelligent robotics [123].\nReferences\n4) Domain-Specific Applications: The domain-specific ap-\n[1] Y. Gao,Y. Xiong, X.Gao, K.Jia, J. Pan,Y. Bi,Y. Dai,\nplications of RAG span a variety of fields, with notable\nJ. Sun, and H. Wang, “Retrieval-augmented generation", "metadata": {"id": "ba08bfc9e8662c6b072df144b2feae15755a1c57", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 14, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "J. Sun, and H. Wang, “Retrieval-augmented generation\ncontributions in medical and industrial contexts. Within the\nmedical realm, RAG retrieves medical databases to ensure for large language models: A survey,” arXiv preprint\nthe accuracy of generated medical reports and diagnoses [96].\narXiv:2312.10997, 2023. 1, 2\n[2] W. Fan, Y. Ding, L. Ning, S. Wang, H. Li, D. Yin,\nMeanwhile, in the industrial field, it digs into relevant defect\nT. Chua, and Q. Li, “A survey on RAG meeting llms:\npatterns and standards to sharpen the precision and efficiency\nTowardsretrieval-augmentedlargelanguagemodels,”in", "metadata": {"id": "c5e8028c9fe5c3ffe85326a90c2615217035f030", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 14, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "Towardsretrieval-augmentedlargelanguagemodels,”in\nofqualitycontrolprocesses[131].Theseapplicationshighlight\nthe versatility and power of RAG across diverse domains, Proceedings of the 30th ACM SIGKDD Conference on\nillustrating its promising potential usage in specific domains. Knowledge Discovery and Data Mining, KDD 2024,\nBarcelona, Spain, August 25-29, 2024, R. Baeza-Yates\nand F. Bonchi, Eds. ACM, 2024, pp. 6491–6501. 1,\nVI. Conclusion\n2, 5\nIn this survey, we have presented a comprehensive review [3] Y. Hu and Y. Lu, “Rag and rau: A survey on retrieval-", "metadata": {"id": "132c79429dca707e4c1ac1c5f8d0a022fc77eecf", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 14, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "of retrieval-augmented generation (RAG) techniques in the augmentedlanguagemodelinnaturallanguageprocess-\ncontext of computer vision (CV). We explored the integration ing,” arXiv preprint arXiv:2404.19543, 2024. 2\nof RAG into visual understanding, visual generation, and [4] L. Zhao, X. Chen, E. Z. Chen, Y. Liu, T. Chen, and\nembodied vision, highlighting the significant potential for S. Sun, “Retrieval-augmented few-shot medical image\nenhancing tasks such as object recognition, scene parsing, segmentation with foundation models,” arXiv preprint", "metadata": {"id": "5e22e7c502fc67d7572095a213800502225f3c1f", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 14, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "and3Dcontentgeneration.Byleveragingexternalknowledge, arXiv:2408.08813, 2024. 2, 3\nRAG improves model performance, addressing challenges [5] S.Gupta,R.Ranjan,andS.N.Singh,“Acomprehensive\nrelated to knowledge limitations, outdated information, and survey of retrieval-augmented generation (rag): Evo-\ndomain-specific expertise. We have identified key advance- lution, current landscape and future directions,” arXiv\nments in RAG for CV, including its application in multimodal preprint arXiv:2410.12837, 2024. 2", "metadata": {"id": "36ef2e68a2e02ec752d8474853f7bbd6ee76473a", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 14, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "ments in RAG for CV, including its application in multimodal preprint arXiv:2410.12837, 2024. 2\nunderstanding and its role in improving the efficiency and [6] P. Zhao, H. Zhang, Q. Yu, Z. Wang, Y. Geng, F. Fu,\nqualityofvisualgenerationtasks.Despiteitspotential,RAGin L. Yang, W. Zhang, J. Jiang, and B. Cui, “Retrieval-\nCVstillfaceschallenges,suchasretrievalefficiency,modality augmented generation for ai-generated content: A sur-\nalignment,andcomputationalcost,whichneedtobeaddressed vey,” arXiv preprint arXiv:2402.19473, 2024. 2", "metadata": {"id": "c0eef0cd5752855a845319ecf585aa439dd48a10", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 14, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "forbroaderadoption.Ourworkalsooutlinesfutureresearchdi- [7] H. Yu, A. Gan, K. Zhang, S. Tong, Q. Liu, and\nrections, particularlyin real-time retrievaloptimization, cross- Z. Liu, “Evaluation of retrieval-augmented generation:\nmodal fusion, and the integration of RAG into embodied A survey,” in CCF Conference on Big Data. Springer,\nAI, 3D content generation, and robotics. This survey serves 2024, pp. 102–120. 2\nas a foundation for future exploration of retrieval-augmented [8] T.T.ProckoandO.Ochoa,“Graphretrieval-augmented", "metadata": {"id": "059fcd28a513d44f5e0e61b3876b7989d620d529", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 14, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "techniques in computer vision, offering valuable insights into generation for large language models: A survey,” in", "metadata": {"id": "98df89285d373e51dc808700494a1e332bd8d536", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 14, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "15\n2024 Conference on AI, Science, Engineering, and rithm using vlm for rag-based video analysis system,”\nTechnology (AIxSET). IEEE, 2024, pp. 166–169. 2 in CVPRW, 2024. 3, 4\n[9] Y. Zhou, Y. Liu, X. Li, J. Jin, H. Qian, Z. Liu, [23] Y. Piadyk, J. Rulff, E. Brewer, M. Hosseini,\nC. Li, Z. Dou, T.-Y. Ho, and P. S. Yu, “Trustworthiness K. Ozbay, M. Sankaradas, S. Chakradhar, and C. Silva,\nin retrieval-augmented generation systems: A survey,” “Streetaware: A high-resolution synchronized multi-\narXiv preprint arXiv:2409.10102, 2024. 2 modal urban scene dataset,” Sensors, vol. 23, no. 7, p.", "metadata": {"id": "e911468b1392d010435cfc4b4535a1a9dca8371f", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 15, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "arXiv preprint arXiv:2409.10102, 2024. 2 modal urban scene dataset,” Sensors, vol. 23, no. 7, p.\n[10] A. Singh, A. Ehtesham, S. Kumar, and T. T. Khoei, 3710, 2023. 4\n“Agentic retrieval-augmented generation: A survey on [24] F. Kossmann, Z. Wu, E. Lai, N. Tatbul, L. Cao,\nagenticrag,”arXivpreprintarXiv:2501.09136,2025. 2, T. Kraska, and S. Madden, “Extract-transform-load for\n3 videostreams,”arXivpreprintarXiv:2310.04830,2023.\n[11] B. Ni, Z. Liu, L. Wang, Y. Lei, Y. Zhao, X. Cheng, 4\nQ. Zeng, L. Dong, Y. Xia, K. Kenthapadi et al., [25] L. Zhang, T. Zhao, H. Ying, Y. Ma, and K. Lee,", "metadata": {"id": "e547898f6374d3635fa920caa23e3d31b9e0c924", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 15, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "Q. Zeng, L. Dong, Y. Xia, K. Kenthapadi et al., [25] L. Zhang, T. Zhao, H. Ying, Y. Ma, and K. Lee,\n“Towards trustworthy retrieval augmented generation “Omagent:Amulti-modalagentframeworkforcomplex\nfor large language models: A survey,” arXiv preprint video understanding with task divide-and-conquer,” in\narXiv:2502.06872, 2025. 2, 3 arxiv, 2024. 3, 4\n[12] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, [26] A. Wang, B. Wu, S. Chen, Z. Chen, H. Guan, W.-N.\nN. Goyal, H. Küttler, M. Lewis, W.-t. Yih, T. Rock- Lee, L. E. Li, and C. Gan, “Sok-bench: A situated", "metadata": {"id": "221c66a4954f207c8f033682ded3ff2a06dfd7f3", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 15, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "täschel et al., “Retrieval-augmented generation for video reasoning benchmark with aligned open-world\nknowledge-intensive nlp tasks,” NeurIPS, vol. 33, pp. knowledge,” in CVPR, 2024, pp. 13384–13394. 4\n9459–9474, 2020. 1 [27] M. Sankaradas, R. K. Rajendran, and S. T. Chakrad-\n[13] S. Yu, C. Tang, B. Xu, J. Cui, J. Ran, Y. Yan, Z. Liu, har, “Streamingrag: Real-time contextual retrieval and\nS. Wang, X. Han, Z. Liu et al., “Visrag: Vision- generation framework,” arXiv, 2025. 4\nbasedretrieval-augmentedgenerationonmulti-modality [28] A. Long, W. Yin, T. Ajanthan, V. Nguyen, P. Purkait,", "metadata": {"id": "bd3b4872143cfd0fe0646ab77e6e48cb5a73c8db", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 15, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "documents,”arXivpreprintarXiv:2410.10594,2024. 1, R. Garg, A. Blair, C. Shen, and A. van den Hengel,\n7, 8 “Retrieval augmented classification for long-tail visual\n[14] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual recognition,” in CVPR, 2022, pp. 6959–6969. 3\nlearning for image recognition,” in CVPR, 2016, pp. [29] J. Kim, E. Cho, S. Kim, and H. J. Kim, “Retrieval-\n770–778. 1 augmentedopen-vocabularyobjectdetection,”inCVPR,\n[15] Y. Lyu, X. Zheng, J. Zhou, and L. Wang, “Unibind: 2024, pp. 17427–17436. 3", "metadata": {"id": "46ca71545d8f3b001ac373847303bd844607b35e", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 15, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "[15] Y. Lyu, X. Zheng, J. Zhou, and L. Wang, “Unibind: 2024, pp. 17427–17436. 3\nLlm-augmented unified and balanced representation [30] Y.ZhouandG.Long,“Style-awarecontrastivelearning\nspace to bind them all,” in CVPR, 2024, pp. 26752– for multi-style image captioning,” in EACL, 2023. 3\n26762. 1, 2 [31] J. Li, D. M. Vo, A. Sugimoto, and H. Nakayama,\n[16] H. Zhang, F. Li, S. Liu, L. Zhang, H. Su, J. Zhu, “Evcap:Retrieval-augmentedimagecaptioningwithex-\nL. M. Ni, and H.-Y. Shum, “Dino: Detr with improved ternal visual-name memory for open-world comprehen-", "metadata": {"id": "a1eb35970ba55f09abc277c5f5379d0d631b4c7f", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 15, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "denoisinganchorboxesforend-to-endobjectdetection,” sion,” in CVPR, 2024, pp. 13733–13742. 3\narXiv preprint arXiv:2203.03605, 2022. 1 [32] R. Ramos, D. Elliott, and B. Martins, “Retrieval-\n[17] N. Ravi, V. Gabeur, Y.-T. Hu, R. Hu, C. Ryali, T. Ma, augmented image captioning,” arXiv preprint\nH. Khedr, R. Rädle, C. Rolland, L. Gustafson et al., arXiv:2302.08268, 2023. 3\n“Sam2:Segmentanythinginimagesandvideos,”arXiv [33] W. Li, J. Li, R. Ramos, R. Tang, and D. Elliott, “Un-\npreprint arXiv:2408.00714, 2024. 1 derstanding retrieval robustness for retrieval-augmented", "metadata": {"id": "5bd7a746d08a1b99c252e3aabf515b9eabe0c043", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 15, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "preprint arXiv:2408.00714, 2024. 1 derstanding retrieval robustness for retrieval-augmented\n[18] Z. Liu, Z. Sun, Y. Zang, W. Li, P. Zhang, X. Dong, image captioning,” arXiv preprint arXiv:2406.02265,\nY. Xiong, D. Lin, and J. Wang, “Rar: Retrieving and 2024. 3\nrankingaugmentedmllmsforvisualrecognition,”arXiv [34] Z. Yang, L. Li, J. Wang, K. Lin, E. Azarnasab,\npreprint arXiv:2403.13805, 2024. 2, 8 F.Ahmed,Z.Liu,C.Liu,M.Zeng,andL.Wang,“Mm-\n[19] Z.Wang,T.Wang,Z.He,G.Hancke,Z.Liu,andR.W. react: Prompting chatgpt for multimodal reasoning and", "metadata": {"id": "5b76f1abf116c04f690e954f584de529e1b9a3e8", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 15, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "Lau, “Phidias: A generative model for creating 3d con- action,” in arxiv, 2023. 3, 4\ntentfromtext,image,and3dconditionswithreference- [35] J. Jin, Y. Zhu, X. Yang, C. Zhang, and Z. Dou,\naugmenteddiffusion,”arXivpreprintarXiv:2409.11406, “Flashrag: A modular toolkit for efficient retrieval-\n2024. 2, 11 augmented generation research,” in arXiv, 2024. 3, 4\n[20] S. Jeong, K. Kim, J. Baek, and S. J. Hwang, “Vide- [36] M. Bonomo and S. Bianco, “Visual rag: Expanding\norag: Retrieval-augmented generation over video cor- mllm visual knowledge without fine-tuning,” 2025. 3", "metadata": {"id": "18c1a0eea483876763ffbced180f0f940e783787", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 15, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "pus,” 2025. 3, 4 [37] J. Xue, Q. Deng, F. Yu, Y. Wang, J. Wang, and Y. Li,\n[21] A. Miech, D. Zhukov, J.-B. Alayrac, M. Tapaswi, “Enhancedmultimodalrag-llmforaccuratevisualques-\nI. Laptev, and J. Sivic, “Howto100m: Learning a text- tion answering,” 2024. 3\nvideoembeddingbywatchinghundredmillionnarrated [38] T.Kim,S.Lee,S.-W.Kim,andD.-J.Kim,“Vipcap:Re-\nvideo clips,” in ICCV, 2019, pp. 2630–2640. 4 trieval text-based visual prompts for lightweight image\n[22] M. A. Arefeen, B. Debnath, M. Y. S. Uddin, and captioning,” 2024. 4", "metadata": {"id": "713036016e9d576f77b48853fcbab86abefd447e", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 15, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "[22] M. A. Arefeen, B. Debnath, M. Y. S. Uddin, and captioning,” 2024. 4\nS. Chakradhar, “Vita: An efficient video-to-text algo- [39] Y. Huang, J. Xu, B. Pei, Y. He, G. Chen, L. Yang,", "metadata": {"id": "a0a510340885b4c76033f2ab8563cf2f1f9efb0f", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 15, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "16\nX. Chen, Y. Wang, Z. Nie, J. Liu et al., “Vinci: A the role of image understanding in visual question\nreal-time embodied smart assistant based on egocentric answering,” in 2017 IEEE Conference on Computer\nvision-language model,” arXiv, 2024. 4 VisionandPatternRecognition,CVPR2017,Honolulu,\n[40] H. Xiong, Z. Yang, J. Yu, Y. Zhuge, L. Zhang, J. Zhu, HI, USA, July 21-26, 2017. IEEE Computer Society,\nand H. Lu, “Streaming video understanding and multi- 2017, pp. 6325–6334. 5\nround interaction with memory-enhanced knowledge,” [56] D.Gurari,Q.Li,A.J.Stangl,A.Guo,C.Lin,K.Grau-", "metadata": {"id": "5038acf8945003949571685965602ffa37d6ae64", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 16, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "2025. 4 man,J.Luo,andJ.P.Bigham,“Vizwizgrandchallenge:\n[41] Y. Luo, X. Zheng, X. Yang, G. Li, H. Lin, J. Huang, Answeringvisualquestionsfromblindpeople,”in2018\nJ. Ji, F. Chao, J. Luo, and R. Ji, “Video-rag: Visually- IEEE Conference on Computer Vision and Pattern\naligned retrieval-augmented long video comprehen- Recognition,CVPR2018,SaltLakeCity,UT,USA,June\nsion,” in arXiv, 2024. 4 18-22,2018. IEEEComputerSociety,2018,pp.3608–\n[42] M. A. Arefeen, B. Debnath, M. Y. S. Uddin, and 3617. 5", "metadata": {"id": "0ada60a5f07b4365e53655a102bae85ecd935448", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 16, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "[42] M. A. Arefeen, B. Debnath, M. Y. S. Uddin, and 3617. 5\nS. Chakradhar, “irag: Advancing rag for videos with [57] K. Marino, M. Rastegari, A. Farhadi, and R. Mottaghi,\nan incremental approach,” in CIKM, 2024. 4 “OK-VQA: A visual question answering benchmark\n[43] N. Spolaõr, H. D. Lee, W. S. R. Takaki, L. A. Ensina, requiring external knowledge,” in IEEE Conference on\nC. S. R. Coy, and F. C. Wu, “A systematic review ComputerVisionandPatternRecognition,CVPR2019,\non content-based video retrieval,” in Eng. Appl. Artif. Long Beach, CA, USA, June 16-20, 2019. Computer", "metadata": {"id": "418a9872cd8b4a3f7c92bd97cea219b53a039f77", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 16, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "Intell., 2020. 4 Vision Foundation / IEEE, 2019, pp. 3195–3204. 5\n[44] Y. Chen, S. Guo, and L. Wang, “A large-scale study on [58] D. Schwenk, A. Khandelwal, C. Clark, K. Marino, and\nvideo action dataset condensation,” 2024. 4 R. Mottaghi, “A-okvqa: A benchmark for visual ques-\n[45] X. Yu, X. Feng, Y. Li, M. Liao, Y.-Q. Yu, X. Feng, tion answering using world knowledge,” in Computer\nW. Zhong, R. Chen, M. Hu, and J. Wu, “Cross-lingual Vision –ECCV 2022, S.Avidan, G. Brostow,M. Cissé,\ntext-rich visual comprehension: An information theory G. M. Farinella, and T. Hassner, Eds. Cham: Springer", "metadata": {"id": "bbca6ebf326a0ae737c54e6e3d80804e37660db9", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 16, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "perspective,” in arxiv, 2024. 4 Nature Switzerland, 2022, pp. 146–162. 5\n[46] Y. He, M. Xia, H. Chen, X. Cun, Y. Gong, J. Xing, [59] P. Lu, S. Mishra, T. Xia, L. Qiu, K. Chang, S. Zhu,\nY.Zhang,X.Wang,C.Weng,Y.Shanetal.,“Animate- O. Tafjord, P. Clark, and A. Kalyan, “Learn to explain:\na-story: Storytelling with retrieval-augmented video Multimodal reasoning via thought chains for science\ngeneration,” arXiv, 2023. 5, 10 question answering,” in NeurIPS 35: Annual Confer-\n[47] M. Zhang, Z. Wang, L. Chen, K. Liu, and J. Lin, ence on Neural Information Processing Systems 2022,", "metadata": {"id": "1f73bd17d7ebfa853f3af84b2aa1535da8d0f13c", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 16, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "“Dialogue director: Bridging the gap in dialogue visu- NeurIPS 2022, New Orleans, LA, USA, November 28 -\nalization for multimodal storytelling,” arXiv, 2024. 5 December 9, 2022, S. Koyejo, S. Mohamed, A. Agar-\n[48] W. Wen, Y. Wang, N. Birkbeck, and B. Adsumilli, wal, D. Belgrave, K. Cho, and A. Oh, Eds., 2022. 5\n“An ensemble approach to short-form video quality [60] H. Hu, Y. Luan, Y. Chen, U. Khandelwal, M. Joshi,\nassessment using multimodal llm,” 2024. 5 K. Lee, K. Toutanova, and M. Chang, “Open-domain", "metadata": {"id": "4640e6913078038bad376bd9c564e1ba99910ca2", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 16, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "assessment using multimodal llm,” 2024. 5 K. Lee, K. Toutanova, and M. Chang, “Open-domain\n[49] H. Fang, X. Sui, H. Yu, J. Kong, S. Yu, B. Chen, visual entity recognition: Towards recognizing millions\nH. Wu, and S.-T. Xia, “Retrievals can be detrimental: of wikipedia entities,” in IEEE/CVF International Con-\nA contrastive backdoor attack paradigm on retrieval- ferenceonComputerVision,ICCV2023,Paris,France,\naugmented diffusion models,” 2025. 5 October 1-6, 2023. IEEE, 2023, pp. 12031–12041. 5", "metadata": {"id": "106d7854455605ad7b91ac41b0e9b0a08b358125", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 16, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "augmented diffusion models,” 2025. 5 October 1-6, 2023. IEEE, 2023, pp. 12031–12041. 5\n[50] F.-T. Hong, Z. Xu, H. Liu, Q. Lin, L. Song, Z. Shu, [61] Y. Chen, H. Hu, Y. Luan, H. Sun, S. Changpinyo,\nY.Zhou,D.Ceylan,andD.Xu,“Free-viewpointhuman A. Ritter, and M.-W. Chang, “Can pre-trained vision\nanimation with pose-correlated reference selection,” andlanguagemodelsanswervisualinformation-seeking\narXiv, 2024. 5 questions?” in Proceedings of the 2023 Conference on\n[51] B.Luo,J.Wang,Z.Wang,J.Zhu,andX.Zhao,“Graph- Empirical Methods in Natural Language Processing,", "metadata": {"id": "fd460ec075e4931506b10abacec8f093a874de79", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 16, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "[51] B.Luo,J.Wang,Z.Wang,J.Zhu,andX.Zhao,“Graph- Empirical Methods in Natural Language Processing,\nbased cross-domain knowledge distillation for cross- H. Bouamor, J. Pino, and K. Bali, Eds. Singapore:\ndataset text-to-image person retrieval,” arXiv, 2025. 5 ACL, 2023, pp. 14948–14968. 5\n[52] Y. Xu, Y. Sun, B. Zhai, M. Li, W. Liang, Y. Li, and [62] J. Cho, D. Mahata, O. Irsoy, Y. He, and M. Bansal,\nS. Du, “Zero-shot video moment retrieval via off-the- “M3docrag: Multi-modal retrieval is what you need for", "metadata": {"id": "f00fe769ff90bfb53b14312003dd56e13cf93267", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 16, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "shelf multimodal large language models,” 2025. 5 multi-page multi-document understanding,” 2024. 5, 6,\n[53] Y.Xu,Y.Sun,B.Zhai,Z.Xie,Y.Jia,andS.Du,“Multi- 7, 8\nmodal fusion and query refinement network for video [63] M. Faysse, H. Sibille, T. Wu, B. Omrani, G. Viaud,\nmoment retrieval and highlight detection,” in ICME, C. HUDELOT, and P. Colombo, “Colpali: Efficient\n2024. 5 document retrieval with vision language models,” in\n[54] C. Chen, F. Lv, Y. Guan, P. Wang, S. Yu, Y. Zhang, ICLR, 2025. 5, 6, 7, 8", "metadata": {"id": "d9c6d6d950fd37d9db47bb91343f04673ef0aa52", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 16, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "[54] C. Chen, F. Lv, Y. Guan, P. Wang, S. Yu, Y. Zhang, ICLR, 2025. 5, 6, 7, 8\nand Z. Tang, “Human-guided image generation for [64] K. Dong, Y. Chang, X. D. Goh, D. Li, R. Tang, and\nexpanding small-scale training image datasets,” 2024. Y.Liu,“Mmdocir:Benchmarkingmulti-modalretrieval\n5 for long documents,” 2025. 5, 6\n[55] Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and [65] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona,\nD. Parikh, “Making the V in VQA matter: Elevating D. Ramanan, P. Dollár, and C. L. Zitnick, “Microsoft", "metadata": {"id": "693388a5c665e6efa31dbe6d086dede3b6d8de8f", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 16, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "17\ncoco: Common objects in context,” in Computer Vision retrieval augmented generation performance,” arXiv,\n– ECCV 2014, D. Fleet, T. Pajdla, B. Schiele, and 2025. 5, 6\nT. Tuytelaars, Eds. Cham: Springer International [77] W. Chen, H. Hu, X. Chen, P. Verga, and W. Cohen,\nPublishing, 2014, pp. 740–755. 5 “MuRAG: Multimodal retrieval-augmented generator\n[66] A. Gupta, P. Dollár, and R. B. Girshick, “LVIS: A for open question answering over images and text,”\ndataset for large vocabulary instance segmentation,” in EMNLP, Y. Goldberg, Z. Kozareva, and Y. Zhang,", "metadata": {"id": "865079cff9595f15d0a4a0592dd75ee949d82aae", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 17, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "in IEEE Conference on Computer Vision and Pattern Eds. Abu Dhabi, United Arab Emirates: ACL, 2022,\nRecognition, CVPR 2019, Long Beach, CA, USA, June pp. 5558–5570. 5, 6, 7, 13\n16-20, 2019. Computer Vision Foundation / IEEE, [78] H. Liu, K. Son, J. Yang, C. Liu, J. Gao, Y. J. Lee,\n2019, pp. 5356–5364. 5, 6 and C. Li, “Learning customized visual models with\n[67] J. Wang, P. Zhang, T. Chu, Y. Cao, Y. Zhou, T. Wu, retrieval-augmented knowledge,” in CVPR. IEEE,\nB. Wang, C. He, and D. Lin, “V3det: Vast vocabulary 2023, pp. 15148–15158. 5, 8, 13", "metadata": {"id": "b1e3636ec75347494a36e8aeb4368cd47db75e03", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 17, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "B. Wang, C. He, and D. Lin, “V3det: Vast vocabulary 2023, pp. 15148–15158. 5, 8, 13\nvisual detection dataset,” in IEEE/CVF International [79] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona,\nConference on Computer Vision, ICCV 2023, Paris, D. Ramanan, P. Dollár, and C. L. Zitnick, “Microsoft\nFrance, October 1-6, 2023. IEEE, 2023, pp. 19787– coco:Commonobjectsincontext,”inComputervision–\n19797. 5, 6 ECCV2014:13thEuropeanconference,zurich,Switzer-\n[68] C. Li, H. Liu, L. H. Li, P. Zhang, J. Aneja, J. Yang, land, September 6-12, 2014, proceedings, part v 13.", "metadata": {"id": "72d9522f0890aa416137d0083e6019d0fb559049", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 17, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "P. Jin, H. Hu, Z. Liu, Y. J. Lee, and J. Gao, “EL- Springer, 2014, pp. 740–755. 6, 10\nEVATER: A benchmark and toolkit for evaluating [80] Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and\nlanguage-augmented visual models,” in NeurIPS 35: D. Parikh, “Making the V in VQA matter: Elevating\nAnnual Conference on Neural Information Processing the role of image understanding in visual question\nSystems 2022, NeurIPS 2022, New Orleans, LA, USA, answering,” in CVPR. IEEE Computer Society, 2017,\nNovember 28 - December 9, 2022, S. Koyejo, S. Mo- pp. 6325–6334. 6", "metadata": {"id": "292924a175b904951a818f76d769be8a3eac8263", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 17, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "November 28 - December 9, 2022, S. Koyejo, S. Mo- pp. 6325–6334. 6\nhamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, [81] C. Li, Z. Li, C. Jing, S. Liu, W. Shao, Y. Wu, P. Luo,\nEds., 2022. 5, 6 Y. Qiao, and K. Zhang, “SearchLVLMs: A plug-and-\n[69] Y. Liu, H. Duan, Y. Zhang, B. Li, S. Zhang, W. Zhao, play framework for augmenting large vision-language\nY. Yuan, J. Wang, C. He, Z. Liu, K. Chen, and D. Lin, models by searching up-to-date internet knowledge,” in\n“Mmbench: Is your multi-modal model an all-around NeurIPS, 2024. 6", "metadata": {"id": "c76c9aac3584f841da8dd9b6b92576e9d662bb2d", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 17, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "“Mmbench: Is your multi-modal model an all-around NeurIPS, 2024. 6\nplayer?” 2023. 5, 6 [82] W. Lin, J. Chen, J. Mei, A. Coca, and B. Byrne,\n[70] B. Li, Y. Ge, Y. Ge, G. Wang, R. Wang, R. Zhang, and “Fine-grained late-interaction multi-modal retrieval for\nY. Shan, “Seed-bench: Benchmarking multimodal large retrieval augmented visual question answering,” in\nlanguage models,” in CVPR, 2024, pp. 13299–13308. NeurIPS, 2023. 7\n5, 6 [83] C.Tan,J.Wei,L.Sun,Z.Gao,S.Li,B.Yu,R.Guo,and\n[71] L. Chen, J. Li, X. Dong, P. Zhang, Y. Zang, Z. Chen, S. Z. Li, “Retrieval meets reasoning: Even high-school", "metadata": {"id": "4330510b4a7f995f7966effe044cb6022259dd1f", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 17, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "H. Duan, J. Wang, Y. Qiao, D. Lin, and F. Zhao, “Are textbook knowledge benefits multimodal reasoning,”\nweontherightwayforevaluatinglargevision-language CoRR, vol. abs/2405.20834, 2024. 7\nmodels?” in NeurIPS, NeurIPS 2024, Vancouver, BC, [84] Z. Chen, C. Xu, Y. Qi, and J. Guo, “Mllm is\nCanada, December 10 - 15, 2024, 2024. 5, 6 a strong reranker: Advancing multimodal retrieval-\n[72] Y.Wu,Q.Long,J.Li,J.Yu,andW.Wang,“Visual-rag: augmented generation via knowledge-enhanced rerank-\nBenchmarkingtext-to-imageretrievalaugmentedgener- ing and noise-injected training,” 2024. 7", "metadata": {"id": "cdd31e74d263d47f276ef421c4355161397599fe", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 17, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "Benchmarkingtext-to-imageretrievalaugmentedgener- ing and noise-injected training,” 2024. 7\nation for visual knowledge intensive queries,” 2025. 5, [85] M. Mathew, D. Karatzas, and C. V. Jawahar, “Docvqa:\n6 Adatasetforvqaondocumentimages,”inWACV,2021,\n[73] W. Hu, J.-C. Gu, Z.-Y. Dou, M. Fayyaz, P. Lu, K.- pp. 2199–2208. 7, 8\nW. Chang, and N. Peng, “MRAG-bench: Vision-centric [86] RAGFlow, “Ragflow is an open-source rag (retrieval-\nevaluationforretrieval-augmentedmultimodalmodels,” augmented generation) engine based on deep document\nin ICLR, 2025. 5, 6 understanding,” 2024. 7, 8", "metadata": {"id": "2a4e0c4a91fa23ac19f382654506a5dda6b5f1ae", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 17, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "in ICLR, 2025. 5, 6 understanding,” 2024. 7, 8\n[74] N. Wasserman, R. Pony, O. Naparstek, A. R. Goldfarb, [87] M. Riedler and S. Langer, “Beyond text: Optimizing\nE. Schwartz, U. Barzelay, and L. Karlinsky, “Real-mm- rag with multimodal inputs for industrial applications,”\nrag: A real-world multi-modal retrieval benchmark,” 2024. 7, 8\n2025. 5, 6 [88] J. Ye, A. Hu, H. Xu, Q. Ye, M. Yan, Y. Dan, C. Zhao,\n[75] Y. Li, Y. Du, K. Zhou, J. Wang, X. Zhao, and J.-R. G. Xu, C. Li, J. Tian, Q. Qi, J. Zhang, and F. Huang,", "metadata": {"id": "447d75a1f64b86d8cb301352c9dd03e462151372", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 17, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "Wen, “Evaluating object hallucination in large vision- “mplug-docowl: Modularized multimodal large lan-\nlanguage models,” in Proceedings of the 2023 Con- guage model for document understanding,” 2023. 7\nference on Empirical Methods in Natural Language [89] Z. Wang, M. Xia, L. He, H. Chen, Y. Liu, R. Zhu,\nProcessing, H. Bouamor, J. Pino, and K. Bali, Eds. K. Liang, X. Wu, H. Liu, S. Malladi, A. Chevalier,\nSingapore: ACL, 2023, pp. 292–305. 5, 6 S. Arora, and D. Chen, “Charxiv: Charting gaps in", "metadata": {"id": "0cfe97261d0fbcf384536259e1cacff48b15bfe9", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 17, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "Singapore: ACL, 2023, pp. 292–305. 5, 6 S. Arora, and D. Chen, “Charxiv: Charting gaps in\n[76] M. Mortaheb, M. A. A. Khojastepour, S. T. Chakrad- realistic chart understanding in multimodal LLMs,” in\nhar,andS.Ulukus,“Rag-check:Evaluatingmultimodal NeurIPS, 2024. 8", "metadata": {"id": "dd869ea0d87da518c1f44b3977646b65cfc3884e", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 17, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "18\n[90] Z. Hu, A. Iscen, C. Sun, Z. Wang, K. Chang, Y. Sun, [105] S.Zhang,Z.Chong,X.Zhang,H.Li,Y.Cheng,Y.Yan,\nC.Schmid,D.A.Ross,andA.Fathi,“Reveal:Retrieval- and X. Liang, “Garmentaligner: Text-to-garment gener-\naugmented visual-language pre-training with multi- ation via retrieval-augmented multi-level corrections,”\nsource multimodal knowledge memory,” in CVPR. in ECCV. Springer, 2025, pp. 148–164. 10\nIEEE, 2023, pp. 23369–23379. 8 [106] W. Ding, Y. Cao, D. Zhao, C. Xiao, and M. Pavone,\n[91] J. Rao, Z. Shan, L. Liu, Y. Zhou, and Y. Yang, “Realgen: Retrieval augmented generation for control-", "metadata": {"id": "234684cd1eb1f73e66c2dfc524874a3285e82008", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 18, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "“Retrieval-basedknowledgeaugmentedvisionlanguage lable traffic scenarios,” in ECCV. Springer, 2025, pp.\npre-training,” in ACM MM. ACM, 2023, pp. 5399– 93–110. 10\n5409. 8 [107] D. Horita, N. Inoue, K. Kikuchi, K. Yamaguchi, and\n[92] V. N. Rao, S. Choudhary, A. Deshpande, R. K. Sat- K.Aizawa,“Retrieval-augmentedlayouttransformerfor\nzoda, and S. Appalaraju, “Raven: Multitask retrieval content-aware layout generation,” in CVPR, 2024, pp.\naugmented vision-language learning,” 2024. 8 67–76. 10", "metadata": {"id": "f502cf40d168c566edb8923dda936557c0f2ccd3", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 18, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "augmented vision-language learning,” 2024. 8 67–76. 10\n[93] J. Sun, J. Zhang, Y. Zhou, Z. Su, X. Qu, and Y. Cheng, [108] H. Yuan, Z. Zhao, S. Wang, S. Xiao, M. Ni, Z. Liu,\n“SURf:Teachinglargevision-languagemodelstoselec- andZ.Dou,“Finerag:Fine-grainedretrieval-augmented\ntively utilize retrieved information,” in EMNLP, Y. Al- text-to-image generation,” in Proceedings of the 31st\nOnaizan, M. Bansal, and Y.-N. Chen, Eds. Miami, InternationalConferenceonComputationalLinguistics,\nFlorida, USA: ACL, 2024, pp. 7611–7629. 8 2025, pp. 11196–11205. 10", "metadata": {"id": "86c03f371e56e33698e15f68fb6fac91bc12c1f4", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 18, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "Florida, USA: ACL, 2024, pp. 7611–7629. 8 2025, pp. 11196–11205. 10\n[94] J. Qi, Z. Xu, R. Shao, Y. Chen, J. Di, Y. Cheng, [109] Y. Chang, M. Narang, H. Suzuki, G. Cao, J. Gao,\nQ. Wang, and L. Huang, “Rora-vlm: Robust retrieval- and Y. Bisk, “Webqa: Multihop and multimodal qa,”\naugmented vision language models,” 2024. 8 in CVPR, 2022, pp. 16495–16504. 10\n[95] P. Xia, K. Zhu, H. Li, T. Wang, W. Shi, S. Wang, [110] Y. Hong, K. Zhang, J. Gu, S. Bi, Y. Zhou, D. Liu,\nL. Zhang, J. Zou, and H. Yao, “Mmed-rag: Versatile F. Liu, K. Sunkavalli, T. Bui, and H. Tan, “Lrm: Large", "metadata": {"id": "6fa1efe96b9f9cb281d716f4574630eb615b64dc", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 18, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "multimodal rag system for medical vision language reconstruction model for single image to 3d,” arXiv\nmodels,” arXiv preprint arXiv:2410.13085, 2024. 8 preprint arXiv:2311.04400, 2023. 11\n[96] P. Xia, K. Zhu, H. Li, H. Zhu, Y. Li, G. Li, L. Zhang, [111] J. Tang, Z. Chen, X. Chen, T. Wang, G. Zeng,\nand H. Yao, “Rule: Reliable multimodal rag for factu- and Z. Liu, “Lgm: Large multi-view gaussian model\nality in medical vision language models,” in EMNLP, for high-resolution 3d content creation,” in ECCV.\n2024, pp. 1081–1093. 8, 14 Springer, 2024, pp. 1–18. 11", "metadata": {"id": "055d23a7bf96919b72f0bbbcd92c54f8111fc895", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 18, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "2024, pp. 1081–1093. 8, 14 Springer, 2024, pp. 1–18. 11\n[97] Y. Lyu, X. Zheng, L. Jiang, Y. Yan, X. Zou, H. Zhou, [112] J. Xu, W. Cheng, Y. Gao, X. Wang, S. Gao, and\nL. Zhang, and X. Hu, “Realrag: Retrieval-augmented Y. Shan, “Instantmesh: Efficient 3d mesh generation\nrealistic image generation via self-reflective contrastive from a single image with sparse-view large reconstruc-\nlearning,” arXiv preprint arXiv:2502.00848, 2025. 9, tion models,” arXiv preprint arXiv:2404.07191, 2024.\n10, 13 10, 11, 13", "metadata": {"id": "f31ea83c9286d359fd51d7d387f94600d1ca823b", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 18, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "10, 13 10, 11, 13\n[98] A. Blattmann, R. Rombach, K. Oktay, J. Müller, and [113] C. Schuhmann, R. Vencu, R. Beaumont, R. Kaczmar-\nB. Ommer, “Retrieval-augmented diffusion models,” czyk, C. Mullis, A. Katta, T. Coombes, J. Jitsev, and\nNeurIPS, vol. 35, pp. 15309–15324, 2022. 9, 13 A. Komatsuzaki, “Laion-400m: Open dataset of clip-\n[99] D. Cioni, L. Berlincioni, F. Becattini, and filtered 400 million image-text pairs,” arXiv preprint\nA. Del Bimbo, “Diffusion based augmentation for arXiv:2111.02114, 2021. 10, 13", "metadata": {"id": "1562d4bb8987ece605a13ffc3b161af731913d81", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 18, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "A. Del Bimbo, “Diffusion based augmentation for arXiv:2111.02114, 2021. 10, 13\ncaptioning and retrieval in cultural heritage,” in ICCV, [114] C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon,\n2023, pp. 1707–1716. 9, 13 R. Wightman, M. Cherti, T. Coombes, A. Katta,\n[100] M. Zhang, X. Guo, L. Pan, Z. Cai, F. Hong, C. Mullis, M. Wortsman et al., “Laion-5b: An open\nH. Li, L. Yang, and Z. Liu, “Remodiffuse: Retrieval- large-scale dataset for training next generation image-\naugmentedmotiondiffusionmodel,”inICCV,2023,pp. text models,” NeurIPS, vol. 35, pp. 25278–25294,\n364–373. 9, 10, 13 2022. 10, 13", "metadata": {"id": "64e64b0f5b1d3c5a6d1c413da5bc2bba563ded2f", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 18, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "364–373. 9, 10, 13 2022. 10, 13\n[101] “Image-based rag: https://baike.baidu.com/item/iRAG/ [115] M. Deitke, D. Schwenk, J. Salvador, L. Weihs,\n65102065?fr=ge_ala,” 2024. 9, 10, 13 O. Michel, E. VanderBilt, L. Schmidt, K. Ehsani,\n[102] Y. Wan, D. Wu, H. Wang, and K.-W. Chang, “The A. Kembhavi, and A. Farhadi, “Objaverse: A universe\nfactuality tax of diversity-intervened text-to-image gen- of annotated 3d objects,” in CVPR, 2023, pp. 13142–\neration: Benchmark and fact-augmented intervention,” 13153. 10, 13", "metadata": {"id": "ad4aaae5428f016ab1d675a3385136cd148679c7", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 18, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "eration: Benchmark and fact-augmented intervention,” 13153. 10, 13\narXiv preprint arXiv:2407.00377, 2024. 9, 13 [116] M. Deitke, R. Liu, M. Wallingford, H. Ngo, O. Michel,\n[103] C. Malaviya, S. Lee, S. Chen, E. Sieber, M. Yatskar, A. Kusupati, A. Fan, C. Laforte, V. Voleti, S. Y. Gadre\nand D. Roth, “Expertqa: Expert-curated questions and et al., “Objaverse-xl: A universe of 10m+ 3d objects,”\nattributed answers,” arXiv preprint arXiv:2309.07852, NeurIPS, vol. 36, 2024. 10, 13\n2023. 9 [117] J. Tang, Z. Li, Z. Hao, X. Liu, G. Zeng, M.-Y.", "metadata": {"id": "acd642bb3200caf773bafb4ce6f134f15c5793a0", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 18, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "2023. 9 [117] J. Tang, Z. Li, Z. Hao, X. Liu, G. Zeng, M.-Y.\n[104] R. Shalev-Arkushin, R. Gal, A. H. Bermano, and Liu, and Q. Zhang, “Edgerunner: Auto-regressive auto-\nO. Fried, “Imagerag: Dynamic image retrieval for encoder for artistic mesh generation,” arXiv preprint\nreference-guided image generation,” arXiv preprint arXiv:2409.18114, 2024. 10, 13\narXiv:2502.09411, 2025. 9, 10, 13 [118] S. Chen, X. Chen, A. Pang, X. Zeng, W. Cheng, Y. Fu,", "metadata": {"id": "9cf969576d6e6616af24b854f3f4df9b5d300da7", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 18, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "19\nF. Yin, Y. Wang, Z. Wang, C. Zhang et al., “Meshxl: detection rates,” 2024. 14\nNeural coordinate field for generative 3d foundation\nmodels,” arXiv preprint arXiv:2405.20853, 2024. 10,\n13\n[119] J. Seo, S. Hong, W. Jang, I. H. Kim, M. Kwak,\nD. Lee, and S. Kim, “Retrieval-augmented score\ndistillation for text-to-3d generation,” arXiv preprint\narXiv:2402.02972, 2024. 11\n[120] Z. Huang, L. Yang, X. Zhou, C. Qin, Y. Yu,\nX. Zheng, Z. Zhou, W. Zhang, Y. Wang, and W. Yang,\n“Interaction-basedretrieval-augmenteddiffusionmodels\nfor protein-specific 3d molecule generation,” in ICML.\n11", "metadata": {"id": "1b9f72a573feaab680c2600f1f532eed710f8ce4", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 19, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "for protein-specific 3d molecule generation,” in ICML.\n11\n[121] M. Zhang, X. Guo, L. Pan, Z. Cai, F. Hong,\nH. Li, L. Yang, and Z. Liu, “Remodiffuse: Retrieval-\naugmented motion diffusion model,” in ICCV (ICCV),\nOctober 2023, pp. 364–373. 11\n[122] Q. Wu, D. Iliash, D. Ritchie, M. Savva, and A. X.\nChang, “Diorama: Unleashing zero-shot single-view 3d\nscene modeling,” 2024. 11\n[123] W. Xu, M. Wang, W. Zhou, and H. Li, “P-rag: Pro-\ngressiveretrievalaugmentedgenerationforplanningon\nembodied everyday task,” in Proceedings of the 32nd\nACM International Conference on Multimedia, 2024,", "metadata": {"id": "d2df3d191e2b803492cab7654c53352976df4b9b", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 19, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "ACM International Conference on Multimedia, 2024,\npp. 6969–6978. 11, 14\n[124] Y. Zhu, Z. Ou, X. Mou, and J. Tang, “Retrieval-\naugmented embodied agents,” in CVPR, 2024, pp.\n17985–17995. 12\n[125] A. M. Nazar, A. Celik, M. Y. Selim, A. Abdallah,\nD. Qiao, and A. M. Eltawil, “Enwar: A rag-empowered\nmulti-modal llm framework for wireless environment\nperception,”arXivpreprintarXiv:2410.18104,2024.12,\n14\n[126] Q. Xie, S. Y. Min, T. Zhang, K. Xu, A. Ba-\njaj, R. Salakhutdinov, M. Johnson-Roberson, and\nY. Bisk, “Embodied-rag: General non-parametric em-", "metadata": {"id": "e0269bd94644cfe20b0cb200a632eb01980506cb", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 19, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "jaj, R. Salakhutdinov, M. Johnson-Roberson, and\nY. Bisk, “Embodied-rag: General non-parametric em-\nbodied memory for retrieval and generation,” in Lan-\nguageGamification-NeurIPS2024Workshop,2024. 12,\n14\n[127] W. Ding, Y. Cao, D. Zhao, C. Xiao, and M. Pavone,\n“Realgen: Retrieval augmented generation for control-\nlable traffic scenarios,” in ECCV. Springer, 2024, pp.\n93–110. 12, 14\n[128] J. Yuan, S. Sun, D. Omeiza, B. Zhao, P. New-\nman, L. Kunze, and M. Gadd, “Rag-driver: Generalis-\nable driving explanations with retrieval-augmented in-\ncontext learning in multi-modal large language model,”", "metadata": {"id": "6301810e33e5bf73d16e2a7b72bb55abf3752827", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 19, "created_at": "2025-09-12T03:12:00Z"}}
{"content": "context learning in multi-modal large language model,”\narXiv preprint arXiv:2402.10828, 2024. 12, 14\n[129] H. Bai, Y. Lyu, L. Jiang, S. Li, H. Lu, X. Lin, and\nL. Wang, “Componerf: Text-guided multi-object com-\npositional nerf with editable 3d scene layout,” arXiv\npreprint arXiv:2303.13843, 2023. 13\n[130] H. Dhamo, F. Manhardt, N. Navab, and F. Tombari,\n“Graph-to-3d: End-to-end generation and manipulation\nof 3d scenes using scene graphs,” in ICCV, 2021, pp.\n16352–16361. 13\n[131] “Retrieval augmented generation: 40% reduction in\ninspection times and notable improvement in defect", "metadata": {"id": "e6855e867990d9eade75b7e853a32426dea9db20", "source": "Retrieval Augmented Generation and Understanding.pdf", "page": 19, "created_at": "2025-09-12T03:12:00Z"}}
